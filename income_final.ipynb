{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Market Income Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.getenv(\"DATA_PATH\")\n",
    "\n",
    "file_path = os.path.join(data_dir, \"Statistics Canada\", \"incomes_quintile_hh.xlsx\")\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"log_comp_all\"] = np.log(df[\"comp_all\"])\n",
    "# Define independent (compx) and dependent (comp) variables\n",
    "compx = df[[\"log_comp_all\"]]  # Log of total income as independent variable\n",
    "\n",
    "# Add a constant term (intercept)\n",
    "compx = sm.add_constant(compx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):  \n",
    "    quantile_col = f\"comp_q{i}\"\n",
    "    log_quantile_col = f\"log_comp_q{i}\"\n",
    "    \n",
    "    if quantile_col not in df.columns:\n",
    "        print(f\"Column {quantile_col} not found in DataFrame.\")\n",
    "        continue  # Skip if the column is missing\n",
    "\n",
    "    # Handle zero or negative values before log transformation\n",
    "    if (df[quantile_col] <= 0).any():\n",
    "        print(f\"Skipping {quantile_col} due to zero/negative values.\")\n",
    "        continue\n",
    "\n",
    "    # Compute log for the current quantile\n",
    "    df[log_quantile_col] = np.log(df[quantile_col])\n",
    "\n",
    "    # Define dependent variable (compy)\n",
    "    compy = df[log_quantile_col]\n",
    "    \n",
    "    # Run the regression\n",
    "    model = sm.OLS(compy, compx).fit()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Quintile {i}\")\n",
    "    print(model.summary())\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Growth Rates for 2022 & 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate growth rates \n",
    "\n",
    "for i in range(1, 6):\n",
    "    df[f\"compq{i}_g\"] = df[f\"comp_q{i}\"].pct_change()\n",
    "    df[f\"tranrq{i}_g\"] = df[f\"tranr_q{i}\"].pct_change()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store growth rates for 2022 and 2023 in a dictionary\n",
    "\n",
    "vars = ['compq1', 'compq2', 'compq3', 'compq4', 'compq5', 'tranrq1', 'tranrq2', 'tranrq3', 'tranrq4', 'tranrq5']\n",
    "growth_rates = {}  # Dictionary to store variable names and values\n",
    "\n",
    "for var in vars:\n",
    "    # Dynamically fetch values for 2022 and 2023 and assign them to keys in the dictionary\n",
    "    growth_rates[f'canada_{var}_2022'] = df.loc[df['year'] == 2022, f'{var}_g'].values[0]\n",
    "    growth_rates[f'canada_{var}_2023'] = df.loc[df['year'] == 2023, f'{var}_g'].values[0]\n",
    "\n",
    "# Now print the stored results from the dictionary\n",
    "for key, value in growth_rates.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import 2021 census, filter data\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.dirname(current_dir)\n",
    "\n",
    "file_path_census = os.path.join(data_dir, \"Statistics Canada\", \"Census 2021\", \"Hierarchical\", \"censush.csv\")\n",
    "census2021 = pd.read_csv(file_path_census)\n",
    "census2021 = census2021[census2021[\"PR\"] == 35]\n",
    "\n",
    "# Drop rows where income equals 88888888 (dropping NA values)\n",
    "census2021 = census2021[census2021[\"TOTINC\"] != 88888888]\n",
    "census2021 = census2021[census2021[\"MRKINC\"] != 88888888]\n",
    "census2021 = census2021[census2021[\"GTRFS\"] != 88888888]\n",
    "census2021 = census2021[census2021[\"TOTINC_AT\"] != 88888888]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop\n",
    "wt_columns = [f\"WT{i}\" for i in range(1, 17)]\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"ABOID\", \"AGEIMM\", \"BFNMEMB\", \"BUILT\", \"CF_RP\", \"CFSTRUCT\", \"CIP2021\", \"CITIZEN\", \"CITOTH\",\n",
    "    \"CONDO\", \"COW\", \"DIST\", \"DTYPE\", \"EF_RP\", \"EFCOVID_ERB\", \"EFDECILE\", \"EFDIMBM_2018\", \"EMPIN\", \n",
    "    \"ETHDER\", \"FCOND\", \"FOL\", \"FPTWK\", \"GENDER\", \"GENSTAT\", \"HDGREE\", \"HLMOSTEN\", \n",
    "    \"HLMOSTFR\", \"HLMOSTNO\", \"HLREGEN\", \"HLREGFR\", \"HLREGNO\", \"HRSWRK\", \"INCTAX\", \"JOBPERM\", \"KOL\", \n",
    "   \"LI_ELIG_OML_U18\", \"LICO_AT\", \"LICO_BT\", \"LOC_ST_RES\", \"LOCSTUD\", \"LOLIMA\", \"LOLIMB\", \n",
    "    \"LOMBM_2018\", \"LSTWRK\", \"LWMOSTEN\", \"LWMOSTFR\", \"LWMOSTNO\", \"LWREGEN\", \"LWREGFR\", \"LWREGNO\", \n",
    "    \"MARSTH\", \"MOB1\", \"MOB5\", \"MODE\"\n",
    "]\n",
    "\n",
    "# Drop the columns\n",
    "census2021 = census2021.drop(columns=wt_columns)\n",
    "census2021 = census2021.drop(columns=columns_to_drop, errors=\"ignore\")  # 'errors=\"ignore\"' ensures no error if a column is missing\n",
    "\n",
    "\n",
    "#set IMMSTAT = 1 if equal to 8\n",
    "census2021[\"IMMSTAT\"] = census2021[\"IMMSTAT\"].replace(8, 1)\n",
    "\n",
    "#create suitable bedroom variavle\n",
    "\n",
    "\n",
    "# Step 1: Define Household-Level Calculation\n",
    "def household_bedsuit(group):\n",
    "    num_couples = (group['CFSTAT'] == 1).sum() // 2 + (group['CFSTAT'] == 2).sum() // 2 # Couples share a room\n",
    "    num_single_parents = (group['CFSTAT'] == 3).sum()  # Each single parent gets a room\n",
    "    num_children = (group['CFSTAT'].isin([4, 5])).sum()  # Count children\n",
    "    num_non_family = (group['CFSTAT'].isin([7, 8])).sum()  # Each gets their own room\n",
    "    num_living_alone = (group['CFSTAT'] == 6).sum()  # People living alone\n",
    "\n",
    "    # If the household has exactly 1 person and they live alone â†’ Assign bedsuit = 0 (bachelor unit)\n",
    "    if len(group) == 1 and num_living_alone == 1:\n",
    "        return 0  \n",
    "\n",
    "    # Start with bedrooms for couples and single parents\n",
    "    bedrooms_needed = num_couples + num_single_parents\n",
    "\n",
    "    # Assign bedrooms for children: Every 2 children share 1 room\n",
    "    if num_children > 0:\n",
    "        bedrooms_needed += (num_children + 1) // 2  # Round up when odd number of children\n",
    "\n",
    "    # Add rooms for non-family members (CFSTAT = 7, 8)\n",
    "    bedrooms_needed += num_non_family\n",
    "\n",
    "    # If NOS == 1, ensure bedsuit does NOT exceed BEDRM\n",
    "    if 'NOS' in group.columns and group['NOS'].eq(1).any():  # Ensure 'NOS' exists\n",
    "        max_bedrooms = group['BEDRM'].dropna().max()  # Get the max BEDRM in household\n",
    "        if pd.notna(max_bedrooms):  \n",
    "            bedrooms_needed = min(bedrooms_needed, max_bedrooms)  # Ensure it doesn't exceed BEDRM\n",
    "\n",
    "    return bedrooms_needed\n",
    "\n",
    "# Step 2: **Initialize `bedsuit` column with NaN**\n",
    "census2021['bedsuit'] = pd.NA\n",
    "\n",
    "# Step 3: Apply household-level logic\n",
    "census2021['bedsuit'] = census2021.groupby('HH_ID')['bedsuit'].transform(\n",
    "    lambda x: household_bedsuit(census2021.loc[x.index])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create unemployment indicator\n",
    "def assign_econ(row):\n",
    "    if row['AGEGRP'] in [1, 2]:\n",
    "        return 3\n",
    "    elif 2 < row['LFACT'] <= 10 and row['AGEGRP'] >= 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the function to create the ECON variable in census21\n",
    "census2021['econ'] = census2021.apply(assign_econ, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the directory of the script (Model), then go up to Data\n",
    "data_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Build the full path to amr2021.xlsx\n",
    "file_path = os.path.join(data_dir, \"amr2021.xlsx\")\n",
    "\n",
    "\n",
    "#create a median market rent variable for Toronto/not-Toronto, by bedrooms for 2021\n",
    "amr2021 = pd.read_excel(file_path)\n",
    "# Perform a left join to add the MMR column to census2021\n",
    "census2021 = census2021.merge(amr2021, on=['bedsuit', 'CMA'], how='left')\n",
    "\n",
    "census2021['mmr'] = census2021['mmr'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Correct missing values (children with no income)\n",
    "census2021.loc[census2021[\"TOTINC\"] > 88000000, \"TOTINC\"] = 0\n",
    "census2021.loc[census2021[\"MRKINC\"] > 88000000, \"MRKINC\"] = 0\n",
    "census2021.loc[census2021[\"GTRFS\"] > 88000000, \"GTRFS\"] = 0\n",
    "census2021.loc[census2021[\"TOTINC_AT\"] > 88000000, \"TOTINC_AT\"] = 0\n",
    "\n",
    "#check missing and NA values\n",
    "variables = [\"TOTINC\", \"MRKINC\", \"GTRFS\"]\n",
    "for var in variables:\n",
    "    count = (census2021[var] > 88000000).sum()\n",
    "    print(f\"Number of records with {var} > 88000000: {count}\")\n",
    "\n",
    "census2021[\"totaltransfers\"] = census2021[\"GTRFS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create quintiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a new column for total income\n",
    "census2021[\"totalincome\"] = census2021[\"MRKINC\"] + census2021[\"GTRFS\"]\n",
    "\n",
    "# Step 2: Compute total household income (sum of MRKINC and GTRFS)\n",
    "household_income = census2021.groupby(\"HH_ID\").agg(\n",
    "    totalincome=(\"totalincome\", \"sum\"),  # Sum of the newly created totalincome\n",
    "    weight=(\"WEIGHT\", \"first\")  # Take first weight per household\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "# Step 3: Compute weighted quintiles\n",
    "def weighted_quantile(values, quantiles, sample_weight):\n",
    "    \"\"\"Calculate weighted quantiles for a given dataset.\"\"\"\n",
    "    sorter = np.argsort(values)\n",
    "    values, sample_weight = values[sorter], sample_weight[sorter]\n",
    "    weighted_cdf = np.cumsum(sample_weight) / np.sum(sample_weight)\n",
    "    return np.interp(quantiles, weighted_cdf, values)\n",
    "\n",
    "# Compute cutoff points for weighted quintiles\n",
    "quantile_cutoffs = weighted_quantile(\n",
    "    household_income[\"totalincome\"].values,\n",
    "    quantiles=[0.2, 0.4, 0.6, 0.8],\n",
    "    sample_weight=household_income[\"weight\"].values\n",
    ")\n",
    "\n",
    "# Step 4: Assign households to weighted quintiles\n",
    "household_income[\"quintile\"] = np.digitize(household_income[\"totalincome\"], bins=quantile_cutoffs, right=True) + 1\n",
    "\n",
    "# Step 5: Merge the quintile info back to the original dataset\n",
    "census2021 = census2021.merge(household_income[[\"HH_ID\", \"quintile\"]], on=\"HH_ID\", how=\"left\")\n",
    "\n",
    "# Step 6: Verify the result\n",
    "print(census2021[[\"HH_ID\", \"TOTINC\", \"quintile\"]].head(10))\n",
    "\n",
    "#subset census2021 for only quintiles 1 and 2\n",
    "census2021 = census2021[census2021[\"quintile\"].isin([1, 2, 3])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify transfer income shares using individual census file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.dirname(current_dir)\n",
    "\n",
    "\n",
    "\n",
    "# import individual census file.\n",
    "file_path_censusi = os.path.join(data_dir, \"Statistics Canada\", \"Census 2021\", \"data_donnees_2021_ind.csv\")\n",
    "\n",
    "census2021i = pd.read_csv(file_path_censusi)\n",
    "\n",
    "census2021i = census2021i[census2021i[\"PR\"] == 35]\n",
    "census2021i = census2021i[census2021i[\"PRIHM\"] == 1]\n",
    "census2021i = census2021i[census2021i[\"HHInc\"] <= 25]\n",
    "\n",
    "#check how many rows have the different values of PRIHM\n",
    "print(census2021i[\"PRIHM\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for columns HHInc,GTRfs, COVID_ERB, CQPPB, EICBN, OASGI, CHDBN, GovtI,  set values greater than 88000000 equal to 0\n",
    "columns_to_fix = [\"HHInc\", \"GTRfs\", \"COVID_ERB\", \"CQPPB\", \"EICBN\", \"OASGI\", \"CHDBN\", \"GovtI\"]\n",
    "\n",
    "# Identify rows where any of the specified columns has a value of 88888888\n",
    "rows_to_drop = census2021i[census2021i[columns_to_fix].eq(88888888).any(axis=1)]\n",
    "\n",
    "# Print how many rows will be dropped\n",
    "print(f\"Rows to be dropped: {len(rows_to_drop)}\")\n",
    "\n",
    "# Drop those rows from the DataFrame\n",
    "census2021i = census2021i.drop(rows_to_drop.index)\n",
    "\n",
    "# Optionally, confirm the rows are dropped (this will show the remaining rows)\n",
    "print(f\"Remaining rows: {len(census2021i)}\")\n",
    "\n",
    "# Replace 99999999 with 0 in the specified columns\n",
    "census2021i[columns_to_fix] = census2021i[columns_to_fix].replace(99999999, 0)\n",
    "\n",
    "# Verify if the replacement is successful\n",
    "rows_to_check = census2021i[census2021i[columns_to_fix].eq(0).any(axis=1)]\n",
    "print(f\"Rows where 99999999 was replaced with 0: {len(rows_to_check)}\")\n",
    "\n",
    "#create a variable that is equal to GTRfs - COVID_ERB - CQPPB - EICBN - OASGI-CHDBN-Govtl\n",
    "census2021i[\"other\"] = census2021i[\"GTRfs\"] - census2021i[\"COVID_ERB\"] - census2021i[\"CQPPB\"] - census2021i[\"EICBN\"] - census2021i[\"OASGI\"] - census2021i[\"CHDBN\"]\n",
    "\n",
    "# Display the head of census2021i with specified columns\n",
    "print(census2021i[['PPSORT', 'HHInc', 'GTRfs', 'COVID_ERB', 'CQPPB', 'EICBN', 'OASGI', 'CHDBN', 'GovtI', 'other']].head())\n",
    "\n",
    "# Define the columns to compute as share of GTRfs\n",
    "share_columns = [\"COVID_ERB\", \"CQPPB\", \"EICBN\", \"OASGI\", \"CHDBN\", \"other\"]\n",
    "\n",
    "# Calculate share of each column as a percentage of GTRfs\n",
    "for column in share_columns:\n",
    "    # Check if the column exists to avoid errors\n",
    "    if column in census2021i.columns and \"GTRfs\" in census2021i.columns:\n",
    "        census2021i[f\"{column}_share\"] = census2021i[column] / census2021i[\"GTRfs\"]\n",
    "    else:\n",
    "        print(f\"Column {column} or GTRfs not found in the dataframe\")\n",
    "\n",
    "# Print the new columns to verify\n",
    "print(census2021i[[\"COVID_ERB_share\", \"CQPPB_share\", \"EICBN_share\", \"OASGI_share\", \"CHDBN_share\", \"other_share\"]].head())\n",
    "\n",
    "# Define income groups\n",
    "low_income = census2021i[\"HHInc\"] < 14\n",
    "mid_income = (census2021i[\"HHInc\"] >= 14) & (census2021i[\"HHInc\"] <= 19)\n",
    "high_income = census2021i[\"HHInc\"] >19\n",
    "\n",
    "# Function to compute weighted shares for a subset of the dataset and save them to individual variables\n",
    "def compute_weighted_shares(df, label, prefix):\n",
    "    weighted_sum_GTRfs = (df[\"GTRfs\"] * df[\"WEIGHT\"]).sum()\n",
    "    \n",
    "    for column in share_columns:\n",
    "        if column in df.columns and \"GTRfs\" in df.columns:\n",
    "            weighted_sum_column = (df[column] * df[\"WEIGHT\"]).sum()\n",
    "            share = weighted_sum_column / weighted_sum_GTRfs if weighted_sum_GTRfs != 0 else 0\n",
    "            # Create individual variables and assign the computed share values\n",
    "            globals()[f\"{prefix}_{column}_share\"] = share\n",
    "        else:\n",
    "            print(f\"Column {column} or GTRfs not found in the dataframe for {label}\")\n",
    "\n",
    "# Compute for low-income group\n",
    "compute_weighted_shares(census2021i[low_income], \"HHInc < 44,000\", \"q1\")\n",
    "\n",
    "# Compute for mid-income group\n",
    "compute_weighted_shares(census2021i[mid_income], \"HHInc 44,001 - 74,000\", \"q2\")\n",
    "\n",
    "# Compute for high-income group\n",
    "compute_weighted_shares(census2021i[high_income], \"HHInc 74,000 - 110,000\", \"q3\")\n",
    "\n",
    "for column in share_columns:\n",
    "    # Print variables for low-income group (q1)\n",
    "    q1_share_value = globals().get(f'q1_{column}_share', 'Not Defined')\n",
    "    print(f\"q1_{column}_share: {q1_share_value}\")\n",
    "    \n",
    "    # Print variables for mid-income group (q2)\n",
    "    q2_share_value = globals().get(f'q2_{column}_share', 'Not Defined')\n",
    "    print(f\"q2_{column}_share: {q2_share_value}\")\n",
    "\n",
    "      # Print variables for high-income group (q3)\n",
    "    q3_share_value = globals().get(f'q3_{column}_share', 'Not Defined')\n",
    "    print(f\"q3_{column}_share: {q3_share_value}\")\n",
    "\n",
    "# Sum all q1 shares excluding q1_COVID_ERB_share\n",
    "q1_total_share = 0\n",
    "for column in share_columns:\n",
    "    if column != \"COVID_ERB\":  # Exclude q1_COVID_ERB_share\n",
    "        q1_total_share += globals().get(f'q1_{column}_share', 0)\n",
    "\n",
    "# Sum all q2 shares excluding q2_COVID_ERB_share\n",
    "q2_total_share = 0\n",
    "for column in share_columns:\n",
    "    if column != \"COVID_ERB\":  # Exclude q2_COVID_ERB_share\n",
    "        q2_total_share += globals().get(f'q2_{column}_share', 0)\n",
    "\n",
    "# Sum all q3 shares excluding q3_COVID_ERB_share\n",
    "q3_total_share = 0\n",
    "for column in share_columns:\n",
    "    if column != \"COVID_ERB\":  # Exclude q3_COVID_ERB_share\n",
    "        q3_total_share += globals().get(f'q3_{column}_share', 0)\n",
    "\n",
    "\n",
    "# Print the total sums for q1 and q2 and q3 (excluding COVID_ERB share)\n",
    "print(f\"Total q1 share (excluding COVID_ERB_share): {q1_total_share:.4f}\")\n",
    "print(f\"Total q2 share (excluding COVID_ERB_share): {q2_total_share:.4f}\")\n",
    "print(f\"Total q3 share (excluding COVID_ERB_share): {q3_total_share:.4f}\")\n",
    "\n",
    "# Define and normalize q1 and q2 share values\n",
    "for column in share_columns:\n",
    "    if column != \"COVID_ERB\":\n",
    "        # Get the share values for q1 and q2 and q3\n",
    "        q1_share = globals().get(f'q1_{column}_share', 0)\n",
    "        q2_share = globals().get(f'q2_{column}_share', 0)\n",
    "        q3_share = globals().get(f'q3_{column}_share', 0)\n",
    "        \n",
    "        # Calculate the total for q1 and q2 and q3 (excluding COVID_ERB share)\n",
    "        q1_total_share = sum(globals().get(f'q1_{col}_share', 0) for col in share_columns if col != \"COVID_ERB\")\n",
    "        q2_total_share = sum(globals().get(f'q2_{col}_share', 0) for col in share_columns if col != \"COVID_ERB\")\n",
    "        q3_total_share = sum(globals().get(f'q3_{col}_share', 0) for col in share_columns if col != \"COVID_ERB\")\n",
    "        \n",
    "        # Normalize the share values\n",
    "        q1_normalized_value = q1_share / q1_total_share if q1_total_share != 0 else 0\n",
    "        q2_normalized_value = q2_share / q2_total_share if q2_total_share != 0 else 0\n",
    "        q3_normalized_value = q3_share / q3_total_share if q3_total_share != 0 else 0\n",
    "        \n",
    "        # Store the normalized share values in the globals() dictionary\n",
    "        globals()[f'q1_{column}_share_n'] = q1_normalized_value\n",
    "        globals()[f'q2_{column}_share_n'] = q2_normalized_value\n",
    "        globals()[f'q3_{column}_share_n'] = q3_normalized_value\n",
    "        \n",
    "        # Print the normalized share values\n",
    "        print(f\"q1_{column}_share_n: {q1_normalized_value:.4f}\")\n",
    "        print(f\"q2_{column}_share_n: {q2_normalized_value:.4f}\")\n",
    "        print(f\"q3_{column}_share_n: {q3_normalized_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate 2021 Core Housing Need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Aggregate total household income\n",
    "household_income2 = census2021.groupby('HH_ID')['totalincome'].sum().reset_index()\n",
    "\n",
    "# Step 2: Merge back into census2021\n",
    "census2021 = census2021.merge(household_income2, on='HH_ID', suffixes=('', '_hh'))\n",
    "\n",
    "\n",
    "# identify households where every preson in unrelated or living alone\n",
    "household_non_family = census2021.groupby('HH_ID')['CFSTAT'].apply(lambda x: set(x).issubset({6, 7})).reset_index()\n",
    "household_non_family.columns = ['HH_ID', 'non_family_household']\n",
    "household_non_family['non_family_household'] = household_non_family['non_family_household'].astype(int)\n",
    "\n",
    "#merge back into census2021\n",
    "census2021 = census2021.merge(household_non_family, on='HH_ID', how='left')\n",
    "\n",
    "#Identify households where at least one maintainer (HHMAINP == 1) is aged 15-29 and attending school\n",
    "maintainers = census2021[(census2021['HHMAINP'] == 1) & \n",
    "                         (census2021['AGEGRP'].isin([3, 4, 5])) & \n",
    "                         (census2021['ATTSCH'] == 1)]\n",
    "\n",
    "excluded_households = maintainers['HH_ID'].unique()\n",
    "census2021['student_household'] = census2021['HH_ID'].isin(excluded_households).astype(int) # 1 if student, 0 otherwise\n",
    "\n",
    "# Step 3: Define Core Housing Need (CHN)\n",
    "census2021['chn'] = 0  # Default to 0 (not in CHN)\n",
    "\n",
    "# Condition: Housing is unaffordable OR unsuitable OR inadequate\n",
    "housing_issue = (\n",
    "    (census2021['SHELCO'] * 12 / census2021['totalincome_hh'] > 0.30) |  # Unaffordable\n",
    "    (census2021['NOS'] == 0) |  # Unsuitable\n",
    "    (census2021['REPAIR'] == 3)  # Inadequate\n",
    ")\n",
    "\n",
    "# Condition: Alternative market rent is also unaffordable\n",
    "market_unaffordable = (census2021['mmr']) * 12 > 0.30 * census2021['totalincome_hh']\n",
    "\n",
    "# Set Core Housing Need (CHN) variable, excluding non-family households with an eligible maintainer\n",
    "census2021.loc[housing_issue & market_unaffordable & \n",
    "               ~((census2021['student_household'] == 1) & (census2021['non_family_household'] == 1)), \n",
    "               'chn'] = 1\n",
    "\n",
    "\n",
    "# Create STIR (Shelter Cost to Income Ratio)\n",
    "census2021[\"stir\"] = (census2021[\"SHELCO\"] * 12) / census2021[\"totalincome_hh\"]\n",
    "\n",
    "# Create ALTSTIR (Alternative STIR using AMR)\n",
    "census2021[\"altstir\"] = (census2021[\"mmr\"] * 12) / census2021[\"totalincome_hh\"]\n",
    "\n",
    "# Update CHN: Exclude individuals with STIR >= 1\n",
    "census2021.loc[census2021[\"stir\"] >= 1, \"chn\"] = 0\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Define Deep Core Housing Need (DCHN)\n",
    "census2021['dchn'] = 0  # Default to 0\n",
    "\n",
    "deep_housing_issue = (\n",
    "    (census2021['SHELCO'] * 12 / census2021['totalincome_hh'] > 0.50) |  # Deeply unaffordable\n",
    "    (census2021['NOS'] == 0) |  # Unsuitable\n",
    "    (census2021['REPAIR'] == 3)  # Inadequate\n",
    ")\n",
    "\n",
    "deep_market_unaffordable = (census2021['mmr']) * 12 > 0.50 * census2021['totalincome_hh']\n",
    "\n",
    "# Apply DCHN flag with same exclusions\n",
    "census2021.loc[deep_housing_issue & deep_market_unaffordable &\n",
    "               ~((census2021['student_household'] == 1) & (census2021['non_family_household'] == 1)), \n",
    "               'dchn'] = 1\n",
    "\n",
    "# Update DCHN: Exclude individuals with STIR >= 1\n",
    "census2021.loc[census2021[\"stir\"] >= 1, \"dchn\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census2022 = census2021.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.dirname(current_dir)\n",
    "\n",
    "# Load the population projection file\n",
    "file_path_pop = os.path.join(data_dir, \"popproj.xlsx\")\n",
    "popproj = pd.read_excel(file_path_pop)\n",
    "\n",
    "# Filter to 2022 growth rates only\n",
    "growth_rates_2022 = popproj[['demo', 'agegrp', 'econ', 2022]]\n",
    "\n",
    "# Create a lookup dictionary: {(agegrp, demo, econ): growth_rate}\n",
    "growth_rate_lookup = {\n",
    "    (row['agegrp'], row['demo'], row['econ']): row[2022]\n",
    "    for _, row in growth_rates_2022.iterrows()\n",
    "}\n",
    "\n",
    "# Define the AGEGRP mapping from numbers to labels\n",
    "agegrp_mapping = {\n",
    "    1: '0to9',\n",
    "    2: '10to14',\n",
    "    3: '15to19',\n",
    "    4: '20to24',\n",
    "    5: '25to29',\n",
    "    6: '30to34',\n",
    "    7: '35to39',\n",
    "    8: '40to44',\n",
    "    9: '45to49',\n",
    "    10: '50to54',\n",
    "    11: '55to64',\n",
    "    12: '65to74',\n",
    "    13: '75plus',\n",
    "    88: \"total\"\n",
    "}\n",
    "\n",
    "# Function to apply growth rates based on AGEGRP, IMMSTAT, and ECON\n",
    "def apply_growth(row):\n",
    "    agegrp_code = row['AGEGRP']\n",
    "    immstat = row['IMMSTAT']\n",
    "    econ = row['econ']\n",
    "\n",
    "    # Map AGEGRP code to string label\n",
    "    agegrp_label = agegrp_mapping.get(agegrp_code, None)\n",
    "    \n",
    "    if agegrp_label is None:\n",
    "        print(f\"Warning: Unknown AGEGRP code {agegrp_code} in row {row.name}\")\n",
    "        return row  # Skip updating this row\n",
    "\n",
    "    # Determine if the person is NPR or non-NPR based on IMMSTAT\n",
    "    demo = 'npr' if immstat == 3 else 'nonnpr'\n",
    "    \n",
    "    # Lookup the growth rate, defaulting to 1 if not found\n",
    "    growth_rate = growth_rate_lookup.get((agegrp_label, demo, econ), 1)\n",
    "    \n",
    "    # Apply the growth rate\n",
    "    row['WEIGHT'] *= (1 + growth_rate)\n",
    "    return row\n",
    "\n",
    "# Apply the updated growth function to census2022\n",
    "census2022 = census2022.apply(apply_growth, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "summary statistics:\n",
    "income quintile ranges\n",
    "number of households per quintile\n",
    "number of household in CHN per quintile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the min and max household income for each quintile\n",
    "print(household_income.groupby(\"quintile\")[\"totalincome\"].agg([\"min\", \"max\"]))\n",
    "\n",
    "# Step 1: Aggregate weights at the household level\n",
    "household_weights = census2021.groupby(\"HH_ID\")[\"WEIGHT\"].first().reset_index()\n",
    "\n",
    "# Step 2: Merge back the quintile information\n",
    "household_weights = household_weights.merge(\n",
    "    household_income[[\"HH_ID\", \"quintile\"]], on=\"HH_ID\", how=\"left\"\n",
    ")\n",
    "\n",
    "# Step 3: Compute weighted household counts by quintile\n",
    "household_weighted_counts = household_weights.groupby(\"quintile\")[\"WEIGHT\"].sum()\n",
    "\n",
    "# Print results with comma separators\n",
    "print(household_weighted_counts.apply(lambda x: f\"{x:,.0f}\"))\n",
    "\n",
    "# Sum total weights across all households\n",
    "total_weight = household_weighted_counts.sum()\n",
    "print(f\"Total households (weighted): {total_weight:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep one row per household in core housing need\n",
    "core_housing_need_households = (\n",
    "    census2021[census2021[\"HCORENEED_IND\"] == 100]\n",
    "    .drop_duplicates(subset=\"HH_ID\")\n",
    ")\n",
    "\n",
    "# Sum of household weights by quintile\n",
    "core_housing_need_weighted = (\n",
    "    core_housing_need_households.groupby(\"quintile\")[\"WEIGHT\"]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "# Format numbers with comma separators\n",
    "print(core_housing_need_weighted.apply(lambda x: f\"{x:,.0f}\"))\n",
    "\n",
    "#print sum of households in core housing need\n",
    "total_core_housing_need = core_housing_need_weighted.sum()\n",
    "print(total_core_housing_need)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grow market income for 2022 based on Canada quintile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_mrkinc(row):\n",
    "    if row['quintile'] == 1:\n",
    "        return row['MRKINC'] * (1 + growth_rates['canada_compq1_2022'])\n",
    "    elif row['quintile'] == 2:\n",
    "        return row['MRKINC'] * (1 + growth_rates['canada_compq2_2022'])\n",
    "    elif row['quintile'] == 3:\n",
    "        return row['MRKINC'] * (1 + growth_rates['canada_compq3_2022'])\n",
    "    elif row['quintile'] == 4:\n",
    "        return row['MRKINC'] * (1 + growth_rates['canada_compq4_2022'])\n",
    "    elif row['quintile'] == 5:\n",
    "        return row['MRKINC'] * (1 + growth_rates['canada_compq5_2022'])\n",
    "    else:\n",
    "        return row['MRKINC']  # Default case if quintile is missing\n",
    "\n",
    "# Apply function row-wise\n",
    "census2022.loc[:, 'MRKINC'] = census2022.apply(adjust_mrkinc, axis=1).astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grow transfer income for 2022 based on Canada quintile data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gtrfs(row):\n",
    "    if row['quintile'] == 1:\n",
    "        return row['GTRFS'] * (1 + growth_rates['canada_tranrq1_2022'])\n",
    "    elif row['quintile'] == 2:\n",
    "        return row['GTRFS'] * (1 + growth_rates['canada_tranrq2_2022'])\n",
    "    elif row['quintile'] == 3:\n",
    "        return row['GTRFS'] * (1 + growth_rates['canada_tranrq3_2022'])\n",
    "    else:\n",
    "        return row['GTRFS']  # Default case if quintile is missing\n",
    "\n",
    "# Apply function row-wise\n",
    "census2022.loc[:, 'GTRFS'] = census2022.apply(adjust_gtrfs, axis=1).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new variable for total income (after growing MRKINC and GTRFS)\n",
    "census2022[\"totalincome\"] = census2022[\"MRKINC\"] + census2022[\"GTRFS\"]\n",
    "census2022[\"totaltransfers\"] = census2022[\"GTRFS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted mean function\n",
    "def weighted_mean(x, w):\n",
    "    return np.sum(x * w) / np.sum(w)\n",
    "\n",
    "# Group by quintile and compute weighted average of totalincome\n",
    "avg_income_by_quintile = (\n",
    "    census2022\n",
    "    .groupby(\"quintile\")\n",
    "    .apply(lambda g: weighted_mean(g[\"totalincome\"], g[\"WEIGHT\"]))\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for nicer display\n",
    "avg_income_by_quintile = avg_income_by_quintile.reset_index(name=\"avg_totalincome\")\n",
    "\n",
    "print(avg_income_by_quintile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grow 2022 shelco based on tenur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(census2022[['SHELCO', 'mmr']].head()) #check values before growing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.dirname(current_dir)\n",
    "\n",
    "\n",
    "file_path_growth = os.path.join(data_dir, \"growthrates.xlsx\")\n",
    "growth = pd.read_excel(file_path_growth)\n",
    "\n",
    "print(growth.head())\n",
    "\n",
    "#increase shelco variable by the 2022 rent variable in the growth df, if tenur is 2, increase by the mortgage variable if tenur is 1\n",
    "\n",
    "year_to_use = 2022 \n",
    "\n",
    "def adjust_shelco(row):\n",
    "    rent = growth.loc[growth['year'] == year_to_use, 'rent'].values[0]\n",
    "    mortgage = growth.loc[growth['year'] == year_to_use, 'mortgage'].values[0]\n",
    "    othercosts = growth.loc[growth['year'] == year_to_use, 'othercosts'].values[0]\n",
    "\n",
    "    if row['TENUR'] == 2:  # Renters\n",
    "        return row['SHELCO'] * (1 + rent)\n",
    "    elif row['TENUR'] == 1:\n",
    "        if row['PRESMORTG'] in [1, 8]:  # Mortgage holders\n",
    "            return row['SHELCO'] * (1 + mortgage)\n",
    "        elif row['PRESMORTG'] == 0:  # No mortgage, use othercosts\n",
    "            return row['SHELCO'] * (1 + othercosts)\n",
    "    \n",
    "    # Default case\n",
    "    return row['SHELCO'] * (1 + rent)\n",
    "#assigns unknown TENUR to renters\n",
    "\n",
    "# Apply\n",
    "census2022['SHELCO'] = census2022.apply(adjust_shelco, axis=1)\n",
    "print(census2022[['SHELCO', 'mmr']].head())  # Check values after growing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase mmr variable by the 2022 rent variable in the growth df\n",
    "census2022['mmr'] = census2022['mmr']*(1+growth.loc[growth['year'] == year_to_use, 'rent'].values[0])\n",
    "print(census2022[['SHELCO', 'mmr']].head()) #check values after growing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create 2023 census df, apply market and transfer income growth rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating 2023 microsimulation\n",
    "census2023 = census2022.copy()\n",
    "\n",
    "# Verify the result\n",
    "print(census2023[[\"HH_ID\", \"PP_ID\", \"TOTINC\", \"quintile\"]].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj weights for pop growth\n",
    "# Filter to 2023 growth rates, now including 'econ'\n",
    "growth_rates_2023 = popproj[['demo', 'agegrp', 'econ', 2023]]\n",
    "\n",
    "# Create the 2023 growth rate lookup dictionary\n",
    "growth_rate_lookup_2023 = {\n",
    "    (row['agegrp'], row['demo'], row['econ']): row[2023]\n",
    "    for _, row in growth_rates_2023.iterrows()\n",
    "}\n",
    "\n",
    "# Apply the 2023 growth rates\n",
    "def apply_growth_2023(row):\n",
    "    agegrp_code = row['AGEGRP']\n",
    "    immstat = row['IMMSTAT']\n",
    "    econ = row['econ']  # Include ECON in the logic\n",
    "\n",
    "    agegrp_label = agegrp_mapping.get(agegrp_code, None)\n",
    "    \n",
    "    if agegrp_label is None:\n",
    "        print(f\"Warning: Unknown AGEGRP code {agegrp_code} in row {row.name}\")\n",
    "        return row  # Skip updating this row\n",
    "\n",
    "    demo = 'npr' if immstat == 3 else 'nonnpr'\n",
    "    \n",
    "    # Lookup the growth rate, defaulting to 1 if not found\n",
    "    growth_rate = growth_rate_lookup_2023.get((agegrp_label, demo, econ), 1)\n",
    "    \n",
    "    row['WEIGHT'] *= (1 + growth_rate)\n",
    "    return row\n",
    "\n",
    "# Apply the updated function to census2023\n",
    "census2023 = census2023.apply(apply_growth_2023, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_mrkinc(row):\n",
    "    if row['quintile'] == 1:\n",
    "        return row['MRKINC'] * (1 + growth_rates['canada_compq1_2023'])\n",
    "    elif row['quintile'] == 2:\n",
    "        return row['MRKINC'] * (1 + growth_rates['canada_compq2_2023'])\n",
    "    elif row['quintile'] == 3:\n",
    "        return row['MRKINC'] * (1 + growth_rates['canada_compq3_2023'])\n",
    "    elif row['quintile'] == 4:\n",
    "        return row['MRKINC'] * (1 + growth_rates['canada_compq4_2023'])\n",
    "    elif row['quintile'] == 5:\n",
    "        return row['MRKINC'] * (1 + growth_rates['canada_compq5_2023'])\n",
    "    else:\n",
    "        return row['MRKINC']  # Default case if quintile is missing\n",
    "\n",
    "# Apply function row-wise\n",
    "census2023.loc[:, 'MRKINC'] = census2023.apply(adjust_mrkinc, axis=1).astype('float64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gtrfs(row):\n",
    "    if row['quintile'] == 1:\n",
    "        return row['GTRFS'] * (1 + growth_rates['canada_tranrq1_2023'])\n",
    "    elif row['quintile'] == 2:\n",
    "        return row['GTRFS'] * (1 + growth_rates['canada_tranrq2_2023'])\n",
    "    elif row['quintile'] == 3:\n",
    "        return row['GTRFS'] * (1 + growth_rates['canada_tranrq3_2023'])\n",
    "    elif row['quintile'] == 4:\n",
    "        return row['GTRFS'] * (1 + growth_rates['canada_tranrq4_2023'])\n",
    "    elif row['quintile'] == 5:\n",
    "        return row['GTRFS'] * (1 + growth_rates['canada_tranrq5_2023'])\n",
    "    else:\n",
    "        return row['GTRFS']  # Default case if quintile is missing\n",
    "\n",
    "# Apply function row-wise\n",
    "census2023.loc[:, 'GTRFS'] = census2023.apply(adjust_gtrfs, axis=1).astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new variable for total income (after growing MRKINC and GTRFS)\n",
    "census2023[\"totalincome\"] = census2023[\"MRKINC\"] + census2023[\"GTRFS\"]\n",
    "census2023[\"totaltransfers\"] = census2023[\"GTRFS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 2023 census, split out transfer income into subcategories, using shares calculated earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target columns and their corresponding share names\n",
    "share_mapping = {\n",
    "    \"cpp\": \"CQPPB_share_n\",\n",
    "    \"ei\": \"EICBN_share_n\",\n",
    "    \"oas\": \"OASGI_share_n\",\n",
    "    \"child\": \"CHDBN_share_n\",\n",
    "    \"social\": \"other_share_n\"\n",
    "}\n",
    "\n",
    "# Loop over quintiles 1 to 3\n",
    "for quintile in [1, 2, 3]:\n",
    "    for column, share_name in share_mapping.items():\n",
    "        # Build the variable name, e.g., 'q1_CQPPB_share_n'\n",
    "        variable_name = f'q{quintile}_{share_name}'\n",
    "        share_value = globals().get(variable_name, None)\n",
    "        \n",
    "        if share_value is None:\n",
    "            print(f\"Error: {variable_name} is not defined.\")\n",
    "        else:\n",
    "            census2023.loc[census2023[\"quintile\"] == quintile, column] = share_value * census2023[\"GTRFS\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update shelco & mmr for 2023\n",
    "\n",
    "year_to_use = 2023  \n",
    "\n",
    "census2023['SHELCO'] = census2023.apply(adjust_shelco, axis=1)\n",
    "census2023['mmr'] = census2023['mmr']*(1+growth.loc[growth['year'] == year_to_use, 'rent'].values[0])\n",
    "print(census2023[['SHELCO', 'mmr']].head())  # Check values after growing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Microsimulations for 2021-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Census data (2021 to 2023) to .csv\n",
    "folder_path = \"../Microsimulations\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "census_years = [2021, 2022, 2023]\n",
    "\n",
    "# Loop through each year and save the corresponding census dataset\n",
    "for year in census_years:\n",
    "    # Dynamically create the census variable name based on the year\n",
    "    census = globals().get(f\"census{year}\", None)\n",
    "    \n",
    "    if census is not None:\n",
    "        # Create the full path for saving the CSV file\n",
    "        csv_filename = f\"{folder_path}/census{year}.csv\"  # Combine folder path and filename\n",
    "\n",
    "        # Save the transformed dataset for the current year to a CSV file\n",
    "        census.to_csv(csv_filename, index=False)  # Save to CSV (without the index)\n",
    "        print(f\"Saved census data for {year} to {csv_filename}\")\n",
    "    else:\n",
    "        print(f\"Dataset for {year} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024 Onwards Growth Rates and Microsimulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = pd.read_excel('..\\\\income_growthrates.xlsx')\n",
    "\n",
    "#print all forecast df values\n",
    "print(forecast)\n",
    "\n",
    "\n",
    "# Creating global object that can be referenced later\n",
    "for index, row in forecast.iterrows():\n",
    "    globals()[row['share']] = row['value'] # Assign each row's value to a global variable with the corresponding name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popproj = pd.read_excel(file_path_pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lookup dictionary for each year's growth rates, considering ECON\n",
    "growth_rate_lookups = {}\n",
    "\n",
    "for year in range(2022, 2031):\n",
    "    growth_rates_year = popproj[['demo', 'agegrp', 'econ', year]]\n",
    "    growth_rate_lookups[year] = {\n",
    "        (row['agegrp'], row['demo'], row['econ']): row[year]\n",
    "        for _, row in growth_rates_year.iterrows()\n",
    "    }\n",
    "\n",
    "def apply_population_growth(row, year):\n",
    "    agegrp_code = row['AGEGRP']\n",
    "    immstat = row['IMMSTAT']\n",
    "    econ = row['econ']  # Ensure ECON is included in calculations\n",
    "    \n",
    "    # Map AGEGRP code to string label\n",
    "    agegrp_label = agegrp_mapping.get(agegrp_code, None)\n",
    "    \n",
    "    if agegrp_label is None:\n",
    "        print(f\"Warning: Unknown AGEGRP code {agegrp_code} in row {row.name}\")\n",
    "        return row  # Leave the row unchanged if AGEGRP is invalid\n",
    "\n",
    "    demo = 'npr' if immstat == 3 else 'nonnpr'\n",
    "    \n",
    "    # Get the growth rate for the year, defaulting to 0% (no change) if missing\n",
    "    growth_rate = growth_rate_lookups[year].get((agegrp_label, demo, econ), 0)\n",
    "    \n",
    "    # Apply the growth rate\n",
    "    row['WEIGHT'] *= (1 + growth_rate)\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years to loop over\n",
    "years = range(2024, 2031)  # Covers 2024 to 2030\n",
    "\n",
    "# Create a dictionary to store census data for each year\n",
    "census= {\"2023\": census2023} #start with 2023 census data\n",
    "weighted_sum_census = {}  # Dictionary to store weighted sum of MRKINC for each year\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    # Initialize the new year's census data by copying the previous year's data\n",
    "    prev_year = str(year - 1)\n",
    "    curr_year = str(year)\n",
    "    census[curr_year] = census[prev_year].copy()\n",
    "\n",
    "    census[curr_year] = census[curr_year].apply(lambda row: apply_population_growth(row, year), axis=1)\n",
    "\n",
    "    year_to_use = year\n",
    "    census[curr_year][\"SHELCO\"] = census[curr_year].apply(adjust_shelco, axis=1)\n",
    "    census[curr_year]['mmr'] = census[curr_year]['mmr']*(1+growth.loc[growth['year'] == year_to_use, 'rent'].values[0])\n",
    "\n",
    "    # Grow transfer income\n",
    "    census[curr_year][\"ei\"] *= (1 + globals()[f\"ei_{curr_year}g\"])\n",
    "    census[curr_year][\"child\"] *= (1 + globals()[f\"child_{curr_year}g\"])\n",
    "    census[curr_year][\"social\"] *= (1 + globals()[f\"social_{curr_year}g\"])\n",
    "    census[curr_year][\"cpp\"] *= (1 + globals()[f\"cpp_{curr_year}g\"])\n",
    "    census[curr_year][\"oas\"] *= (1 + globals()[f\"oas_{curr_year}g\"])\n",
    "\n",
    "    # Compute total transfers for the year\n",
    "    census[curr_year][f\"totaltransfers\"] = (\n",
    "        census[curr_year][\"ei\"] +\n",
    "        census[curr_year][\"child\"] +\n",
    "        census[curr_year][\"social\"] +\n",
    "        census[curr_year][\"cpp\"] +\n",
    "        census[curr_year][\"oas\"]\n",
    "    )\n",
    "\n",
    "    # Grow market income by Canada-wide shares of total income growth for each quintile\n",
    "    #canada_compq1 = 0.7834 * globals()[f\"comp_{curr_year}g\"]\n",
    "    #canada_compq2 = 0.9956 * globals()[f\"comp_{curr_year}g\"]\n",
    "    #canada_compq3 = 1.0034* globals()[f\"comp_{curr_year}g\"]\n",
    "\n",
    "    canada_compq1 = 1 * globals()[f\"comp_{curr_year}g\"]\n",
    "    canada_compq2 = 1 * globals()[f\"comp_{curr_year}g\"]\n",
    "    canada_compq3 = 1* globals()[f\"comp_{curr_year}g\"]\n",
    "\n",
    "\n",
    "\n",
    "    census[curr_year].loc[census[curr_year][\"quintile\"] == 1, \"MRKINC\"] *= (1 + canada_compq1)\n",
    "    census[curr_year].loc[census[curr_year][\"quintile\"] == 2, \"MRKINC\"] *= (1 + canada_compq2)\n",
    "    census[curr_year].loc[census[curr_year][\"quintile\"] == 3, \"MRKINC\"] *= (1 + canada_compq3)\n",
    "    \n",
    "    #create total income variable = transfer income + market income\n",
    "    census[curr_year][\"totalincome\"] = census[curr_year][\"MRKINC\"] + census[curr_year][\"totaltransfers\"]\n",
    "\n",
    "    # Create the full path for saving the CSV file\n",
    "    csv_filename = f\"{folder_path}/census{curr_year}.csv\"  # Combine folder path and filename\n",
    "\n",
    "    # Save the transformed dataset for the current year to a CSV file\n",
    "    census[curr_year].to_csv(csv_filename, index=False)  # Save to CSV (without the index)\n",
    "    print(f\"Saved census data for {curr_year} to {csv_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check weights were applied properly for first record\n",
    "\n",
    "\n",
    "#print the variables IMMSTAT, AGEGRP, WEIGHT for the first each record in each censusdf then calculate the percent change in the printed WEIGHT value\n",
    "for year, df in census.items():\n",
    "    print(f\"Year: {year}\")\n",
    "    print(df[['IMMSTAT', 'AGEGRP', 'WEIGHT']].head(1))\n",
    "\n",
    "#create an array out of each printed WEIGHT value from the previous loop\n",
    "weights = [df['WEIGHT'].values[0] for df in census.values()]\n",
    "\n",
    "\n",
    "#print weights array\n",
    "print(weights)\n",
    "\n",
    "#calculate the percent change in the values in the weights array\n",
    "percent_change = [(weights[i] - weights[i - 1]) / weights[i - 1] * 100 for i in range(1, len(weights))]\n",
    "print(percent_change)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create trace file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years you want to include\n",
    "years = range(2021, 2031)\n",
    "\n",
    "# List to hold second rows\n",
    "second_rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = f\"../Microsimulations/census{year}.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Read the full CSV and select the second row (index 1)\n",
    "        second_row = pd.read_csv(file_path).iloc[[0]].copy()\n",
    "        \n",
    "        # Add a 'year' column\n",
    "        second_row['year'] = year\n",
    "        \n",
    "        # Append to the list\n",
    "        second_rows.append(second_row)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for year {year}: {file_path}\")\n",
    "        continue\n",
    "    except IndexError:\n",
    "        print(f\"File {file_path} does not have a second row.\")\n",
    "        continue\n",
    "\n",
    "# Combine all second rows into one DataFrame\n",
    "summary_df = pd.concat(second_rows, ignore_index=True)\n",
    "\n",
    "# Define the columns you want to keep\n",
    "columns_to_keep = [\n",
    "    'year', 'HH_ID', 'PP_ID', 'SHELCO', 'TENUR', 'PRESMORTG', 'AGEGRP' 'quintile', \n",
    "    'totalincome', 'totaltransfers', 'social', 'cpp', \n",
    "    'ei', 'oas', 'child', 'MRKINC', 'GTRFS'\n",
    "]\n",
    "\n",
    "# Keep only the desired columns (ignore missing ones)\n",
    "summary_df = summary_df[[col for col in columns_to_keep if col in summary_df.columns]]\n",
    "\n",
    "# Set 'year' as the index\n",
    "summary_df.set_index('year', inplace=True)\n",
    "\n",
    "# Transpose the DataFrame\n",
    "summary_transposed = summary_df.transpose()\n",
    "\n",
    "# Save the transposed DataFrame to Excel\n",
    "output_path = \"../Microsimulations/census_trace.xlsx\"\n",
    "summary_transposed.to_excel(output_path)\n",
    "\n",
    "print(f\"Created transposed summary Excel file with years as columns at {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define input and output folders\n",
    "input_folder = \"../Microsimulations\"\n",
    "output_folder = \"../Microsimulations/household\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define years for processing\n",
    "years = range(2021, 2031)  # Covers 2023 to 2030\n",
    "\n",
    "# Loop through each year's census simulation file\n",
    "for year in years:\n",
    "    input_file = f\"{input_folder}/census{year}.csv\"\n",
    "    output_file = f\"{output_folder}/census{year}_household.csv\"\n",
    "    \n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Skipping {year}: File not found -> {input_file}\")\n",
    "        continue\n",
    "\n",
    "    # Load individual-level census data\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Compute average WEIGHT for adults (AGEGRP > 2)\n",
    "    adult_weights = df[df[\"AGEGRP\"] > 2].groupby(\"HH_ID\")[\"WEIGHT\"].mean().reset_index()\n",
    "    adult_weights.rename(columns={\"WEIGHT\": \"avg_adult_weight\"}, inplace=True)\n",
    "\n",
    "    # Group by HH_ID and aggregate\n",
    "    household_df = df.groupby(\"HH_ID\").agg({\n",
    "        \"BEDRM\": \"first\",\n",
    "        \"HCORENEED_IND\": \"first\",\n",
    "        \"REPAIR\": \"first\",\n",
    "        \"NOS\": \"first\",\n",
    "        \"PRESMORTG\": \"first\",\n",
    "        \"SHELCO\": \"first\",\n",
    "        \"TENUR\": \"first\",\n",
    "        \"SUBSIDY\": \"first\",\n",
    "        \"quintile\": \"first\",\n",
    "        \"totalincome\": \"sum\",\n",
    "        \"MRKINC\": \"sum\",\n",
    "        \"TOTINC_AT\": \"sum\",\n",
    "        \"totaltransfers\": \"sum\",\n",
    "        \"mmr\":\"first\",\n",
    "        \"student_household\": \"first\",\n",
    "        \"non_family_household\": \"first\",\n",
    "        \"chn\":\"first\",\n",
    "         \"dchn\":\"first\",\n",
    "        \"bedsuit\":\"first\",\n",
    "        \"stir\" : \"first\"\n",
    "    }).reset_index()\n",
    "\n",
    "   # Merge average adult weight into the household-level dataframe\n",
    "    household_df = household_df.merge(adult_weights, on=\"HH_ID\", how=\"left\")\n",
    "\n",
    "   # Rename the merged column to WEIGHT\n",
    "    household_df.rename(columns={\"avg_adult_weight\": \"WEIGHT\"}, inplace=True)\n",
    "\n",
    "    # Save the transformed household-level data\n",
    "    household_df.to_csv(output_file, index=False)\n",
    "\n",
    "    ###To do: gross up weights to align with FAO household projection###\n",
    "    \n",
    "    print(f\"Saved household-level simulation for {year} -> {output_file}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years you want to include\n",
    "years = range(2021, 2031)\n",
    "\n",
    "# List to hold second rows\n",
    "second_rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_path = f\"../Microsimulations/household/census{year}_household.csv\"\n",
    "    \n",
    "    try:\n",
    "        # Read the full CSV and select the second row (index 1)\n",
    "        second_row = pd.read_csv(file_path).iloc[[0]].copy()\n",
    "        \n",
    "        # Add a 'year' column\n",
    "        second_row['year'] = year\n",
    "        \n",
    "        # Append to the list\n",
    "        second_rows.append(second_row)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for year {year}: {file_path}\")\n",
    "        continue\n",
    "    except IndexError:\n",
    "        print(f\"File {file_path} does not have a second row.\")\n",
    "        continue\n",
    "\n",
    "# Combine all second rows into one DataFrame\n",
    "summary_df = pd.concat(second_rows, ignore_index=True)\n",
    "\n",
    "# Define the columns you want to keep\n",
    "columns_to_keep = [\n",
    "    'year', 'HH_ID','SHELCO', 'NOS', 'REPAIR', 'BEDRM', 'TENUR', 'PRESMORTG', 'AGEGRP' 'quintile', \n",
    "    'totalincome', 'totaltransfers', 'MRKINC', 'GTRFS'\n",
    "]\n",
    "\n",
    "# Keep only the desired columns (ignore missing ones)\n",
    "summary_df = summary_df[[col for col in columns_to_keep if col in summary_df.columns]]\n",
    "\n",
    "# Set 'year' as the index\n",
    "summary_df.set_index('year', inplace=True)\n",
    "\n",
    "# Transpose the DataFrame\n",
    "summary_transposed = summary_df.transpose()\n",
    "\n",
    "# Save the transposed DataFrame to Excel\n",
    "output_path = \"../Microsimulations/household/census_trace_hh.xlsx\"\n",
    "summary_transposed.to_excel(output_path)\n",
    "\n",
    "print(f\"Created transposed summary Excel file with years as columns at {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the years and quintiles\n",
    "years = range(2021, 2031)\n",
    "quintiles = [1, 2, 3]\n",
    "\n",
    "# Dictionaries to store weighted sums and weighted counts by year and quintile\n",
    "weighted_HH_MRKINC = {year: {} for year in years}\n",
    "weighted_HH_transfers = {year: {} for year in years}\n",
    "weighted_HH_totalincome = {year: {} for year in years}  # NEW for totalincome\n",
    "weighted_HH_counts = {year: {} for year in years}  # To store sum of weights per quintile\n",
    "\n",
    "# Loop through each year and calculate weighted sums & counts by quintile\n",
    "for year in years:\n",
    "    file_path = f\"../Microsimulations/household/census{year}_household.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Ensure household-level aggregation by taking the first value per HH_ID\n",
    "        df_household = df.groupby(\"HH_ID\").agg(\n",
    "            {\n",
    "                \"MRKINC\": \"first\",\n",
    "                \"totaltransfers\": \"first\",\n",
    "                \"totalincome\": \"first\",  # NEW: Add totalincome\n",
    "                \"WEIGHT\": \"first\",\n",
    "                \"quintile\": \"first\"\n",
    "            }\n",
    "        ).reset_index()\n",
    "\n",
    "        for q in quintiles:\n",
    "            df_q = df_household[df_household[\"quintile\"] == q]\n",
    "            \n",
    "            # Calculate weighted sums\n",
    "            weighted_HH_MRKINC[year][q] = (df_q[\"MRKINC\"] * df_q[\"WEIGHT\"]).sum()\n",
    "            weighted_HH_transfers[year][q] = (df_q[\"totaltransfers\"] * df_q[\"WEIGHT\"]).sum()\n",
    "            weighted_HH_totalincome[year][q] = (df_q[\"totalincome\"] * df_q[\"WEIGHT\"]).sum()  # NEW: Total income\n",
    "            \n",
    "            # Calculate total weight (sum of weights) per quintile\n",
    "            weighted_HH_counts[year][q] = df_q[\"WEIGHT\"].sum()\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for year {year}: {file_path}\")\n",
    "        continue\n",
    "\n",
    "# Compute per-household weighted averages\n",
    "avg_HH_MRKINC = {\n",
    "    year: {q: (weighted_HH_MRKINC[year][q] / weighted_HH_counts[year][q]) if weighted_HH_counts[year][q] != 0 else None for q in quintiles}\n",
    "    for year in years\n",
    "}\n",
    "avg_HH_transfers = {\n",
    "    year: {q: (weighted_HH_transfers[year][q] / weighted_HH_counts[year][q]) if weighted_HH_counts[year][q] != 0 else None for q in quintiles}\n",
    "    for year in years\n",
    "}\n",
    "avg_HH_totalincome = {  # NEW: Compute avg for total income\n",
    "    year: {q: (weighted_HH_totalincome[year][q] / weighted_HH_counts[year][q]) if weighted_HH_counts[year][q] != 0 else None for q in quintiles}\n",
    "    for year in years\n",
    "}\n",
    "\n",
    "# Print Year-over-Year Growth for Household Market Income (MRKINC)\n",
    "print(\"\\n--- Year-over-Year Per-Household MRKINC Growth by Quintile ---\")\n",
    "for q in quintiles:\n",
    "    print(f\"\\nQuintile {q}:\")\n",
    "    for year in range(2022, 2031):  # Start from 2023 since we compare against the previous year\n",
    "        prev = avg_HH_MRKINC.get(year - 1, {}).get(q)\n",
    "        curr = avg_HH_MRKINC.get(year, {}).get(q)\n",
    "        growth = ((curr - prev) / prev) * 100 if prev is not None and prev != 0 else None\n",
    "        print(f\"Year {year} | Avg HH MRKINC YoY Growth: {growth:.2f}%\" if growth is not None else f\"Year {year} | Avg HH MRKINC YoY Growth: N/A\")\n",
    "\n",
    "# Print Year-over-Year Growth for Household Total Transfers\n",
    "print(\"\\n--- Year-over-Year Per-Household Total Transfers Growth by Quintile ---\")\n",
    "for q in quintiles:\n",
    "    print(f\"\\nQuintile {q}:\")\n",
    "    for year in range(2022, 2031):\n",
    "        prev = avg_HH_transfers.get(year - 1, {}).get(q)\n",
    "        curr = avg_HH_transfers.get(year, {}).get(q)\n",
    "        growth = ((curr - prev) / prev) * 100 if prev is not None and prev != 0 else None\n",
    "        print(f\"Year {year} | Avg HH Transfers YoY Growth: {growth:.2f}%\" if growth is not None else f\"Year {year} | Avg HH Transfers YoY Growth: N/A\")\n",
    "\n",
    "# Print Year-over-Year Growth for Household Total Income\n",
    "print(\"\\n--- Year-over-Year Per-Household Total Income Growth by Quintile ---\")\n",
    "for q in quintiles:\n",
    "    print(f\"\\nQuintile {q}:\")\n",
    "    for year in range(2022, 2031):\n",
    "        prev = avg_HH_totalincome.get(year - 1, {}).get(q)\n",
    "        curr = avg_HH_totalincome.get(year, {}).get(q)\n",
    "        growth = ((curr - prev) / prev) * 100 if prev is not None and prev != 0 else None\n",
    "        print(f\"Year {year} | Avg HH Total Income YoY Growth: {growth:.2f}%\" if growth is not None else f\"Year {year} | Avg HH Total Income YoY Growth: N/A\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the years and benefits of interest\n",
    "years = [2023, 2024, 2025]\n",
    "benefits = [\"social\", \"cpp\", \"ei\", \"oas\", \"child\", \"totaltransfers\"]\n",
    "\n",
    "# Dictionary to store weighted sums and counts per year\n",
    "weighted_sums = {year: {b: 0 for b in benefits} for year in years}\n",
    "total_weights = {year: 0 for year in years}\n",
    "\n",
    "# Process each year's data\n",
    "for year in years:\n",
    "    file_path = f\"../Microsimulations/census{year}.csv\"\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Group by household (HH_ID) and aggregate:\n",
    "        df_household = df.groupby(\"HH_ID\").agg(\n",
    "            {b: \"sum\" for b in benefits} | {\"WEIGHT\": \"first\", \"quintile\": \"first\"}  # Sum benefits, keep first weight & quintile\n",
    "        ).reset_index()\n",
    "        \n",
    "        # Filter for Quintile 1\n",
    "        df_q1 = df_household[df_household[\"quintile\"] == 1]\n",
    "        \n",
    "        # Compute weighted sums for each benefit\n",
    "        for b in benefits:\n",
    "            weighted_sums[year][b] = (df_q1[b] * df_q1[\"WEIGHT\"]).sum()\n",
    "        \n",
    "        # Compute total weight for Quintile 1\n",
    "        total_weights[year] = df_q1[\"WEIGHT\"].sum()\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found for year {year}: {file_path}\")\n",
    "        continue\n",
    "\n",
    "# Compute weighted averages\n",
    "weighted_averages = {\n",
    "    year: {b: (weighted_sums[year][b] / total_weights[year]) if total_weights[year] > 0 else None for b in benefits}\n",
    "    for year in years\n",
    "}\n",
    "\n",
    "# Print results\n",
    "print(\"\\n--- Weighted Average Household Benefit Values for Quintile 1 ---\")\n",
    "for year in years:\n",
    "    print(f\"\\nYear {year}:\")\n",
    "    for b in benefits:\n",
    "        value = weighted_averages[year][b]\n",
    "        print(f\"  {b.capitalize()} Benefit: {value:,.2f}\" if value is not None else f\"  {b.capitalize()} Benefit: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set input path and load data\n",
    "input_base_path = \"../Microsimulations/\"\n",
    "df = pd.read_csv(f\"{input_base_path}census2021.csv\")\n",
    "\n",
    "# Step 1: Flag if individual is a non-permanent resident (NPR)\n",
    "df['is_npr'] = df['IMMSTAT'] == 3\n",
    "\n",
    "# Step 2: Group by HH_ID to summarize household composition\n",
    "hh_summary = df.groupby('HH_ID')['is_npr'].agg(\n",
    "    total_members='count',\n",
    "    n_nprs='sum'\n",
    ")\n",
    "hh_summary['n_non_nprs'] = hh_summary['total_members'] - hh_summary['n_nprs']\n",
    "\n",
    "# Step 3: Classify household type\n",
    "def classify_household(row):\n",
    "    if row['n_nprs'] == row['total_members']:\n",
    "        return 'All NPRs'\n",
    "    elif row['n_nprs'] == 0:\n",
    "        return 'No NPRs'\n",
    "    else:\n",
    "        return 'Mixed'\n",
    "\n",
    "hh_summary['household_type'] = hh_summary.apply(classify_household, axis=1)\n",
    "\n",
    "# Step 4: Merge back to assign each person their household type\n",
    "df = df.merge(hh_summary[['household_type']], on='HH_ID', how='left')\n",
    "\n",
    "# Step 5: Filter to only NPRs\n",
    "npr_only = df[df['IMMSTAT'] == 3]\n",
    "\n",
    "# Step 6: Weighted count of NPRs by household type\n",
    "weighted_npr_distribution = npr_only.groupby('household_type')['WEIGHT'].sum().round(0).astype(int)\n",
    "\n",
    "# Display result\n",
    "print(\"ðŸ“Š Weighted count of NPRs by household type:\")\n",
    "print(weighted_npr_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get NPR household IDs (All NPRs or Mixed)\n",
    "npr_household_ids = hh_summary[hh_summary['household_type'].isin(['All NPRs', 'Mixed'])].index.tolist()\n",
    "\n",
    "# Convert to DataFrame\n",
    "npr_hh_df = pd.DataFrame(npr_household_ids, columns=['HH_ID'])\n",
    "\n",
    "# Export to CSV\n",
    "npr_hh_df.to_csv(\"../Microsimulations/npr_household_ids.csv\", index=False)\n",
    "\n",
    "print(\"âœ… Exported NPR household IDs to: ../Microsimulations/npr_household_ids.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
