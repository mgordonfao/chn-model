{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_dir = os.getenv(\"DATA_PATH\")\n",
    "folder_path = os.path.join(data_dir, \"Microsimulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CHN values and added stir/alt_stir for 2022\n",
      "Updated CHN values and added stir/alt_stir for 2023\n",
      "Updated CHN values and added stir/alt_stir for 2024\n",
      "Updated CHN values and added stir/alt_stir for 2025\n",
      "Updated CHN values and added stir/alt_stir for 2026\n",
      "Updated CHN values and added stir/alt_stir for 2027\n",
      "Updated CHN values and added stir/alt_stir for 2028\n",
      "Updated CHN values and added stir/alt_stir for 2029\n",
      "Updated CHN values and added stir/alt_stir for 2030\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "input_base_path = os.path.join(folder_path, \"household\")\n",
    "output_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Dictionary to store updated data\n",
    "census_data = {}\n",
    "\n",
    "# Loop through the years 2022 to 2030\n",
    "for year in range(2022, 2031):\n",
    "    input_file_path = os.path.join(input_base_path, f\"census{year}_household.csv\")\n",
    "    output_file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    \n",
    "    if os.path.exists(input_file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(input_file_path)\n",
    "        \n",
    "        # Initialize CHN column to 0\n",
    "        census_df['chn'] = 0\n",
    "\n",
    "        # Define housing issue conditions\n",
    "        housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.30) |  # Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define market unaffordability condition\n",
    "        market_unaffordable = (census_df['mmr']) * 12 > 0.30 * census_df['totalincome']\n",
    "\n",
    "        # Update CHN variable\n",
    "        census_df.loc[\n",
    "            housing_issue & market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'chn'\n",
    "        ] = 1\n",
    "\n",
    "        # Now create stir and alt_stir after chn is assigned\n",
    "       # census_df['stir'] = census_df['SHELCO'] * 12 / census_df['totalincome']\n",
    "        #census_df['alt_stir'] = (census_df['mmr']) * 12 / census_df['totalincome']\n",
    "\n",
    "\n",
    "        # Update CHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"chn\"] = 0\n",
    "        \n",
    "\n",
    "        # Define deep core housing issue condition (using 50% income threshold)\n",
    "        deep_housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.50) |  # Deeply Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define deep market unaffordability condition (50% threshold)\n",
    "        deep_market_unaffordable = (census_df['mmr']) * 12 > 0.50 * census_df['totalincome']\n",
    "\n",
    "        # Initialize dchn column to 0\n",
    "        census_df['dchn'] = 0\n",
    "\n",
    "        # Update dchn variable\n",
    "        census_df.loc[\n",
    "            deep_housing_issue & deep_market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'dchn'\n",
    "        ] = 1\n",
    "\n",
    "        # Update DCHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"dchn\"] = 0\n",
    "\n",
    "        # Export updated data\n",
    "        census_df.to_csv(output_file_path, index=False)\n",
    "        \n",
    "        print(f\"Updated CHN values and added stir/alt_stir for {year}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: C:/Users/mgordon/OneDrive - Financial Accountability Office of Ontario/FA2404 Housing and Homelessness Update/Data\\Microsimulations\\with_chn\\census2021_household_chn.csv\n"
     ]
    }
   ],
   "source": [
    "#add 2021 census file to folder\n",
    "\n",
    "# Define input and output paths\n",
    "input_base_path = os.path.join(folder_path, \"household\")\n",
    "output_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# File name\n",
    "input_file_name = \"census2021_household.csv\"\n",
    "output_file_name = \"census2021_household_chn.csv\"\n",
    "\n",
    "# Full paths\n",
    "input_file_path = os.path.join(input_base_path, input_file_name)\n",
    "output_file_path = os.path.join(output_base_path, output_file_name)\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "\n",
    "  # Now create stir and alt_stir after chn is assigned\n",
    "df['stir'] = df['SHELCO'] * 12 / df['totalincome']\n",
    "df['alt_stir'] = (df['mmr']) * 12 / df['totalincome']\n",
    "\n",
    "#net income share\n",
    "df['netshare'] = (\n",
    "    df['TOTINC_AT'] / df['totalincome']\n",
    ").clip(upper=1.0)\n",
    "\n",
    "#net income\n",
    "df['netinc'] = df['totalincome'] * df['netshare']\n",
    "\n",
    "# Save the modified dataframe\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021] CHN weighted count: 684245.5791455524, DCHN weighted count: 154141.0278820697\n",
      "[2022] CHN weighted count: 768459.7971429066, DCHN weighted count: 189153.10440888445\n",
      "[2023] CHN weighted count: 878051.8715488891, DCHN weighted count: 235515.96500592493\n",
      "[2024] CHN weighted count: 918123.9496870566, DCHN weighted count: 250291.51886131393\n",
      "[2025] CHN weighted count: 942574.9216363034, DCHN weighted count: 260233.4142954576\n",
      "[2026] CHN weighted count: 962076.6157002894, DCHN weighted count: 266639.7399537616\n",
      "[2027] CHN weighted count: 958460.4392855032, DCHN weighted count: 266588.1096230006\n",
      "[2028] CHN weighted count: 970296.3275081685, DCHN weighted count: 271314.65747217473\n",
      "[2029] CHN weighted count: 977263.0225034186, DCHN weighted count: 272381.90853394964\n",
      "[2030] CHN weighted count: 987783.0503600006, DCHN weighted count: 275454.0300173497\n",
      "\n",
      "Total household weights per year:\n",
      "Sum of WEIGHT for 2021: 3310969.362428404\n",
      "Sum of WEIGHT for 2022: 3393369.992059277\n",
      "Sum of WEIGHT for 2023: 3506218.7139442815\n",
      "Sum of WEIGHT for 2024: 3618931.0267847637\n",
      "Sum of WEIGHT for 2025: 3652464.700934248\n",
      "Sum of WEIGHT for 2026: 3680027.7488595005\n",
      "Sum of WEIGHT for 2027: 3707116.336482008\n",
      "Sum of WEIGHT for 2028: 3759582.0574440784\n",
      "Sum of WEIGHT for 2029: 3810996.4508053204\n",
      "Sum of WEIGHT for 2030: 3860604.032507048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path for processed files\n",
    "output_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Dictionaries to store weighted household counts\n",
    "chn_weighted_counts = {}\n",
    "dchn_weighted_counts = {}\n",
    "\n",
    "# Loop through years 2022 to 2030\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if required columns exist\n",
    "        required_columns = {'chn', 'dchn', 'WEIGHT', 'HCORENEED_IND'}\n",
    "        missing_columns = required_columns - set(census_df.columns)\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"Skipping {year} due to missing columns: {missing_columns}\")\n",
    "            continue  # Skip processing this file\n",
    "\n",
    "        # Exclude households where HCORENEED_IND == 888\n",
    "        filtered_df = census_df[census_df['HCORENEED_IND'] != 888]\n",
    "\n",
    "        # Calculate weighted count of households where chn == 1\n",
    "        chn_weight = filtered_df.loc[filtered_df['chn'] == 1, 'WEIGHT'].sum()\n",
    "        chn_weighted_counts[year] = chn_weight\n",
    "\n",
    "        # Calculate weighted count of households where dchn == 1\n",
    "        dchn_weight = filtered_df.loc[filtered_df['dchn'] == 1, 'WEIGHT'].sum()\n",
    "        dchn_weighted_counts[year] = dchn_weight\n",
    "\n",
    "        print(f\"[{year}] CHN weighted count: {chn_weight}, DCHN weighted count: {dchn_weight}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "print(\"\\nTotal household weights per year:\")\n",
    "\n",
    "# Print the sum of WEIGHT for each year\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "        print(f\"Sum of WEIGHT for {year}: {census_df['WEIGHT'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define year to process\n",
    "year = 2021\n",
    "file_path = os.path.join(folder_path, f\"census{year}.csv\")\n",
    "\n",
    "# Check if file exists before proceeding\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Create jobless variable\n",
    "    df[\"jobless\"] = df[\"LFACT\"].between(3, 10).astype(int)\n",
    "    \n",
    "    # Calculate share of records with jobless == 1 for each AGEGRP and IMMSTAT\n",
    "    summary = df.groupby([\"AGEGRP\", df[\"IMMSTAT\"].apply(lambda x: \"IMMSTAT_3\" if x == 3 else \"IMMSTAT_not_3\")])[\"jobless\"].mean().reset_index()\n",
    "    \n",
    "    # Save summary to a CSV file\n",
    "    output_path = os.path.join(folder_path, f\"census_share_{year}.csv\")\n",
    "    summary.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated netshare and netinc for 2022\n",
      "‚úÖ Updated netshare and netinc for 2023\n",
      "‚úÖ Updated netshare and netinc for 2024\n",
      "‚úÖ Updated netshare and netinc for 2025\n",
      "‚úÖ Updated netshare and netinc for 2026\n",
      "‚úÖ Updated netshare and netinc for 2027\n",
      "‚úÖ Updated netshare and netinc for 2028\n",
      "‚úÖ Updated netshare and netinc for 2029\n",
      "‚úÖ Updated netshare and netinc for 2030\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load netshare from 2021\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "census2021_household = pd.read_csv(os.path.join(input_base_path, \"census2021_household_chn.csv\"))\n",
    "netshare_2021 = (census2021_household['TOTINC_AT'] / census2021_household['totalincome']).clip(upper=1.0)\n",
    "\n",
    "# Apply to years 2022‚Äì2030\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "for year in range(2022, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Assign netshare from 2021 (assumes same order/row count)\n",
    "        census_df['netshare'] = netshare_2021.values\n",
    "        census_df['netinc'] = census_df['totalincome'] * census_df['netshare']\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"‚úÖ Updated netshare and netinc for {year}\")\n",
    "    else:\n",
    "        print(f\"‚ùå File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Calculated gap for 2021 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2022 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2023 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2024 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2025 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2026 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2027 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2028 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2029 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2030 using updated CHN conditions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Default gap to 0.0\n",
    "        census_df['gap'] = 0.0\n",
    "\n",
    "        # Only calculate gap where CHN = 1\n",
    "        chn_condition = census_df['chn'] == 1\n",
    "\n",
    "        # Use SHELCO if NOS == 1, REPAIR != 3, and SHELCO < mmr \n",
    "        use_shelco = (\n",
    "            chn_condition &\n",
    "            (census_df['NOS'] == 1) &\n",
    "            (census_df['REPAIR'] != 3) &\n",
    "            (census_df['SHELCO'] < census_df['mmr'])\n",
    "        )\n",
    "\n",
    "        # Use AMR (mmr) otherwise\n",
    "        use_amr = chn_condition & ~use_shelco  # CHN == 1 but doesn't meet SHELCO condition\n",
    "\n",
    "        # Apply SHELCO-based gap\n",
    "        census_df.loc[use_shelco, 'gap'] = (\n",
    "            census_df.loc[use_shelco, 'SHELCO'] * 12 - 0.3 * census_df.loc[use_shelco, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Apply AMR-based gap\n",
    "        census_df.loc[use_amr, 'gap'] = (\n",
    "            (census_df.loc[use_amr, 'mmr']) * 12 - 0.3 * census_df.loc[use_amr, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"‚úÖ Calculated gap for {year} using updated CHN conditions\")\n",
    "    else:\n",
    "        print(f\"‚ùå File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030: CHN=1 count: 8505, GAP > 0 count: 8505\n"
     ]
    }
   ],
   "source": [
    "# After assigning gaps\n",
    "print(f\"{year}: CHN=1 count: {census_df['chn'].sum()}, GAP > 0 count: {(census_df['gap'] > 0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ 2021: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 27,916\n",
      "‚úÖ Finished processing 2021\n",
      "üìÖ 2022: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 33,665\n",
      "‚úÖ Finished processing 2022\n",
      "üìÖ 2023: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 51,530\n",
      "‚úÖ Finished processing 2023\n",
      "üìÖ 2024: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 67,545\n",
      "‚úÖ Finished processing 2024\n",
      "üìÖ 2025: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 62,548\n",
      "‚úÖ Finished processing 2025\n",
      "üìÖ 2026: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 57,076\n",
      "‚úÖ Finished processing 2026\n",
      "üìÖ 2027: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 49,609\n",
      "‚úÖ Finished processing 2027\n",
      "üìÖ 2028: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 49,606\n",
      "‚úÖ Finished processing 2028\n",
      "üìÖ 2029: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 49,920\n",
      "‚úÖ Finished processing 2029\n",
      "üìÖ 2030: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 49,223\n",
      "‚úÖ Finished processing 2030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load NPR household IDs from CSV\n",
    "npr_hh_path = os.path.join(folder_path, \"npr_household_ids.csv\")\n",
    "npr_hh_df = pd.read_csv(npr_hh_path)\n",
    "npr_household_ids = npr_hh_df['HH_ID'].tolist()\n",
    "\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "for year in range(2021, 2031):  # 2021 to 2030 inclusive\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # ‚úÖ Add nprhh column based on imported NPR household IDs\n",
    "        if 'HH_ID' in census_df.columns:\n",
    "            census_df['nprhh'] = census_df['HH_ID'].isin(npr_household_ids).astype(int)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è HH_ID column not found in {year} dataset.\")\n",
    "            census_df['nprhh'] = 0\n",
    "\n",
    "        # Initialize COHB to 0.0\n",
    "        census_df['cohb'] = 0.0\n",
    "\n",
    "        # Condition: renter, in core housing need, stir > 0.3\n",
    "        condition = (\n",
    "            (census_df['TENUR'] == 2) &\n",
    "            (census_df['chn'] == 1) &\n",
    "            (census_df['stir'] > 0.3)\n",
    "        )\n",
    "\n",
    "        # Pre-calculate COHB components\n",
    "        mmr_80 = 0.8 * 12 * census_df.loc[condition, 'mmr']\n",
    "        shelco_100_capped = (12 * census_df.loc[condition, 'SHELCO']).clip(\n",
    "            upper=(12 * census_df.loc[condition, 'mmr'])\n",
    "        )\n",
    "        eligible_cost = pd.concat([mmr_80, shelco_100_capped], axis=1).max(axis=1)\n",
    "\n",
    "        netinc_30 = 0.3 * census_df.loc[condition, 'netinc']\n",
    "        cohb_values = eligible_cost - netinc_30\n",
    "\n",
    "        # Final COHB assignment with clipping\n",
    "        census_df.loc[condition, 'cohb'] = cohb_values.clip(lower=0)\n",
    "\n",
    "        # üî¢ Filter: nprhh == 1, chn == 1, and exclude HCORENEED_IND == 888\n",
    "        filter_condition = (\n",
    "            (census_df['nprhh'] == 1) &\n",
    "            (census_df['chn'] == 1) &\n",
    "            (census_df['HCORENEED_IND'] != 888)\n",
    "        )\n",
    "\n",
    "        if 'WEIGHT' in census_df.columns:\n",
    "            weighted_count = census_df.loc[filter_condition, 'WEIGHT'].sum()\n",
    "            print(f\"üìÖ {year}: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: {weighted_count:,.0f}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è WEIGHT column missing in {year} data.\")\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"‚úÖ Finished processing {year}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùå File not found for {year}: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 4401.67\n",
      "2022: 4919.80\n",
      "2023: 5625.75\n",
      "2024: 6019.27\n",
      "2025: 6287.90\n",
      "2026: 6476.86\n",
      "2027: 6539.40\n",
      "2028: 6670.99\n",
      "2029: 6723.64\n",
      "2030: 6829.38\n",
      "\n",
      "üìä Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 3783.17\n",
      "2022: 4105.45\n",
      "2023: 4540.28\n",
      "2024: 4840.74\n",
      "2025: 5028.76\n",
      "2026: 5155.58\n",
      "2027: 5230.50\n",
      "2028: 5342.70\n",
      "2029: 5409.72\n",
      "2030: 5513.75\n",
      "\n",
      "üìä Weighted Mean Income (ALL households, INCLUDING HCORENEED_IND == 888):\n",
      "2021: $59,184.27\n",
      "2022: $60,507.28\n",
      "2023: $61,719.46\n",
      "2024: $64,771.23\n",
      "2025: $66,716.81\n",
      "2026: $68,422.17\n",
      "2027: $70,485.50\n",
      "2028: $72,256.10\n",
      "2029: $74,023.55\n",
      "2030: $75,798.59\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "mean_cohb = {}\n",
    "mean_gap = {}\n",
    "mean_income = {}  # NEW dictionary to store weighted avg income\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df_full = pd.read_csv(file_path)  # full, unfiltered\n",
    "\n",
    "        # Ensure the necessary columns exist\n",
    "        if all(col in census_df_full.columns for col in ['cohb', 'gap', 'WEIGHT', 'HCORENEED_IND', 'totalincome']):\n",
    "            \n",
    "            # 1Ô∏è‚É£ For COHB & GAP ‚Üí exclude HCORENEED_IND == 888\n",
    "            census_df = census_df_full[census_df_full['HCORENEED_IND'] != 888].copy()\n",
    "\n",
    "            # COHB > 0\n",
    "            cohb_positive = census_df[census_df['cohb'] > 0]\n",
    "            if not cohb_positive.empty:\n",
    "                weighted_mean_cohb = (cohb_positive['cohb'] * cohb_positive['WEIGHT']).sum() / cohb_positive['WEIGHT'].sum()\n",
    "                mean_cohb[year] = weighted_mean_cohb\n",
    "            else:\n",
    "                mean_cohb[year] = 0\n",
    "\n",
    "            # GAP > 0\n",
    "            gap_positive = census_df[census_df['gap'] > 0]\n",
    "            if not gap_positive.empty:\n",
    "                weighted_mean_gap = (gap_positive['gap'] * gap_positive['WEIGHT']).sum() / gap_positive['WEIGHT'].sum()\n",
    "                mean_gap[year] = weighted_mean_gap\n",
    "            else:\n",
    "                mean_gap[year] = 0\n",
    "\n",
    "            # 2Ô∏è‚É£ For Income ‚Üí use *full*, unfiltered dataframe\n",
    "            if not census_df_full.empty:\n",
    "                weighted_mean_income = (census_df_full['totalincome'] * census_df_full['WEIGHT']).sum() / census_df_full['WEIGHT'].sum()\n",
    "                mean_income[year] = weighted_mean_income\n",
    "            else:\n",
    "                mean_income[year] = 0\n",
    "        else:\n",
    "            print(f\"‚ùå Missing columns in {year}, skipping.\")\n",
    "    else:\n",
    "        print(f\"‚ùå File not found for {year}\")\n",
    "\n",
    "# ‚úÖ Print results\n",
    "print(\"\\nüìä Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_cohb.items():\n",
    "    print(f\"{year}: {val:.2f}\")\n",
    "\n",
    "print(\"\\nüìä Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_gap.items():\n",
    "    print(f\"{year}: {val:.2f}\")\n",
    "\n",
    "print(\"\\nüìä Weighted Mean Income (ALL households, INCLUDING HCORENEED_IND == 888):\")\n",
    "for year, val in mean_income.items():\n",
    "    print(f\"{year}: ${val:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chn_trace_row52.csv created at C:/Users/mgordon/OneDrive - Financial Accountability Office of Ontario/FA2404 Housing and Homelessness Update/Data\\Microsimulations\\with_chn\\chn_trace_row53.csv\n"
     ]
    }
   ],
   "source": [
    "#trace file\n",
    "\n",
    "\n",
    "# Folder path\n",
    "base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Years to process\n",
    "years = range(2021, 2031)\n",
    "\n",
    "# Row number to extract (0-based index)\n",
    "target_row = 51  # Change this to any row index you want\n",
    "\n",
    "# List to store selected rows\n",
    "selected_rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_name = f\"census{year}_household_chn.csv\"\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if len(df) > target_row:\n",
    "            selected_row = df.iloc[target_row]\n",
    "            selected_rows.append(selected_row)\n",
    "        else:\n",
    "            print(f\"File {file_name} has less than {target_row + 1} rows.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Combine and save to chn_trace.csv\n",
    "if selected_rows:\n",
    "    chn_trace_df = pd.DataFrame(selected_rows)\n",
    "    output_path = os.path.join(base_path, f\"chn_trace_row{target_row + 2}.csv\")\n",
    "    chn_trace_df.to_csv(output_path, index=False)\n",
    "    print(f\"chn_trace_row{target_row + 1}.csv created at {output_path}\")\n",
    "else:\n",
    "    print(\"No data found to create trace file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Weighted Average Netshare by Quintile:\n",
      "\n",
      "   quintile  weighted_netshare\n",
      "0         1             0.9442\n",
      "1         2             0.9152\n",
      "2         3             0.8822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\510544637.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2021_household_chn.csv\"))\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['netinc', 'totalincome', 'WEIGHT', 'quintile']\n",
    "if all(col in df.columns for col in required_cols):\n",
    "    # Compute netshare safely\n",
    "    df['netshare'] = df['netinc'] / df['totalincome']\n",
    "    df = df.replace([float('inf'), -float('inf')], pd.NA).dropna(subset=['netshare'])\n",
    "\n",
    "    # Group by quintile and calculate weighted average netshare\n",
    "    summary = (\n",
    "        df.groupby('quintile')\n",
    "        .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    "        .reset_index(name='weighted_netshare')\n",
    "    )\n",
    "\n",
    "    # Format output\n",
    "    summary['weighted_netshare'] = summary['weighted_netshare'].round(4)\n",
    "    print(\"\\n‚úÖ Weighted Average Netshare by Quintile:\\n\")\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"‚ùå Missing required columns: netinc, totalincome, WEIGHT, or quintile.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       totalincome   netinc   netshare\n",
      "5860           401  -7000.0 -17.456359\n",
      "30378         1000  -6000.0  -6.000000\n",
      "25865         4400 -23000.0  -5.227273\n",
      "8073         11300 -50000.0  -4.424779\n",
      "5262          1000  -4000.0  -4.000000\n",
      "16587        13400 -53000.0  -3.955224\n",
      "24238          801  -3000.0  -3.745318\n",
      "9991          6800 -20000.0  -2.941176\n",
      "7770         11900 -30000.0  -2.521008\n",
      "4357          4500 -11000.0  -2.444444\n"
     ]
    }
   ],
   "source": [
    "print(df[['totalincome', 'netinc', 'netshare']].sort_values(by='netshare').head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore for now: COHB/affordable housing program analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted average income: $19,998.39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "output_path = os.path.join(folder_path, \"with_chn\", \"subset_2024.csv\")\n",
    "# Example: assuming df2024 is loaded\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2024_household_chn.csv\"))\n",
    "\n",
    "df = df[\n",
    "    (df['TENUR'] == 2) &\n",
    "    (df['chn'] == 1) &\n",
    "    (df['SUBSIDY'] == 0)\n",
    "].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_households_below_weighted_avg(df, target_avg=20000):\n",
    "    df_sorted = df.sort_values(by=\"totalincome\").reset_index(drop=True)\n",
    "    \n",
    "    df_sorted['cum_weighted_income'] = (df_sorted['totalincome'] * df_sorted['WEIGHT']).cumsum()\n",
    "    df_sorted['cum_weight'] = df_sorted['WEIGHT'].cumsum()\n",
    "    df_sorted['cum_weighted_avg'] = df_sorted['cum_weighted_income'] / df_sorted['cum_weight']\n",
    "\n",
    "    # Find the row where weighted average is closest to target_avg\n",
    "    df_sorted['abs_diff'] = (df_sorted['cum_weighted_avg'] - target_avg).abs()\n",
    "    best_idx = df_sorted['abs_diff'].idxmin()\n",
    "\n",
    "    subset_df = df_sorted.loc[:best_idx].copy()\n",
    "    subset_df.drop(columns=['cum_weighted_income', 'cum_weight', 'cum_weighted_avg', 'abs_diff'], inplace=True)\n",
    "\n",
    "    return subset_df\n",
    "\n",
    "\n",
    "\n",
    "subset_df = select_households_below_weighted_avg(df)\n",
    "\n",
    "subset_df['estgap'] = 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "subset_df['ntgap'] = 0.8 * 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "\n",
    "\n",
    "weighted_avg = (subset_df['totalincome'] * subset_df['WEIGHT']).sum() / subset_df['WEIGHT'].sum()\n",
    "print(f\"Weighted average income: ${weighted_avg:,.2f}\")\n",
    "subset_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final weighted avg COHB: $10,165.81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "output_path = os.path.join(folder_path, \"with_chn\", \"final_subset_2024_greedy.csv\")\n",
    "# Load your subset\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"subset_2024.csv\"))\n",
    "\n",
    "target_weight = 22000\n",
    "target_avg_cohb = 10600\n",
    "\n",
    "# Create a new column: absolute difference from target\n",
    "df['cohb_diff'] = (df['cohb'] - target_avg_cohb).abs()\n",
    "\n",
    "# Sort by closest to target COHB first\n",
    "df_sorted = df.sort_values(by='cohb_diff').reset_index(drop=True)\n",
    "\n",
    "selected_rows = []\n",
    "total_weight = 0\n",
    "total_weighted_cohb = 0\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    weight = row['WEIGHT']\n",
    "    cohb = row['cohb']\n",
    "\n",
    "    if total_weight + weight > target_weight:\n",
    "        remaining_weight = target_weight - total_weight\n",
    "        total_weighted_cohb += cohb * remaining_weight\n",
    "        row_copy = row.copy()\n",
    "        row_copy['WEIGHT'] = remaining_weight\n",
    "        selected_rows.append(row_copy)\n",
    "        total_weight = target_weight\n",
    "        break\n",
    "    else:\n",
    "        total_weight += weight\n",
    "        total_weighted_cohb += cohb * weight\n",
    "        selected_rows.append(row)\n",
    "\n",
    "# Calculate final weighted average\n",
    "weighted_avg_cohb = total_weighted_cohb / total_weight\n",
    "print(f\"‚úÖ Final weighted avg COHB: ${weighted_avg_cohb:,.2f}\")\n",
    "\n",
    "# Save\n",
    "final_df = pd.DataFrame(selected_rows)\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ 2024: Weighted avg COHB = $10,165.12 over 22,007 weighted households\n",
      "‚úÖ 2025: Weighted avg COHB = $10,587.99 over 21,979 weighted households\n",
      "‚úÖ 2026: Weighted avg COHB = $10,902.19 over 21,885 weighted households\n",
      "‚úÖ 2027: Weighted avg COHB = $11,070.07 over 21,788 weighted households\n",
      "‚úÖ 2028: Weighted avg COHB = $11,299.94 over 21,955 weighted households\n",
      "‚úÖ 2029: Weighted avg COHB = $11,449.78 over 22,134 weighted households\n",
      "‚úÖ 2030: Weighted avg COHB = $11,656.61 over 22,318 weighted households\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your final selected households (from 2024)\n",
    "final_subset = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"final_subset_2024_greedy.csv\"))\n",
    "\n",
    "\n",
    "# Get the list of selected HH_IDs\n",
    "selected_hh_ids = final_subset['HH_ID'].unique()\n",
    "\n",
    "# Range of years to check\n",
    "years = range(2024, 2031)  # 2025 to 2030 inclusive\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    # Load that year's census file\n",
    "    df_year = pd.read_csv(os.path.join(folder_path, \"with_chn\", f\"census{year}_household_chn.csv\"))\n",
    "    \n",
    "    # Filter to only the selected HH_IDs\n",
    "    df_matched = df_year[df_year['HH_ID'].isin(selected_hh_ids)].copy()\n",
    "\n",
    "    if df_matched.empty:\n",
    "        print(f\"‚ö†Ô∏è No matching HH_IDs found in {year} data.\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate weighted average COHB\n",
    "    weighted_avg_cohb = (df_matched['cohb'] * df_matched['WEIGHT']).sum() / df_matched['WEIGHT'].sum()\n",
    "\n",
    "    print(f\"‚úÖ {year}: Weighted avg COHB = ${weighted_avg_cohb:,.2f} over {df_matched['WEIGHT'].sum():,.0f} weighted households\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2023_household_chn.csv\"))\n",
    "\n",
    "\n",
    "# Define conditions based on bedsuit values\n",
    "conditions = [\n",
    "    (df['bedsuit'] == 0) & (df['totalincome'] <= 40000),\n",
    "    (df['bedsuit'] == 1) & (df['totalincome'] <= 49000),\n",
    "    (df['bedsuit'] == 2) & (df['totalincome'] <= 56000),\n",
    "    (df['bedsuit'] == 3) & (df['totalincome'] <= 62000),\n",
    "    (df['bedsuit'] >= 4) & (df['totalincome'] <= 75000),\n",
    "]\n",
    "\n",
    "# Combine all conditions using logical OR\n",
    "combined_condition = conditions[0]\n",
    "for cond in conditions[1:]:\n",
    "    combined_condition |= cond\n",
    "\n",
    "# Create the filtered subset\n",
    "subset_df = df[combined_condition].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENUR\n",
      "1.0    241147.576884\n",
      "2.0    445077.157150\n",
      "Name: WEIGHT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filter for households in core housing need and valid HCORENEED_IND\n",
    "chn_df = subset_df[(subset_df['chn'] == 1) & (subset_df['HCORENEED_IND'] != 888)]\n",
    "\n",
    "# Group by TENUR and sum the WEIGHT for each group\n",
    "weighted_chn_by_tenur = chn_df.groupby('TENUR')['WEIGHT'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(weighted_chn_by_tenur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate CNIT to use for asset threshold shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedsuit\n",
      "0.0     46739.260049\n",
      "1.0     58171.510037\n",
      "2.0     68383.754562\n",
      "3.0     82392.623954\n",
      "4.0    104664.859686\n",
      "5.0    103760.971054\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 104547.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\413785005.py:16: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2023_household_chn.csv\"))\n",
    "\n",
    "# Step 2: Create a filtered copy for households in core housing need with valid HCORENEED_IND\n",
    "df_copy = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)].copy()\n",
    "\n",
    "# Step 3: Create the cnit variable\n",
    "df_copy['cnit'] = (12 * df_copy['mmr']) / 0.3\n",
    "\n",
    "# Step 4: Calculate weighted average CNIT by bedsuit\n",
    "weighted_avg_cnit = (\n",
    "    df_copy\n",
    "    .groupby('bedsuit')\n",
    "    .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(weighted_avg_cnit)\n",
    "\n",
    "\n",
    "# Filter households with bedsuit >= 4\n",
    "df_4plus = df_copy[df_copy['bedsuit'] >= 4]\n",
    "\n",
    "# Calculate weighted average CNIT\n",
    "weighted_avg_cnit_4plus = (df_4plus['cnit'] * df_4plus['WEIGHT']).sum() / df_4plus['WEIGHT'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Weighted average CNIT for 4+ bedroom-suitable households: {weighted_avg_cnit_4plus:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2023 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     46739.260049\n",
      "1.0     58171.510037\n",
      "2.0     68383.754562\n",
      "3.0     82392.623954\n",
      "4.0    104664.859686\n",
      "5.0    103760.971054\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 104547.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2024 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     49168.414346\n",
      "1.0     61234.767167\n",
      "2.0     72050.645307\n",
      "3.0     86849.170762\n",
      "4.0    110182.989784\n",
      "5.0    108820.268116\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 109994.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2025 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     50890.834160\n",
      "1.0     63433.209644\n",
      "2.0     74552.513586\n",
      "3.0     89787.577795\n",
      "4.0    113900.082036\n",
      "5.0    113075.526211\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 113790.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2026 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     52211.967423\n",
      "1.0     64891.608915\n",
      "2.0     76405.471141\n",
      "3.0     92143.129955\n",
      "4.0    116577.713349\n",
      "5.0    116421.940234\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 116557.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2027 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     53095.496088\n",
      "1.0     65941.991508\n",
      "2.0     77756.706121\n",
      "3.0     93747.119127\n",
      "4.0    118842.391422\n",
      "5.0    118866.606682\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 118845.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2028 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     54153.253545\n",
      "1.0     67136.744979\n",
      "2.0     79283.313113\n",
      "3.0     95687.747382\n",
      "4.0    121389.292514\n",
      "5.0    121249.097609\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 121372.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2029 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     54998.407829\n",
      "1.0     68162.101955\n",
      "2.0     80546.498405\n",
      "3.0     97269.013974\n",
      "4.0    123246.887682\n",
      "5.0    123089.473839\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 123227.23\n",
      "\n",
      "Year 2030 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     55983.419020\n",
      "1.0     69327.349676\n",
      "2.0     81964.661446\n",
      "3.0     99078.868801\n",
      "4.0    125429.553285\n",
      "5.0    125352.727606\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 125419.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your directory\n",
    "base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Loop through years 2023 to 2030\n",
    "for year in range(2023, 2031):\n",
    "    file_path = os.path.join(base_path, f\"census{year}_household_chn.csv\")\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter for households in core housing need with valid HCORENEED_IND\n",
    "    df_copy = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)].copy()\n",
    "    \n",
    "    # Create the cnit variable\n",
    "    df_copy['cnit'] = (12 * df_copy['mmr']) / 0.3\n",
    "    \n",
    "    # Calculate weighted average CNIT by bedsuit\n",
    "    weighted_avg_cnit = (\n",
    "        df_copy\n",
    "        .groupby('bedsuit')\n",
    "        .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    "    )\n",
    "    \n",
    "    # Display the result for this year\n",
    "    print(f\"\\nYear {year} - Weighted average CNIT by bedsuit:\")\n",
    "    print(weighted_avg_cnit)\n",
    "    \n",
    "    # Filter households with bedsuit >= 4\n",
    "    df_4plus = df_copy[df_copy['bedsuit'] >= 4]\n",
    "    \n",
    "    # Calculate weighted average CNIT for 4+ bedroom-suitable households\n",
    "    if not df_4plus.empty:\n",
    "        weighted_avg_cnit_4plus = (df_4plus['cnit'] * df_4plus['WEIGHT']).sum() / df_4plus['WEIGHT'].sum()\n",
    "        print(f\"Weighted average CNIT for 4+ bedroom-suitable households: {weighted_avg_cnit_4plus:.2f}\")\n",
    "    else:\n",
    "        print(\"No 4+ bedroom-suitable households in this year.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted count (TENUR = 2): 547,209\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 122,220\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 208,622\n",
      "Total weighted households (filtered): 878,052\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2023_household_chn.csv\"))\n",
    "\n",
    "# Subset for CHN == 1, HCORENEED_IND != 888, and NPRHH == 0\n",
    "filtered_df = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888) ]\n",
    "#& (df['nprhh'] == 0)\n",
    "# Calculate weighted counts for each category\n",
    "tenur_2 = filtered_df[filtered_df['TENUR'] == 2]['WEIGHT'].sum()\n",
    "tenur_1_presmortg_0 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 0)]['WEIGHT'].sum()\n",
    "tenur_1_presmortg_1 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 1)]['WEIGHT'].sum()\n",
    "\n",
    "\n",
    "# Total weighted households (after filtering)\n",
    "total_weighted = filtered_df['WEIGHT'].sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Weighted count (TENUR = 2): {tenur_2:,.0f}\")\n",
    "print(f\"Weighted count (TENUR = 1 & PRESMORTG = 0): {tenur_1_presmortg_0:,.0f}\")\n",
    "print(f\"Weighted count (TENUR = 1 & PRESMORTG = 1): {tenur_1_presmortg_1:,.0f}\")\n",
    "print(f\"Total weighted households (filtered): {total_weighted:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ Year 2023:\n",
      "Weighted count (TENUR = 2): 547,209\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 122,220\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 208,622\n",
      "Total weighted households (filtered): 878,052\n",
      "\n",
      "üìÖ Year 2024:\n",
      "Weighted count (TENUR = 2): 576,120\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 126,014\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 215,990\n",
      "Total weighted households (filtered): 918,124\n",
      "\n",
      "üìÖ Year 2025:\n",
      "Weighted count (TENUR = 2): 589,236\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 131,226\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 222,112\n",
      "Total weighted households (filtered): 942,575\n",
      "\n",
      "üìÖ Year 2026:\n",
      "Weighted count (TENUR = 2): 597,275\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 136,773\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 228,028\n",
      "Total weighted households (filtered): 962,077\n",
      "\n",
      "üìÖ Year 2027:\n",
      "Weighted count (TENUR = 2): 590,953\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 141,208\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 226,300\n",
      "Total weighted households (filtered): 958,460\n",
      "\n",
      "üìÖ Year 2028:\n",
      "Weighted count (TENUR = 2): 595,972\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 146,074\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 228,250\n",
      "Total weighted households (filtered): 970,296\n",
      "\n",
      "üìÖ Year 2029:\n",
      "Weighted count (TENUR = 2): 596,902\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 153,121\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 227,240\n",
      "Total weighted households (filtered): 977,263\n",
      "\n",
      "üìÖ Year 2030:\n",
      "Weighted count (TENUR = 2): 600,567\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 159,203\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 228,012\n",
      "Total weighted households (filtered): 987,783\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your directory\n",
    "base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Loop through years 2023 to 2030\n",
    "for year in range(2023, 2031):\n",
    "    file_path = os.path.join(base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the data\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Subset for CHN == 1, HCORENEED_IND != 888, and NPRHH == 0\n",
    "        filtered_df = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)]\n",
    "\n",
    "        # Calculate weighted counts\n",
    "        tenur_2 = filtered_df[filtered_df['TENUR'] == 2]['WEIGHT'].sum()\n",
    "        tenur_1_presmortg_0 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 0)]['WEIGHT'].sum()\n",
    "        tenur_1_presmortg_1 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 1)]['WEIGHT'].sum()\n",
    "\n",
    "        # Total weighted households (after filtering)\n",
    "        total_weighted = filtered_df['WEIGHT'].sum()\n",
    "\n",
    "        # Print the results for this year\n",
    "        print(f\"\\nüìÖ Year {year}:\")\n",
    "        print(f\"Weighted count (TENUR = 2): {tenur_2:,.0f}\")\n",
    "        print(f\"Weighted count (TENUR = 1 & PRESMORTG = 0): {tenur_1_presmortg_0:,.0f}\")\n",
    "        print(f\"Weighted count (TENUR = 1 & PRESMORTG = 1): {tenur_1_presmortg_1:,.0f}\")\n",
    "        print(f\"Total weighted households (filtered): {total_weighted:,.0f}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"‚ùå File not found for year {year}, skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate avg income growth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
