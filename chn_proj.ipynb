{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CHN values and added stir/alt_stir for 2022\n",
      "Updated CHN values and added stir/alt_stir for 2023\n",
      "Updated CHN values and added stir/alt_stir for 2024\n",
      "Updated CHN values and added stir/alt_stir for 2025\n",
      "Updated CHN values and added stir/alt_stir for 2026\n",
      "Updated CHN values and added stir/alt_stir for 2027\n",
      "Updated CHN values and added stir/alt_stir for 2028\n",
      "Updated CHN values and added stir/alt_stir for 2029\n",
      "Updated CHN values and added stir/alt_stir for 2030\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "input_base_path = \"../Microsimulations/household/\"\n",
    "output_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Dictionary to store updated data\n",
    "census_data = {}\n",
    "\n",
    "# Loop through the years 2022 to 2030\n",
    "for year in range(2022, 2031):\n",
    "    input_file_path = os.path.join(input_base_path, f\"census{year}_household.csv\")\n",
    "    output_file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    \n",
    "    if os.path.exists(input_file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(input_file_path)\n",
    "        \n",
    "        # Initialize CHN column to 0\n",
    "        census_df['chn'] = 0\n",
    "\n",
    "        # Define housing issue conditions\n",
    "        housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.30) |  # Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define market unaffordability condition\n",
    "        market_unaffordable = (census_df['mmr']) * 12 > 0.30 * census_df['totalincome']\n",
    "\n",
    "        # Update CHN variable\n",
    "        census_df.loc[\n",
    "            housing_issue & market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'chn'\n",
    "        ] = 1\n",
    "\n",
    "        # Now create stir and alt_stir after chn is assigned\n",
    "       # census_df['stir'] = census_df['SHELCO'] * 12 / census_df['totalincome']\n",
    "        #census_df['alt_stir'] = (census_df['mmr']) * 12 / census_df['totalincome']\n",
    "\n",
    "\n",
    "        # Update CHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"chn\"] = 0\n",
    "        \n",
    "\n",
    "        # Define deep core housing issue condition (using 50% income threshold)\n",
    "        deep_housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.50) |  # Deeply Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define deep market unaffordability condition (50% threshold)\n",
    "        deep_market_unaffordable = (census_df['mmr']) * 12 > 0.50 * census_df['totalincome']\n",
    "\n",
    "        # Initialize dchn column to 0\n",
    "        census_df['dchn'] = 0\n",
    "\n",
    "        # Update dchn variable\n",
    "        census_df.loc[\n",
    "            deep_housing_issue & deep_market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'dchn'\n",
    "        ] = 1\n",
    "\n",
    "        # Update DCHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"dchn\"] = 0\n",
    "\n",
    "        # Export updated data\n",
    "        census_df.to_csv(output_file_path, index=False)\n",
    "        \n",
    "        print(f\"Updated CHN values and added stir/alt_stir for {year}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: ../Microsimulations/with_chn/census2021_household_chn.csv\n"
     ]
    }
   ],
   "source": [
    "#add 2021 census file to folder\n",
    "\n",
    "# Define input and output paths\n",
    "input_base_path = \"../Microsimulations/household/\"\n",
    "output_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# File name\n",
    "input_file_name = \"census2021_household.csv\"\n",
    "output_file_name = \"census2021_household_chn.csv\"\n",
    "\n",
    "# Full paths\n",
    "input_file_path = os.path.join(input_base_path, input_file_name)\n",
    "output_file_path = os.path.join(output_base_path, output_file_name)\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "\n",
    "  # Now create stir and alt_stir after chn is assigned\n",
    "df['stir'] = df['SHELCO'] * 12 / df['totalincome']\n",
    "df['alt_stir'] = (df['mmr']) * 12 / df['totalincome']\n",
    "\n",
    "#net income share\n",
    "df['netshare'] = (\n",
    "    df['TOTINC_AT'] / df['totalincome']\n",
    ").clip(upper=1.0)\n",
    "\n",
    "#net income\n",
    "df['netinc'] = df['totalincome'] * df['netshare']\n",
    "\n",
    "# Save the modified dataframe\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021] CHN weighted count: 684245.5791455524, DCHN weighted count: 154141.0278820697\n",
      "[2022] CHN weighted count: 775663.1996845966, DCHN weighted count: 193171.441375075\n",
      "[2023] CHN weighted count: 898977.6674971142, DCHN weighted count: 246696.35102513284\n",
      "[2024] CHN weighted count: 961182.9994609419, DCHN weighted count: 267950.07722888654\n",
      "[2025] CHN weighted count: 1002609.9839466489, DCHN weighted count: 286965.47288308386\n",
      "[2026] CHN weighted count: 1041591.8021459938, DCHN weighted count: 308772.29141312605\n",
      "[2027] CHN weighted count: 1081257.9059293726, DCHN weighted count: 330058.47826986696\n",
      "[2028] CHN weighted count: 1139147.8167526051, DCHN weighted count: 356986.8514300904\n",
      "[2029] CHN weighted count: 1201660.7058479595, DCHN weighted count: 388469.46203185746\n",
      "[2030] CHN weighted count: 1268572.9000548227, DCHN weighted count: 425247.337325424\n",
      "\n",
      "Total household weights per year:\n",
      "Sum of WEIGHT for 2021: 3310969.362428404\n",
      "Sum of WEIGHT for 2022: 3391447.579802723\n",
      "Sum of WEIGHT for 2023: 3499096.9189999024\n",
      "Sum of WEIGHT for 2024: 3601408.5594069078\n",
      "Sum of WEIGHT for 2025: 3640330.1222850895\n",
      "Sum of WEIGHT for 2026: 3669265.6668494893\n",
      "Sum of WEIGHT for 2027: 3697396.7557604937\n",
      "Sum of WEIGHT for 2028: 3750344.351647416\n",
      "Sum of WEIGHT for 2029: 3799703.2702311664\n",
      "Sum of WEIGHT for 2030: 3846727.361787364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path for processed files\n",
    "output_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# Dictionaries to store weighted household counts\n",
    "chn_weighted_counts = {}\n",
    "dchn_weighted_counts = {}\n",
    "\n",
    "# Loop through years 2022 to 2030\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if required columns exist\n",
    "        required_columns = {'chn', 'dchn', 'WEIGHT', 'HCORENEED_IND'}\n",
    "        missing_columns = required_columns - set(census_df.columns)\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"Skipping {year} due to missing columns: {missing_columns}\")\n",
    "            continue  # Skip processing this file\n",
    "\n",
    "        # Exclude households where HCORENEED_IND == 888\n",
    "        filtered_df = census_df[census_df['HCORENEED_IND'] != 888]\n",
    "\n",
    "        # Calculate weighted count of households where chn == 1\n",
    "        chn_weight = filtered_df.loc[filtered_df['chn'] == 1, 'WEIGHT'].sum()\n",
    "        chn_weighted_counts[year] = chn_weight\n",
    "\n",
    "        # Calculate weighted count of households where dchn == 1\n",
    "        dchn_weight = filtered_df.loc[filtered_df['dchn'] == 1, 'WEIGHT'].sum()\n",
    "        dchn_weighted_counts[year] = dchn_weight\n",
    "\n",
    "        print(f\"[{year}] CHN weighted count: {chn_weight}, DCHN weighted count: {dchn_weight}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "print(\"\\nTotal household weights per year:\")\n",
    "\n",
    "# Print the sum of WEIGHT for each year\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "        print(f\"Sum of WEIGHT for {year}: {census_df['WEIGHT'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define year to process\n",
    "year = 2021\n",
    "file_path = f\"../Microsimulations/census{year}.csv\"\n",
    "\n",
    "# Check if file exists before proceeding\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Create jobless variable\n",
    "    df[\"jobless\"] = df[\"LFACT\"].between(2, 10).astype(int)\n",
    "    \n",
    "    # Calculate share of records with jobless == 1 for each AGEGRP and IMMSTAT\n",
    "    summary = df.groupby([\"AGEGRP\", df[\"IMMSTAT\"].apply(lambda x: \"IMMSTAT_3\" if x == 3 else \"IMMSTAT_not_3\")])[\"jobless\"].mean().reset_index()\n",
    "    \n",
    "    # Save summary to a CSV file\n",
    "    output_path = f\"../Microsimulations/census_share_{year}.csv\"\n",
    "    summary.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated netshare and netinc for 2022\n",
      "✅ Updated netshare and netinc for 2023\n",
      "✅ Updated netshare and netinc for 2024\n",
      "✅ Updated netshare and netinc for 2025\n",
      "✅ Updated netshare and netinc for 2026\n",
      "✅ Updated netshare and netinc for 2027\n",
      "✅ Updated netshare and netinc for 2028\n",
      "✅ Updated netshare and netinc for 2029\n",
      "✅ Updated netshare and netinc for 2030\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load netshare from 2021\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "census2021_household = pd.read_csv(os.path.join(input_base_path, \"census2021_household_chn.csv\"))\n",
    "netshare_2021 = (census2021_household['TOTINC_AT'] / census2021_household['totalincome']).clip(upper=1.0)\n",
    "\n",
    "# Apply to years 2022–2030\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "for year in range(2022, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Assign netshare from 2021 (assumes same order/row count)\n",
    "        census_df['netshare'] = netshare_2021.values\n",
    "        census_df['netinc'] = census_df['totalincome'] * census_df['netshare']\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Updated netshare and netinc for {year}\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Calculated gap for 2021 using updated CHN conditions\n",
      "✅ Calculated gap for 2022 using updated CHN conditions\n",
      "✅ Calculated gap for 2023 using updated CHN conditions\n",
      "✅ Calculated gap for 2024 using updated CHN conditions\n",
      "✅ Calculated gap for 2025 using updated CHN conditions\n",
      "✅ Calculated gap for 2026 using updated CHN conditions\n",
      "✅ Calculated gap for 2027 using updated CHN conditions\n",
      "✅ Calculated gap for 2028 using updated CHN conditions\n",
      "✅ Calculated gap for 2029 using updated CHN conditions\n",
      "✅ Calculated gap for 2030 using updated CHN conditions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Default gap to 0.0\n",
    "        census_df['gap'] = 0.0\n",
    "\n",
    "        # Only calculate gap where CHN = 1\n",
    "        chn_condition = census_df['chn'] == 1\n",
    "\n",
    "        # Use SHELCO if NOS == 1, REPAIR != 3, and SHELCO < mmr \n",
    "        use_shelco = (\n",
    "            chn_condition &\n",
    "            (census_df['NOS'] == 1) &\n",
    "            (census_df['REPAIR'] != 3) &\n",
    "            (census_df['SHELCO'] < census_df['mmr'])\n",
    "        )\n",
    "\n",
    "        # Use AMR (mmr) otherwise\n",
    "        use_amr = chn_condition & ~use_shelco  # CHN == 1 but doesn't meet SHELCO condition\n",
    "\n",
    "        # Apply SHELCO-based gap\n",
    "        census_df.loc[use_shelco, 'gap'] = (\n",
    "            census_df.loc[use_shelco, 'SHELCO'] * 12 - 0.3 * census_df.loc[use_shelco, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Apply AMR-based gap\n",
    "        census_df.loc[use_amr, 'gap'] = (\n",
    "            (census_df.loc[use_amr, 'mmr']) * 12 - 0.3 * census_df.loc[use_amr, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Calculated gap for {year} using updated CHN conditions\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030: CHN=1 count: 10960, GAP > 0 count: 10960\n"
     ]
    }
   ],
   "source": [
    "# After assigning gaps\n",
    "print(f\"{year}: CHN=1 count: {census_df['chn'].sum()}, GAP > 0 count: {(census_df['gap'] > 0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added COHB values for renters in CHN in 2021\n",
      "✅ Added COHB values for renters in CHN in 2022\n",
      "✅ Added COHB values for renters in CHN in 2023\n",
      "✅ Added COHB values for renters in CHN in 2024\n",
      "✅ Added COHB values for renters in CHN in 2025\n",
      "✅ Added COHB values for renters in CHN in 2026\n",
      "✅ Added COHB values for renters in CHN in 2027\n",
      "✅ Added COHB values for renters in CHN in 2028\n",
      "✅ Added COHB values for renters in CHN in 2029\n",
      "✅ Added COHB values for renters in CHN in 2030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "for year in range(2021, 2031):  # 2021 to 2030 inclusive\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Initialize COHB to 0.0\n",
    "        census_df['cohb'] = 0.0\n",
    "\n",
    "        # Condition: renter, in core housing need, stir > 0.3\n",
    "        condition = (\n",
    "            (census_df['TENUR'] == 2) &\n",
    "            (census_df['chn'] == 1) &\n",
    "            (census_df['stir'] > 0.3)\n",
    "        )\n",
    "\n",
    "        # Pre-calculate parts\n",
    "        mmr_80 = 0.8 * 12 * census_df.loc[condition, 'mmr']\n",
    "        shelco_100_capped = (12 * census_df.loc[condition, 'SHELCO']).clip(upper=(12 * census_df.loc[condition, 'mmr']))\n",
    "        eligible_cost = pd.concat([mmr_80, shelco_100_capped], axis=1).max(axis=1)\n",
    "\n",
    "        netinc_30 = 0.3 * census_df.loc[condition, 'netinc']\n",
    "        cohb_values = eligible_cost - netinc_30\n",
    "\n",
    "        # Clip to ensure no negative COHB\n",
    "        census_df.loc[condition, 'cohb'] = cohb_values.clip(lower=0)\n",
    "\n",
    "        # Save back to file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Added COHB values for renters in CHN in {year}\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 4401.67\n",
      "2022: 4946.46\n",
      "2023: 5712.38\n",
      "2024: 6168.52\n",
      "2025: 6626.07\n",
      "2026: 7156.80\n",
      "2027: 7646.38\n",
      "2028: 8236.55\n",
      "2029: 8893.56\n",
      "2030: 9675.36\n",
      "\n",
      "📊 Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 3783.17\n",
      "2022: 4123.14\n",
      "2023: 4580.42\n",
      "2024: 4900.40\n",
      "2025: 5191.18\n",
      "2026: 5561.62\n",
      "2027: 5898.50\n",
      "2028: 6299.93\n",
      "2029: 6745.96\n",
      "2030: 7260.19\n"
     ]
    }
   ],
   "source": [
    "mean_cohb = {}\n",
    "mean_gap = {}\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure the necessary columns exist\n",
    "        if all(col in census_df.columns for col in ['cohb', 'gap', 'WEIGHT', 'HCORENEED_IND']):\n",
    "            # Exclude records where HCORENEED_IND == 888\n",
    "            census_df = census_df[census_df['HCORENEED_IND'] != 888]\n",
    "\n",
    "            # Filter rows where cohb > 0\n",
    "            cohb_positive = census_df[census_df['cohb'] > 0]\n",
    "            if not cohb_positive.empty:\n",
    "                weighted_mean_cohb = (cohb_positive['cohb'] * cohb_positive['WEIGHT']).sum() / cohb_positive['WEIGHT'].sum()\n",
    "                mean_cohb[year] = weighted_mean_cohb\n",
    "            else:\n",
    "                mean_cohb[year] = 0\n",
    "\n",
    "            # Filter rows where gap > 0\n",
    "            gap_positive = census_df[census_df['gap'] > 0]\n",
    "            if not gap_positive.empty:\n",
    "                weighted_mean_gap = (gap_positive['gap'] * gap_positive['WEIGHT']).sum() / gap_positive['WEIGHT'].sum()\n",
    "                mean_gap[year] = weighted_mean_gap\n",
    "            else:\n",
    "                mean_gap[year] = 0\n",
    "        else:\n",
    "            print(f\"❌ Missing columns in {year}, skipping.\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}\")\n",
    "\n",
    "# ✅ Print results\n",
    "print(\"\\n📊 Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_cohb.items():\n",
    "    print(f\"{year}: {val:.2f}\")\n",
    "\n",
    "print(\"\\n📊 Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_gap.items():\n",
    "    print(f\"{year}: {val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chn_trace_row52.csv created at ../Microsimulations/with_chn/chn_trace_row53.csv\n"
     ]
    }
   ],
   "source": [
    "#trace file\n",
    "\n",
    "\n",
    "# Folder path\n",
    "base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# Years to process\n",
    "years = range(2021, 2031)\n",
    "\n",
    "# Row number to extract (0-based index)\n",
    "target_row = 51  # Change this to any row index you want\n",
    "\n",
    "# List to store selected rows\n",
    "selected_rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_name = f\"census{year}_household_chn.csv\"\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if len(df) > target_row:\n",
    "            selected_row = df.iloc[target_row]\n",
    "            selected_rows.append(selected_row)\n",
    "        else:\n",
    "            print(f\"File {file_name} has less than {target_row + 1} rows.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Combine and save to chn_trace.csv\n",
    "if selected_rows:\n",
    "    chn_trace_df = pd.DataFrame(selected_rows)\n",
    "    output_path = os.path.join(base_path, f\"chn_trace_row{target_row + 2}.csv\")\n",
    "    chn_trace_df.to_csv(output_path, index=False)\n",
    "    print(f\"chn_trace_row{target_row + 1}.csv created at {output_path}\")\n",
    "else:\n",
    "    print(\"No data found to create trace file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Weighted Average Netshare by Quintile:\n",
      "\n",
      "   quintile  weighted_netshare\n",
      "0         1             0.9442\n",
      "1         2             0.9152\n",
      "2         3             0.8822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_31240\\3567599826.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/census2021_household_chn.csv\")\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['netinc', 'totalincome', 'WEIGHT', 'quintile']\n",
    "if all(col in df.columns for col in required_cols):\n",
    "    # Compute netshare safely\n",
    "    df['netshare'] = df['netinc'] / df['totalincome']\n",
    "    df = df.replace([float('inf'), -float('inf')], pd.NA).dropna(subset=['netshare'])\n",
    "\n",
    "    # Group by quintile and calculate weighted average netshare\n",
    "    summary = (\n",
    "        df.groupby('quintile')\n",
    "        .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    "        .reset_index(name='weighted_netshare')\n",
    "    )\n",
    "\n",
    "    # Format output\n",
    "    summary['weighted_netshare'] = summary['weighted_netshare'].round(4)\n",
    "    print(\"\\n✅ Weighted Average Netshare by Quintile:\\n\")\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"❌ Missing required columns: netinc, totalincome, WEIGHT, or quintile.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       totalincome   netinc   netshare\n",
      "5860           401  -7000.0 -17.456359\n",
      "30378         1000  -6000.0  -6.000000\n",
      "25865         4400 -23000.0  -5.227273\n",
      "8073         11300 -50000.0  -4.424779\n",
      "5262          1000  -4000.0  -4.000000\n",
      "16587        13400 -53000.0  -3.955224\n",
      "24238          801  -3000.0  -3.745318\n",
      "9991          6800 -20000.0  -2.941176\n",
      "7770         11900 -30000.0  -2.521008\n",
      "4357          4500 -11000.0  -2.444444\n"
     ]
    }
   ],
   "source": [
    "print(df[['totalincome', 'netinc', 'netshare']].sort_values(by='netshare').head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore for now: COHB/affordable housing program analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted average income: $37,548.84\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example: assuming df2024 is loaded\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/census2024_household_chn.csv\")\n",
    "df = df[\n",
    "    (df['TENUR'] == 2) &\n",
    "    (df['HCORENEED_IND'] != 888) &\n",
    "    (df['SUBSIDY'] == 1)\n",
    "].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_households_below_weighted_avg(df, target_avg=20000):\n",
    "    df_sorted = df.sort_values(by=\"totalincome\")\n",
    "    \n",
    "    subset_rows = []\n",
    "    total_weighted_income = 0\n",
    "    total_weight = 0\n",
    "\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        income = row['totalincome']\n",
    "        weight = row['WEIGHT']\n",
    "        \n",
    "        # Predict the new weighted average if we add this household\n",
    "        new_total_weighted_income = total_weighted_income + income * weight\n",
    "        new_total_weight = total_weight + weight\n",
    "        new_weighted_avg = new_total_weighted_income / new_total_weight\n",
    "        \n",
    "        if new_weighted_avg <= target_avg:\n",
    "            subset_rows.append(row)\n",
    "            total_weighted_income = new_total_weighted_income\n",
    "            total_weight = new_total_weight\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    subset_df = pd.DataFrame(subset_rows)\n",
    "    return subset_df\n",
    "\n",
    "\n",
    "subset_df = select_households_below_weighted_avg(df)\n",
    "\n",
    "subset_df['estgap'] = 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "subset_df['ntgap'] = 0.8 * 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "\n",
    "\n",
    "weighted_avg = (subset_df['totalincome'] * subset_df['WEIGHT']).sum() / subset_df['WEIGHT'].sum()\n",
    "print(f\"Weighted average income: ${weighted_avg:,.2f}\")\n",
    "subset_df.to_csv(\"../Microsimulations/with_chn/subset_2024.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Total weighted sample: 55,181\n",
      "🏠 Weighted count (estgap > 0): 55,181\n",
      "🏠 Weighted count (ntgap < 0): 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your subset\n",
    "subset_df = pd.read_csv(\"../Microsimulations/with_chn/subset_2022.csv\")\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "shuffled_df = subset_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Select rows until cumulative WEIGHT is close to 555606 ± 500\n",
    "target_weight = 55606\n",
    "tolerance = 500\n",
    "\n",
    "selected_rows = []\n",
    "total_weight = 0\n",
    "\n",
    "for _, row in shuffled_df.iterrows():\n",
    "    row_weight = row['WEIGHT']\n",
    "    if total_weight + row_weight > target_weight + tolerance:\n",
    "        continue\n",
    "    selected_rows.append(row)\n",
    "    total_weight += row_weight\n",
    "    if total_weight >= target_weight - tolerance:\n",
    "        break\n",
    "\n",
    "# Convert to DataFrame\n",
    "sample_df = pd.DataFrame(selected_rows)\n",
    "\n",
    "# Calculate weighted count where estgap > 0\n",
    "estgap_positive_weighted = sample_df.loc[sample_df['estgap'] > 0, 'WEIGHT'].sum()\n",
    "\n",
    "# Calculate weighted count where ntgap < 0\n",
    "ntgap_negative_weighted = sample_df.loc[sample_df['ntgap'] < 0, 'WEIGHT'].sum()\n",
    "\n",
    "print(f\"✅ Total weighted sample: {total_weight:,.0f}\")\n",
    "print(f\"🏠 Weighted count (estgap > 0): {estgap_positive_weighted:,.0f}\")\n",
    "print(f\"🏠 Weighted count (ntgap < 0): {ntgap_negative_weighted:,.0f}\")\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "sample_df.to_csv(\"../Microsimulations/with_chn/sample_subset_2022.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Match found on attempt 54: Weighted avg COHB = $6,119.38\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your subset\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/subset_2024.csv\")\n",
    "\n",
    "# Target constraints\n",
    "target_weight = 514\n",
    "target_avg_cohb = 6154\n",
    "tolerance = 50  # allowable ± range for avg COHB\n",
    "\n",
    "best_match = None\n",
    "closest_diff = float('inf')\n",
    "\n",
    "for i in range(10000):  # Max tries\n",
    "    # Random sample\n",
    "    sample = df.sample(frac=1, replace=False).copy()\n",
    "    selected_rows = []\n",
    "    total_weight = 0\n",
    "    total_weighted_cohb = 0\n",
    "\n",
    "    for _, row in sample.iterrows():\n",
    "        weight = row['WEIGHT']\n",
    "        cohb = row['cohb']\n",
    "\n",
    "        if total_weight + weight > target_weight:\n",
    "            remaining_weight = target_weight - total_weight\n",
    "            total_weighted_cohb += cohb * remaining_weight\n",
    "            row_copy = row.copy()\n",
    "            row_copy['WEIGHT'] = remaining_weight\n",
    "            selected_rows.append(row_copy)\n",
    "            total_weight = target_weight\n",
    "            break\n",
    "        else:\n",
    "            total_weight += weight\n",
    "            total_weighted_cohb += cohb * weight\n",
    "            selected_rows.append(row)\n",
    "\n",
    "    weighted_avg_cohb = total_weighted_cohb / total_weight\n",
    "    diff = abs(weighted_avg_cohb - target_avg_cohb)\n",
    "\n",
    "    if diff < closest_diff:\n",
    "        closest_diff = diff\n",
    "        best_match = pd.DataFrame(selected_rows)\n",
    "\n",
    "    if diff <= tolerance:\n",
    "        print(f\"✅ Match found on attempt {i+1}: Weighted avg COHB = ${weighted_avg_cohb:,.2f}\")\n",
    "        break\n",
    "\n",
    "else:\n",
    "    print(f\"⚠️ Closest match after 10,000 attempts: Weighted avg COHB = ${weighted_avg_cohb:,.2f}\")\n",
    "\n",
    "# Save the best match\n",
    "best_match.to_csv(\"../Microsimulations/with_chn/final_subset_2022_random_targeted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
