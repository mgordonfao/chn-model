{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CHN values and added stir/alt_stir for 2022\n",
      "Updated CHN values and added stir/alt_stir for 2023\n",
      "Updated CHN values and added stir/alt_stir for 2024\n",
      "Updated CHN values and added stir/alt_stir for 2025\n",
      "Updated CHN values and added stir/alt_stir for 2026\n",
      "Updated CHN values and added stir/alt_stir for 2027\n",
      "Updated CHN values and added stir/alt_stir for 2028\n",
      "Updated CHN values and added stir/alt_stir for 2029\n",
      "Updated CHN values and added stir/alt_stir for 2030\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "input_base_path = \"../Microsimulations/household/\"\n",
    "output_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Dictionary to store updated data\n",
    "census_data = {}\n",
    "\n",
    "# Loop through the years 2022 to 2030\n",
    "for year in range(2022, 2031):\n",
    "    input_file_path = os.path.join(input_base_path, f\"census{year}_household.csv\")\n",
    "    output_file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    \n",
    "    if os.path.exists(input_file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(input_file_path)\n",
    "        \n",
    "        # Initialize CHN column to 0\n",
    "        census_df['chn'] = 0\n",
    "\n",
    "        # Define housing issue conditions\n",
    "        housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.30) |  # Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define market unaffordability condition\n",
    "        market_unaffordable = (census_df['mmr']) * 12 > 0.30 * census_df['totalincome']\n",
    "\n",
    "        # Update CHN variable\n",
    "        census_df.loc[\n",
    "            housing_issue & market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'chn'\n",
    "        ] = 1\n",
    "\n",
    "        # Now create stir and alt_stir after chn is assigned\n",
    "       # census_df['stir'] = census_df['SHELCO'] * 12 / census_df['totalincome']\n",
    "        #census_df['alt_stir'] = (census_df['mmr']) * 12 / census_df['totalincome']\n",
    "\n",
    "\n",
    "        # Update CHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"chn\"] = 0\n",
    "        \n",
    "\n",
    "        # Define deep core housing issue condition (using 50% income threshold)\n",
    "        deep_housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.50) |  # Deeply Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define deep market unaffordability condition (50% threshold)\n",
    "        deep_market_unaffordable = (census_df['mmr']) * 12 > 0.50 * census_df['totalincome']\n",
    "\n",
    "        # Initialize dchn column to 0\n",
    "        census_df['dchn'] = 0\n",
    "\n",
    "        # Update dchn variable\n",
    "        census_df.loc[\n",
    "            deep_housing_issue & deep_market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'dchn'\n",
    "        ] = 1\n",
    "\n",
    "        # Update DCHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"dchn\"] = 0\n",
    "\n",
    "        # Export updated data\n",
    "        census_df.to_csv(output_file_path, index=False)\n",
    "        \n",
    "        print(f\"Updated CHN values and added stir/alt_stir for {year}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: ../Microsimulations/with_chn/census2021_household_chn.csv\n"
     ]
    }
   ],
   "source": [
    "#add 2021 census file to folder\n",
    "\n",
    "# Define input and output paths\n",
    "input_base_path = \"../Microsimulations/household/\"\n",
    "output_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# File name\n",
    "input_file_name = \"census2021_household.csv\"\n",
    "output_file_name = \"census2021_household_chn.csv\"\n",
    "\n",
    "# Full paths\n",
    "input_file_path = os.path.join(input_base_path, input_file_name)\n",
    "output_file_path = os.path.join(output_base_path, output_file_name)\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "\n",
    "  # Now create stir and alt_stir after chn is assigned\n",
    "df['stir'] = df['SHELCO'] * 12 / df['totalincome']\n",
    "df['alt_stir'] = (df['mmr']) * 12 / df['totalincome']\n",
    "\n",
    "#net income share\n",
    "df['netshare'] = (\n",
    "    df['TOTINC_AT'] / df['totalincome']\n",
    ").clip(upper=1.0)\n",
    "\n",
    "#net income\n",
    "df['netinc'] = df['totalincome'] * df['netshare']\n",
    "\n",
    "# Save the modified dataframe\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021] CHN weighted count: 684245.5791455524, DCHN weighted count: 154141.0278820697\n",
      "[2022] CHN weighted count: 780038.1020617182, DCHN weighted count: 194254.39663840504\n",
      "[2023] CHN weighted count: 904065.3983660915, DCHN weighted count: 248148.73576574316\n",
      "[2024] CHN weighted count: 966356.654397038, DCHN weighted count: 269542.41999433096\n",
      "[2025] CHN weighted count: 974767.4830919917, DCHN weighted count: 273203.8083427341\n",
      "[2026] CHN weighted count: 998330.49527342, DCHN weighted count: 286715.6583432241\n",
      "[2027] CHN weighted count: 1000906.7056510741, DCHN weighted count: 289150.87217011396\n",
      "[2028] CHN weighted count: 1034241.9018145248, DCHN weighted count: 306165.2832864901\n",
      "[2029] CHN weighted count: 1060506.9472327402, DCHN weighted count: 318774.7841629269\n",
      "[2030] CHN weighted count: 1098836.33322642, DCHN weighted count: 334785.85443800245\n",
      "\n",
      "Total household weights per year:\n",
      "Sum of WEIGHT for 2021: 3310969.362428404\n",
      "Sum of WEIGHT for 2022: 3428518.6897024196\n",
      "Sum of WEIGHT for 2023: 3541771.1285604546\n",
      "Sum of WEIGHT for 2024: 3640476.2317533693\n",
      "Sum of WEIGHT for 2025: 3672069.5703686047\n",
      "Sum of WEIGHT for 2026: 3700824.8072495824\n",
      "Sum of WEIGHT for 2027: 3729599.963229115\n",
      "Sum of WEIGHT for 2028: 3785653.9166671736\n",
      "Sum of WEIGHT for 2029: 3838852.557442392\n",
      "Sum of WEIGHT for 2030: 3889702.734139791\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path for processed files\n",
    "output_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# Dictionaries to store weighted household counts\n",
    "chn_weighted_counts = {}\n",
    "dchn_weighted_counts = {}\n",
    "\n",
    "# Loop through years 2022 to 2030\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if required columns exist\n",
    "        required_columns = {'chn', 'dchn', 'WEIGHT', 'HCORENEED_IND'}\n",
    "        missing_columns = required_columns - set(census_df.columns)\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"Skipping {year} due to missing columns: {missing_columns}\")\n",
    "            continue  # Skip processing this file\n",
    "\n",
    "        # Exclude households where HCORENEED_IND == 888\n",
    "        filtered_df = census_df[census_df['HCORENEED_IND'] != 888]\n",
    "\n",
    "        # Calculate weighted count of households where chn == 1\n",
    "        chn_weight = filtered_df.loc[filtered_df['chn'] == 1, 'WEIGHT'].sum()\n",
    "        chn_weighted_counts[year] = chn_weight\n",
    "\n",
    "        # Calculate weighted count of households where dchn == 1\n",
    "        dchn_weight = filtered_df.loc[filtered_df['dchn'] == 1, 'WEIGHT'].sum()\n",
    "        dchn_weighted_counts[year] = dchn_weight\n",
    "\n",
    "        print(f\"[{year}] CHN weighted count: {chn_weight}, DCHN weighted count: {dchn_weight}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "print(\"\\nTotal household weights per year:\")\n",
    "\n",
    "# Print the sum of WEIGHT for each year\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "        print(f\"Sum of WEIGHT for {year}: {census_df['WEIGHT'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define year to process\n",
    "year = 2021\n",
    "file_path = f\"../Microsimulations/census{year}.csv\"\n",
    "\n",
    "# Check if file exists before proceeding\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Create jobless variable\n",
    "    df[\"jobless\"] = df[\"LFACT\"].between(2, 10).astype(int)\n",
    "    \n",
    "    # Calculate share of records with jobless == 1 for each AGEGRP and IMMSTAT\n",
    "    summary = df.groupby([\"AGEGRP\", df[\"IMMSTAT\"].apply(lambda x: \"IMMSTAT_3\" if x == 3 else \"IMMSTAT_not_3\")])[\"jobless\"].mean().reset_index()\n",
    "    \n",
    "    # Save summary to a CSV file\n",
    "    output_path = f\"../Microsimulations/census_share_{year}.csv\"\n",
    "    summary.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated netshare and netinc for 2022\n",
      "✅ Updated netshare and netinc for 2023\n",
      "✅ Updated netshare and netinc for 2024\n",
      "✅ Updated netshare and netinc for 2025\n",
      "✅ Updated netshare and netinc for 2026\n",
      "✅ Updated netshare and netinc for 2027\n",
      "✅ Updated netshare and netinc for 2028\n",
      "✅ Updated netshare and netinc for 2029\n",
      "✅ Updated netshare and netinc for 2030\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load netshare from 2021\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "census2021_household = pd.read_csv(os.path.join(input_base_path, \"census2021_household_chn.csv\"))\n",
    "netshare_2021 = (census2021_household['TOTINC_AT'] / census2021_household['totalincome']).clip(upper=1.0)\n",
    "\n",
    "# Apply to years 2022–2030\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "for year in range(2022, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Assign netshare from 2021 (assumes same order/row count)\n",
    "        census_df['netshare'] = netshare_2021.values\n",
    "        census_df['netinc'] = census_df['totalincome'] * census_df['netshare']\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Updated netshare and netinc for {year}\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Calculated gap for 2021 using updated CHN conditions\n",
      "✅ Calculated gap for 2022 using updated CHN conditions\n",
      "✅ Calculated gap for 2023 using updated CHN conditions\n",
      "✅ Calculated gap for 2024 using updated CHN conditions\n",
      "✅ Calculated gap for 2025 using updated CHN conditions\n",
      "✅ Calculated gap for 2026 using updated CHN conditions\n",
      "✅ Calculated gap for 2027 using updated CHN conditions\n",
      "✅ Calculated gap for 2028 using updated CHN conditions\n",
      "✅ Calculated gap for 2029 using updated CHN conditions\n",
      "✅ Calculated gap for 2030 using updated CHN conditions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Default gap to 0.0\n",
    "        census_df['gap'] = 0.0\n",
    "\n",
    "        # Only calculate gap where CHN = 1\n",
    "        chn_condition = census_df['chn'] == 1\n",
    "\n",
    "        # Use SHELCO if NOS == 1, REPAIR != 3, and SHELCO < mmr \n",
    "        use_shelco = (\n",
    "            chn_condition &\n",
    "            (census_df['NOS'] == 1) &\n",
    "            (census_df['REPAIR'] != 3) &\n",
    "            (census_df['SHELCO'] < census_df['mmr'])\n",
    "        )\n",
    "\n",
    "        # Use AMR (mmr) otherwise\n",
    "        use_amr = chn_condition & ~use_shelco  # CHN == 1 but doesn't meet SHELCO condition\n",
    "\n",
    "        # Apply SHELCO-based gap\n",
    "        census_df.loc[use_shelco, 'gap'] = (\n",
    "            census_df.loc[use_shelco, 'SHELCO'] * 12 - 0.3 * census_df.loc[use_shelco, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Apply AMR-based gap\n",
    "        census_df.loc[use_amr, 'gap'] = (\n",
    "            (census_df.loc[use_amr, 'mmr']) * 12 - 0.3 * census_df.loc[use_amr, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Calculated gap for {year} using updated CHN conditions\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030: CHN=1 count: 9402, GAP > 0 count: 9402\n"
     ]
    }
   ],
   "source": [
    "# After assigning gaps\n",
    "print(f\"{year}: CHN=1 count: {census_df['chn'].sum()}, GAP > 0 count: {(census_df['gap'] > 0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 2021: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 27,916\n",
      "✅ Finished processing 2021\n",
      "📅 2022: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 34,130\n",
      "✅ Finished processing 2022\n",
      "📅 2023: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 51,704\n",
      "✅ Finished processing 2023\n",
      "📅 2024: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 70,992\n",
      "✅ Finished processing 2024\n",
      "📅 2025: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 65,233\n",
      "✅ Finished processing 2025\n",
      "📅 2026: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 59,412\n",
      "✅ Finished processing 2026\n",
      "📅 2027: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 52,282\n",
      "✅ Finished processing 2027\n",
      "📅 2028: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 53,928\n",
      "✅ Finished processing 2028\n",
      "📅 2029: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 54,750\n",
      "✅ Finished processing 2029\n",
      "📅 2030: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 55,717\n",
      "✅ Finished processing 2030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load NPR household IDs from CSV\n",
    "npr_hh_path = \"../Microsimulations/npr_household_ids.csv\"\n",
    "npr_hh_df = pd.read_csv(npr_hh_path)\n",
    "npr_household_ids = npr_hh_df['HH_ID'].tolist()\n",
    "\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "for year in range(2021, 2031):  # 2021 to 2030 inclusive\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # ✅ Add nprhh column based on imported NPR household IDs\n",
    "        if 'HH_ID' in census_df.columns:\n",
    "            census_df['nprhh'] = census_df['HH_ID'].isin(npr_household_ids).astype(int)\n",
    "        else:\n",
    "            print(f\"⚠️ HH_ID column not found in {year} dataset.\")\n",
    "            census_df['nprhh'] = 0\n",
    "\n",
    "        # Initialize COHB to 0.0\n",
    "        census_df['cohb'] = 0.0\n",
    "\n",
    "        # Condition: renter, in core housing need, stir > 0.3\n",
    "        condition = (\n",
    "            (census_df['TENUR'] == 2) &\n",
    "            (census_df['chn'] == 1) &\n",
    "            (census_df['stir'] > 0.3)\n",
    "        )\n",
    "\n",
    "        # Pre-calculate COHB components\n",
    "        mmr_80 = 0.8 * 12 * census_df.loc[condition, 'mmr']\n",
    "        shelco_100_capped = (12 * census_df.loc[condition, 'SHELCO']).clip(\n",
    "            upper=(12 * census_df.loc[condition, 'mmr'])\n",
    "        )\n",
    "        eligible_cost = pd.concat([mmr_80, shelco_100_capped], axis=1).max(axis=1)\n",
    "\n",
    "        netinc_30 = 0.3 * census_df.loc[condition, 'netinc']\n",
    "        cohb_values = eligible_cost - netinc_30\n",
    "\n",
    "        # Final COHB assignment with clipping\n",
    "        census_df.loc[condition, 'cohb'] = cohb_values.clip(lower=0)\n",
    "\n",
    "        # 🔢 Filter: nprhh == 1, chn == 1, and exclude HCORENEED_IND == 888\n",
    "        filter_condition = (\n",
    "            (census_df['nprhh'] == 1) &\n",
    "            (census_df['chn'] == 1) &\n",
    "            (census_df['HCORENEED_IND'] != 888)\n",
    "        )\n",
    "\n",
    "        if 'WEIGHT' in census_df.columns:\n",
    "            weighted_count = census_df.loc[filter_condition, 'WEIGHT'].sum()\n",
    "            print(f\"📅 {year}: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: {weighted_count:,.0f}\")\n",
    "        else:\n",
    "            print(f\"⚠️ WEIGHT column missing in {year} data.\")\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Finished processing {year}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 4401.67\n",
      "2022: 4942.43\n",
      "2023: 5705.12\n",
      "2024: 6156.13\n",
      "2025: 6343.33\n",
      "2026: 6675.12\n",
      "2027: 6832.66\n",
      "2028: 7196.44\n",
      "2029: 7484.86\n",
      "2030: 7889.83\n",
      "\n",
      "📊 Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 3783.17\n",
      "2022: 4123.48\n",
      "2023: 4578.37\n",
      "2024: 4893.83\n",
      "2025: 5034.25\n",
      "2026: 5269.51\n",
      "2027: 5391.20\n",
      "2028: 5651.24\n",
      "2029: 5864.31\n",
      "2030: 6149.34\n",
      "\n",
      "📊 Weighted Mean Income (ALL households, INCLUDING HCORENEED_IND == 888):\n",
      "2021: $59,184.27\n",
      "2022: $60,552.19\n",
      "2023: $61,698.69\n",
      "2024: $63,332.90\n",
      "2025: $65,495.95\n",
      "2026: $67,564.44\n",
      "2027: $69,704.52\n",
      "2028: $71,445.37\n",
      "2029: $73,177.80\n",
      "2030: $74,966.99\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "mean_cohb = {}\n",
    "mean_gap = {}\n",
    "mean_income = {}  # NEW dictionary to store weighted avg income\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df_full = pd.read_csv(file_path)  # full, unfiltered\n",
    "\n",
    "        # Ensure the necessary columns exist\n",
    "        if all(col in census_df_full.columns for col in ['cohb', 'gap', 'WEIGHT', 'HCORENEED_IND', 'totalincome']):\n",
    "            \n",
    "            # 1️⃣ For COHB & GAP → exclude HCORENEED_IND == 888\n",
    "            census_df = census_df_full[census_df_full['HCORENEED_IND'] != 888].copy()\n",
    "\n",
    "            # COHB > 0\n",
    "            cohb_positive = census_df[census_df['cohb'] > 0]\n",
    "            if not cohb_positive.empty:\n",
    "                weighted_mean_cohb = (cohb_positive['cohb'] * cohb_positive['WEIGHT']).sum() / cohb_positive['WEIGHT'].sum()\n",
    "                mean_cohb[year] = weighted_mean_cohb\n",
    "            else:\n",
    "                mean_cohb[year] = 0\n",
    "\n",
    "            # GAP > 0\n",
    "            gap_positive = census_df[census_df['gap'] > 0]\n",
    "            if not gap_positive.empty:\n",
    "                weighted_mean_gap = (gap_positive['gap'] * gap_positive['WEIGHT']).sum() / gap_positive['WEIGHT'].sum()\n",
    "                mean_gap[year] = weighted_mean_gap\n",
    "            else:\n",
    "                mean_gap[year] = 0\n",
    "\n",
    "            # 2️⃣ For Income → use *full*, unfiltered dataframe\n",
    "            if not census_df_full.empty:\n",
    "                weighted_mean_income = (census_df_full['totalincome'] * census_df_full['WEIGHT']).sum() / census_df_full['WEIGHT'].sum()\n",
    "                mean_income[year] = weighted_mean_income\n",
    "            else:\n",
    "                mean_income[year] = 0\n",
    "        else:\n",
    "            print(f\"❌ Missing columns in {year}, skipping.\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}\")\n",
    "\n",
    "# ✅ Print results\n",
    "print(\"\\n📊 Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_cohb.items():\n",
    "    print(f\"{year}: {val:.2f}\")\n",
    "\n",
    "print(\"\\n📊 Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_gap.items():\n",
    "    print(f\"{year}: {val:.2f}\")\n",
    "\n",
    "print(\"\\n📊 Weighted Mean Income (ALL households, INCLUDING HCORENEED_IND == 888):\")\n",
    "for year, val in mean_income.items():\n",
    "    print(f\"{year}: ${val:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chn_trace_row52.csv created at ../Microsimulations/with_chn/chn_trace_row53.csv\n"
     ]
    }
   ],
   "source": [
    "#trace file\n",
    "\n",
    "\n",
    "# Folder path\n",
    "base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# Years to process\n",
    "years = range(2021, 2031)\n",
    "\n",
    "# Row number to extract (0-based index)\n",
    "target_row = 51  # Change this to any row index you want\n",
    "\n",
    "# List to store selected rows\n",
    "selected_rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_name = f\"census{year}_household_chn.csv\"\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if len(df) > target_row:\n",
    "            selected_row = df.iloc[target_row]\n",
    "            selected_rows.append(selected_row)\n",
    "        else:\n",
    "            print(f\"File {file_name} has less than {target_row + 1} rows.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Combine and save to chn_trace.csv\n",
    "if selected_rows:\n",
    "    chn_trace_df = pd.DataFrame(selected_rows)\n",
    "    output_path = os.path.join(base_path, f\"chn_trace_row{target_row + 2}.csv\")\n",
    "    chn_trace_df.to_csv(output_path, index=False)\n",
    "    print(f\"chn_trace_row{target_row + 1}.csv created at {output_path}\")\n",
    "else:\n",
    "    print(\"No data found to create trace file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Weighted Average Netshare by Quintile:\n",
      "\n",
      "   quintile  weighted_netshare\n",
      "0         1             0.9442\n",
      "1         2             0.9152\n",
      "2         3             0.8822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_35476\\3567599826.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/census2021_household_chn.csv\")\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['netinc', 'totalincome', 'WEIGHT', 'quintile']\n",
    "if all(col in df.columns for col in required_cols):\n",
    "    # Compute netshare safely\n",
    "    df['netshare'] = df['netinc'] / df['totalincome']\n",
    "    df = df.replace([float('inf'), -float('inf')], pd.NA).dropna(subset=['netshare'])\n",
    "\n",
    "    # Group by quintile and calculate weighted average netshare\n",
    "    summary = (\n",
    "        df.groupby('quintile')\n",
    "        .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    "        .reset_index(name='weighted_netshare')\n",
    "    )\n",
    "\n",
    "    # Format output\n",
    "    summary['weighted_netshare'] = summary['weighted_netshare'].round(4)\n",
    "    print(\"\\n✅ Weighted Average Netshare by Quintile:\\n\")\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"❌ Missing required columns: netinc, totalincome, WEIGHT, or quintile.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       totalincome   netinc   netshare\n",
      "5860           401  -7000.0 -17.456359\n",
      "30378         1000  -6000.0  -6.000000\n",
      "25865         4400 -23000.0  -5.227273\n",
      "8073         11300 -50000.0  -4.424779\n",
      "5262          1000  -4000.0  -4.000000\n",
      "16587        13400 -53000.0  -3.955224\n",
      "24238          801  -3000.0  -3.745318\n",
      "9991          6800 -20000.0  -2.941176\n",
      "7770         11900 -30000.0  -2.521008\n",
      "4357          4500 -11000.0  -2.444444\n"
     ]
    }
   ],
   "source": [
    "print(df[['totalincome', 'netinc', 'netshare']].sort_values(by='netshare').head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore for now: COHB/affordable housing program analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted average income: $19,998.97\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example: assuming df2024 is loaded\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/census2024_household_chn.csv\")\n",
    "df = df[\n",
    "    (df['TENUR'] == 2) &\n",
    "    (df['chn'] == 1) &\n",
    "    (df['SUBSIDY'] == 0)\n",
    "].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_households_below_weighted_avg(df, target_avg=20000):\n",
    "    df_sorted = df.sort_values(by=\"totalincome\").reset_index(drop=True)\n",
    "    \n",
    "    df_sorted['cum_weighted_income'] = (df_sorted['totalincome'] * df_sorted['WEIGHT']).cumsum()\n",
    "    df_sorted['cum_weight'] = df_sorted['WEIGHT'].cumsum()\n",
    "    df_sorted['cum_weighted_avg'] = df_sorted['cum_weighted_income'] / df_sorted['cum_weight']\n",
    "\n",
    "    # Find the row where weighted average is closest to target_avg\n",
    "    df_sorted['abs_diff'] = (df_sorted['cum_weighted_avg'] - target_avg).abs()\n",
    "    best_idx = df_sorted['abs_diff'].idxmin()\n",
    "\n",
    "    subset_df = df_sorted.loc[:best_idx].copy()\n",
    "    subset_df.drop(columns=['cum_weighted_income', 'cum_weight', 'cum_weighted_avg', 'abs_diff'], inplace=True)\n",
    "\n",
    "    return subset_df\n",
    "\n",
    "\n",
    "\n",
    "subset_df = select_households_below_weighted_avg(df)\n",
    "\n",
    "subset_df['estgap'] = 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "subset_df['ntgap'] = 0.8 * 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "\n",
    "\n",
    "weighted_avg = (subset_df['totalincome'] * subset_df['WEIGHT']).sum() / subset_df['WEIGHT'].sum()\n",
    "print(f\"Weighted average income: ${weighted_avg:,.2f}\")\n",
    "subset_df.to_csv(\"../Microsimulations/with_chn/subset_2024.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final weighted avg COHB: $10,011.93\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your subset\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/subset_2024.csv\")\n",
    "\n",
    "target_weight = 22000\n",
    "target_avg_cohb = 10200\n",
    "\n",
    "# Create a new column: absolute difference from target\n",
    "df['cohb_diff'] = (df['cohb'] - target_avg_cohb).abs()\n",
    "\n",
    "# Sort by closest to target COHB first\n",
    "df_sorted = df.sort_values(by='cohb_diff').reset_index(drop=True)\n",
    "\n",
    "selected_rows = []\n",
    "total_weight = 0\n",
    "total_weighted_cohb = 0\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    weight = row['WEIGHT']\n",
    "    cohb = row['cohb']\n",
    "\n",
    "    if total_weight + weight > target_weight:\n",
    "        remaining_weight = target_weight - total_weight\n",
    "        total_weighted_cohb += cohb * remaining_weight\n",
    "        row_copy = row.copy()\n",
    "        row_copy['WEIGHT'] = remaining_weight\n",
    "        selected_rows.append(row_copy)\n",
    "        total_weight = target_weight\n",
    "        break\n",
    "    else:\n",
    "        total_weight += weight\n",
    "        total_weighted_cohb += cohb * weight\n",
    "        selected_rows.append(row)\n",
    "\n",
    "# Calculate final weighted average\n",
    "weighted_avg_cohb = total_weighted_cohb / total_weight\n",
    "print(f\"✅ Final weighted avg COHB: ${weighted_avg_cohb:,.2f}\")\n",
    "\n",
    "# Save\n",
    "final_df = pd.DataFrame(selected_rows)\n",
    "final_df.to_csv(\"../Microsimulations/with_chn/final_subset_2024_greedy.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2025: Weighted avg COHB = $10,334.44 over 22,131 weighted households\n",
      "✅ 2026: Weighted avg COHB = $10,826.09 over 22,129 weighted households\n",
      "✅ 2027: Weighted avg COHB = $11,099.23 over 22,127 weighted households\n",
      "✅ 2028: Weighted avg COHB = $11,606.73 over 22,342 weighted households\n",
      "✅ 2029: Weighted avg COHB = $12,032.85 over 22,559 weighted households\n",
      "✅ 2030: Weighted avg COHB = $12,605.90 over 22,775 weighted households\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your final selected households (from 2024)\n",
    "final_subset = pd.read_csv(\"../Microsimulations/with_chn/final_subset_2024_greedy.csv\")\n",
    "\n",
    "# Get the list of selected HH_IDs\n",
    "selected_hh_ids = final_subset['HH_ID'].unique()\n",
    "\n",
    "# Range of years to check\n",
    "years = range(2025, 2031)  # 2025 to 2030 inclusive\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    # Load that year's census file\n",
    "    df_year = pd.read_csv(f\"../Microsimulations/with_chn/census{year}_household_chn.csv\")\n",
    "    \n",
    "    # Filter to only the selected HH_IDs\n",
    "    df_matched = df_year[df_year['HH_ID'].isin(selected_hh_ids)].copy()\n",
    "\n",
    "    if df_matched.empty:\n",
    "        print(f\"⚠️ No matching HH_IDs found in {year} data.\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate weighted average COHB\n",
    "    weighted_avg_cohb = (df_matched['cohb'] * df_matched['WEIGHT']).sum() / df_matched['WEIGHT'].sum()\n",
    "\n",
    "    print(f\"✅ {year}: Weighted avg COHB = ${weighted_avg_cohb:,.2f} over {df_matched['WEIGHT'].sum():,.0f} weighted households\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/census2023_household_chn.csv\")\n",
    "\n",
    "# Define conditions based on bedsuit values\n",
    "conditions = [\n",
    "    (df['bedsuit'] == 0) & (df['totalincome'] <= 40000),\n",
    "    (df['bedsuit'] == 1) & (df['totalincome'] <= 49000),\n",
    "    (df['bedsuit'] == 2) & (df['totalincome'] <= 56000),\n",
    "    (df['bedsuit'] == 3) & (df['totalincome'] <= 62000),\n",
    "    (df['bedsuit'] >= 4) & (df['totalincome'] <= 75000),\n",
    "]\n",
    "\n",
    "# Combine all conditions using logical OR\n",
    "combined_condition = conditions[0]\n",
    "for cond in conditions[1:]:\n",
    "    combined_condition |= cond\n",
    "\n",
    "# Create the filtered subset\n",
    "subset_df = df[combined_condition].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENUR\n",
      "1.0    253775.983728\n",
      "2.0    458417.788214\n",
      "Name: WEIGHT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filter for households in core housing need and valid HCORENEED_IND\n",
    "chn_df = subset_df[(subset_df['chn'] == 1) & (subset_df['HCORENEED_IND'] != 888)]\n",
    "\n",
    "# Group by TENUR and sum the WEIGHT for each group\n",
    "weighted_chn_by_tenur = chn_df.groupby('TENUR')['WEIGHT'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(weighted_chn_by_tenur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate CNIT to use for asset threshold shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedsuit\n",
      "0.0     46731.162911\n",
      "1.0     58018.454537\n",
      "2.0     68325.779534\n",
      "3.0     82359.619866\n",
      "4.0    104444.414944\n",
      "5.0    103756.898433\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 104356.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_35476\\1518896814.py:16: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/census2023_household_chn.csv\")\n",
    "\n",
    "# Step 2: Create a filtered copy for households in core housing need with valid HCORENEED_IND\n",
    "df_copy = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)].copy()\n",
    "\n",
    "# Step 3: Create the cnit variable\n",
    "df_copy['cnit'] = (12 * df_copy['mmr']) / 0.3\n",
    "\n",
    "# Step 4: Calculate weighted average CNIT by bedsuit\n",
    "weighted_avg_cnit = (\n",
    "    df_copy\n",
    "    .groupby('bedsuit')\n",
    "    .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(weighted_avg_cnit)\n",
    "\n",
    "\n",
    "# Filter households with bedsuit >= 4\n",
    "df_4plus = df_copy[df_copy['bedsuit'] >= 4]\n",
    "\n",
    "# Calculate weighted average CNIT\n",
    "weighted_avg_cnit_4plus = (df_4plus['cnit'] * df_4plus['WEIGHT']).sum() / df_4plus['WEIGHT'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Weighted average CNIT for 4+ bedroom-suitable households: {weighted_avg_cnit_4plus:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2023 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     46731.162911\n",
      "1.0     58018.454537\n",
      "2.0     68325.779534\n",
      "3.0     82359.619866\n",
      "4.0    104444.414944\n",
      "5.0    103756.898433\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 104356.54\n",
      "\n",
      "Year 2024 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     49244.140167\n",
      "1.0     61206.291065\n",
      "2.0     72060.698785\n",
      "3.0     86757.505496\n",
      "4.0    109694.312029\n",
      "5.0    108813.774293\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 109574.90\n",
      "\n",
      "Year 2025 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     50718.846922\n",
      "1.0     63023.823872\n",
      "2.0     74179.475379\n",
      "3.0     89380.192669\n",
      "4.0    112934.292086\n",
      "5.0    112485.576431\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 112875.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_35476\\583096455.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_35476\\583096455.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_35476\\583096455.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_35476\\583096455.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2026 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     52612.105365\n",
      "1.0     65241.854332\n",
      "2.0     76924.655983\n",
      "3.0     92646.132505\n",
      "4.0    117012.682711\n",
      "5.0    116055.984180\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 116889.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_35476\\583096455.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_35476\\583096455.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_35476\\583096455.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2027 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     53817.672610\n",
      "1.0     66601.763324\n",
      "2.0     78646.344988\n",
      "3.0     94858.407493\n",
      "4.0    119538.665259\n",
      "5.0    119142.451908\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 119489.56\n",
      "\n",
      "Year 2028 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     55750.035606\n",
      "1.0     68929.297514\n",
      "2.0     81584.507200\n",
      "3.0     98216.504707\n",
      "4.0    123593.852162\n",
      "5.0    123474.877602\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 123579.24\n",
      "\n",
      "Year 2029 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     57444.931307\n",
      "1.0     70876.594488\n",
      "2.0     84026.370742\n",
      "3.0    101274.354984\n",
      "4.0    127271.609096\n",
      "5.0    127259.817799\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 127270.17\n",
      "\n",
      "Year 2030 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     59603.029695\n",
      "1.0     73338.504702\n",
      "2.0     87155.788177\n",
      "3.0    104935.674609\n",
      "4.0    131726.985889\n",
      "5.0    132096.369526\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 131771.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_35476\\583096455.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your directory\n",
    "base_path = \"../Microsimulations/with_chn\"\n",
    "\n",
    "# Loop through years 2023 to 2030\n",
    "for year in range(2023, 2031):\n",
    "    file_path = os.path.join(base_path, f\"census{year}_household_chn.csv\")\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter for households in core housing need with valid HCORENEED_IND\n",
    "    df_copy = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)].copy()\n",
    "    \n",
    "    # Create the cnit variable\n",
    "    df_copy['cnit'] = (12 * df_copy['mmr']) / 0.3\n",
    "    \n",
    "    # Calculate weighted average CNIT by bedsuit\n",
    "    weighted_avg_cnit = (\n",
    "        df_copy\n",
    "        .groupby('bedsuit')\n",
    "        .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    "    )\n",
    "    \n",
    "    # Display the result for this year\n",
    "    print(f\"\\nYear {year} - Weighted average CNIT by bedsuit:\")\n",
    "    print(weighted_avg_cnit)\n",
    "    \n",
    "    # Filter households with bedsuit >= 4\n",
    "    df_4plus = df_copy[df_copy['bedsuit'] >= 4]\n",
    "    \n",
    "    # Calculate weighted average CNIT for 4+ bedroom-suitable households\n",
    "    if not df_4plus.empty:\n",
    "        weighted_avg_cnit_4plus = (df_4plus['cnit'] * df_4plus['WEIGHT']).sum() / df_4plus['WEIGHT'].sum()\n",
    "        print(f\"Weighted average CNIT for 4+ bedroom-suitable households: {weighted_avg_cnit_4plus:.2f}\")\n",
    "    else:\n",
    "        print(\"No 4+ bedroom-suitable households in this year.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted count (TENUR = 2): 514,038\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 130,864\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 207,459\n",
      "Total weighted households (filtered): 852,361\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/census2023_household_chn.csv\")\n",
    "\n",
    "# Subset for CHN == 1, HCORENEED_IND != 888, and NPRHH == 0\n",
    "filtered_df = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888) & (df['nprhh'] == 0)]\n",
    "\n",
    "# Calculate weighted counts for each category\n",
    "tenur_2 = filtered_df[filtered_df['TENUR'] == 2]['WEIGHT'].sum()\n",
    "tenur_1_presmortg_0 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 0)]['WEIGHT'].sum()\n",
    "tenur_1_presmortg_1 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 1)]['WEIGHT'].sum()\n",
    "\n",
    "\n",
    "# Total weighted households (after filtering)\n",
    "total_weighted = filtered_df['WEIGHT'].sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Weighted count (TENUR = 2): {tenur_2:,.0f}\")\n",
    "print(f\"Weighted count (TENUR = 1 & PRESMORTG = 0): {tenur_1_presmortg_0:,.0f}\")\n",
    "print(f\"Weighted count (TENUR = 1 & PRESMORTG = 1): {tenur_1_presmortg_1:,.0f}\")\n",
    "print(f\"Total weighted households (filtered): {total_weighted:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Year 2023:\n",
      "Weighted count (TENUR = 2): 558,317\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 132,479\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 213,269\n",
      "Total weighted households (filtered): 904,065\n",
      "\n",
      "📅 Year 2024:\n",
      "Weighted count (TENUR = 2): 602,557\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 137,372\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 226,427\n",
      "Total weighted households (filtered): 966,357\n",
      "\n",
      "📅 Year 2025:\n",
      "Weighted count (TENUR = 2): 606,156\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 139,861\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 228,750\n",
      "Total weighted households (filtered): 974,767\n",
      "\n",
      "📅 Year 2026:\n",
      "Weighted count (TENUR = 2): 620,781\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 142,762\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 234,788\n",
      "Total weighted households (filtered): 998,330\n",
      "\n",
      "📅 Year 2027:\n",
      "Weighted count (TENUR = 2): 619,563\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 144,917\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 236,426\n",
      "Total weighted households (filtered): 1,000,907\n",
      "\n",
      "📅 Year 2028:\n",
      "Weighted count (TENUR = 2): 639,492\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 148,488\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 246,262\n",
      "Total weighted households (filtered): 1,034,242\n",
      "\n",
      "📅 Year 2029:\n",
      "Weighted count (TENUR = 2): 654,228\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 151,533\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 254,746\n",
      "Total weighted households (filtered): 1,060,507\n",
      "\n",
      "📅 Year 2030:\n",
      "Weighted count (TENUR = 2): 677,329\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 155,470\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 266,037\n",
      "Total weighted households (filtered): 1,098,836\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your directory\n",
    "base_path = \"../Microsimulations/with_chn\"\n",
    "\n",
    "# Loop through years 2023 to 2030\n",
    "for year in range(2023, 2031):\n",
    "    file_path = os.path.join(base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the data\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Subset for CHN == 1, HCORENEED_IND != 888, and NPRHH == 0\n",
    "        filtered_df = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)]\n",
    "\n",
    "        # Calculate weighted counts\n",
    "        tenur_2 = filtered_df[filtered_df['TENUR'] == 2]['WEIGHT'].sum()\n",
    "        tenur_1_presmortg_0 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 0)]['WEIGHT'].sum()\n",
    "        tenur_1_presmortg_1 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 1)]['WEIGHT'].sum()\n",
    "\n",
    "        # Total weighted households (after filtering)\n",
    "        total_weighted = filtered_df['WEIGHT'].sum()\n",
    "\n",
    "        # Print the results for this year\n",
    "        print(f\"\\n📅 Year {year}:\")\n",
    "        print(f\"Weighted count (TENUR = 2): {tenur_2:,.0f}\")\n",
    "        print(f\"Weighted count (TENUR = 1 & PRESMORTG = 0): {tenur_1_presmortg_0:,.0f}\")\n",
    "        print(f\"Weighted count (TENUR = 1 & PRESMORTG = 1): {tenur_1_presmortg_1:,.0f}\")\n",
    "        print(f\"Total weighted households (filtered): {total_weighted:,.0f}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ File not found for year {year}, skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate avg income growth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
