{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_dir = os.getenv(\"DATA_PATH\")\n",
    "folder_path = os.path.join(data_dir, \"Microsimulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CHN values and added stir/alt_stir for 2022\n",
      "Updated CHN values and added stir/alt_stir for 2023\n",
      "Updated CHN values and added stir/alt_stir for 2024\n",
      "Updated CHN values and added stir/alt_stir for 2025\n",
      "Updated CHN values and added stir/alt_stir for 2026\n",
      "Updated CHN values and added stir/alt_stir for 2027\n",
      "Updated CHN values and added stir/alt_stir for 2028\n",
      "Updated CHN values and added stir/alt_stir for 2029\n",
      "Updated CHN values and added stir/alt_stir for 2030\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "input_base_path = os.path.join(folder_path, \"household\")\n",
    "output_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Dictionary to store updated data\n",
    "census_data = {}\n",
    "\n",
    "# Loop through the years 2022 to 2030\n",
    "for year in range(2022, 2031):\n",
    "    input_file_path = os.path.join(input_base_path, f\"census{year}_household.csv\")\n",
    "    output_file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    \n",
    "    if os.path.exists(input_file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(input_file_path)\n",
    "        \n",
    "        # Initialize CHN column to 0\n",
    "        census_df['chn'] = 0\n",
    "\n",
    "        # Define housing issue conditions\n",
    "        housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.30) |  # Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define market unaffordability condition\n",
    "        market_unaffordable = (census_df['mmr']) * 12 > 0.30 * census_df['totalincome']\n",
    "\n",
    "        # Update CHN variable\n",
    "        census_df.loc[\n",
    "            housing_issue & market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'chn'\n",
    "        ] = 1\n",
    "\n",
    "        # Now create stir and alt_stir after chn is assigned\n",
    "       # census_df['stir'] = census_df['SHELCO'] * 12 / census_df['totalincome']\n",
    "        #census_df['alt_stir'] = (census_df['mmr']) * 12 / census_df['totalincome']\n",
    "\n",
    "\n",
    "        # Update CHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"chn\"] = 0\n",
    "        \n",
    "\n",
    "        # Define deep core housing issue condition (using 50% income threshold)\n",
    "        deep_housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.50) |  # Deeply Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define deep market unaffordability condition (50% threshold)\n",
    "        deep_market_unaffordable = (census_df['mmr']) * 12 > 0.50 * census_df['totalincome']\n",
    "\n",
    "        # Initialize dchn column to 0\n",
    "        census_df['dchn'] = 0\n",
    "\n",
    "        # Update dchn variable\n",
    "        census_df.loc[\n",
    "            deep_housing_issue & deep_market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'dchn'\n",
    "        ] = 1\n",
    "\n",
    "        # Update DCHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"dchn\"] = 0\n",
    "\n",
    "        # Export updated data\n",
    "        census_df.to_csv(output_file_path, index=False)\n",
    "        \n",
    "        print(f\"Updated CHN values and added stir/alt_stir for {year}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: C:/Users/mgordon/OneDrive - Financial Accountability Office of Ontario/FA2404 Housing and Homelessness Update/Data\\Microsimulations\\with_chn\\census2021_household_chn.csv\n"
     ]
    }
   ],
   "source": [
    "#add 2021 census file to folder\n",
    "\n",
    "# Define input and output paths\n",
    "input_base_path = os.path.join(folder_path, \"household\")\n",
    "output_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# File name\n",
    "input_file_name = \"census2021_household.csv\"\n",
    "output_file_name = \"census2021_household_chn.csv\"\n",
    "\n",
    "# Full paths\n",
    "input_file_path = os.path.join(input_base_path, input_file_name)\n",
    "output_file_path = os.path.join(output_base_path, output_file_name)\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "\n",
    "  # Now create stir and alt_stir after chn is assigned\n",
    "df['stir'] = df['SHELCO'] * 12 / df['totalincome']\n",
    "df['alt_stir'] = (df['mmr']) * 12 / df['totalincome']\n",
    "\n",
    "#net income share\n",
    "df['netshare'] = (\n",
    "    df['TOTINC_AT'] / df['totalincome']\n",
    ").clip(upper=1.0)\n",
    "\n",
    "#net income\n",
    "df['netinc'] = df['totalincome'] * df['netshare']\n",
    "\n",
    "# Save the modified dataframe\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021] CHN weighted count: 684245.5791455524, DCHN weighted count: 154141.0278820697\n",
      "[2022] CHN weighted count: 768459.7971429066, DCHN weighted count: 189153.10440888445\n",
      "[2023] CHN weighted count: 878051.8715488891, DCHN weighted count: 235515.96500592493\n",
      "[2024] CHN weighted count: 918123.9496870566, DCHN weighted count: 250291.51886131393\n",
      "[2025] CHN weighted count: 942574.9216363034, DCHN weighted count: 260233.4142954576\n",
      "[2026] CHN weighted count: 962076.6157002894, DCHN weighted count: 266639.7399537616\n",
      "[2027] CHN weighted count: 958460.4392855032, DCHN weighted count: 266588.1096230006\n",
      "[2028] CHN weighted count: 970296.3275081685, DCHN weighted count: 271314.65747217473\n",
      "[2029] CHN weighted count: 977263.0225034186, DCHN weighted count: 272381.90853394964\n",
      "[2030] CHN weighted count: 987783.0503600006, DCHN weighted count: 275454.0300173497\n",
      "\n",
      "Total household weights per year:\n",
      "Sum of WEIGHT for 2021: 3310969.362428404\n",
      "Sum of WEIGHT for 2022: 3393369.992059277\n",
      "Sum of WEIGHT for 2023: 3506218.7139442815\n",
      "Sum of WEIGHT for 2024: 3618931.0267847637\n",
      "Sum of WEIGHT for 2025: 3652464.700934248\n",
      "Sum of WEIGHT for 2026: 3680027.7488595005\n",
      "Sum of WEIGHT for 2027: 3707116.336482008\n",
      "Sum of WEIGHT for 2028: 3759582.0574440784\n",
      "Sum of WEIGHT for 2029: 3810996.4508053204\n",
      "Sum of WEIGHT for 2030: 3860604.032507048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path for processed files\n",
    "output_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Dictionaries to store weighted household counts\n",
    "chn_weighted_counts = {}\n",
    "dchn_weighted_counts = {}\n",
    "\n",
    "# Loop through years 2022 to 2030\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if required columns exist\n",
    "        required_columns = {'chn', 'dchn', 'WEIGHT', 'HCORENEED_IND'}\n",
    "        missing_columns = required_columns - set(census_df.columns)\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"Skipping {year} due to missing columns: {missing_columns}\")\n",
    "            continue  # Skip processing this file\n",
    "\n",
    "        # Exclude households where HCORENEED_IND == 888\n",
    "        filtered_df = census_df[census_df['HCORENEED_IND'] != 888]\n",
    "\n",
    "        # Calculate weighted count of households where chn == 1\n",
    "        chn_weight = filtered_df.loc[filtered_df['chn'] == 1, 'WEIGHT'].sum()\n",
    "        chn_weighted_counts[year] = chn_weight\n",
    "\n",
    "        # Calculate weighted count of households where dchn == 1\n",
    "        dchn_weight = filtered_df.loc[filtered_df['dchn'] == 1, 'WEIGHT'].sum()\n",
    "        dchn_weighted_counts[year] = dchn_weight\n",
    "\n",
    "        print(f\"[{year}] CHN weighted count: {chn_weight}, DCHN weighted count: {dchn_weight}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "print(\"\\nTotal household weights per year:\")\n",
    "\n",
    "# Print the sum of WEIGHT for each year\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "        print(f\"Sum of WEIGHT for {year}: {census_df['WEIGHT'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define year to process\n",
    "year = 2021\n",
    "file_path = os.path.join(folder_path, f\"census{year}.csv\")\n",
    "\n",
    "# Check if file exists before proceeding\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Create jobless variable\n",
    "    df[\"jobless\"] = df[\"LFACT\"].between(3, 10).astype(int)\n",
    "    \n",
    "    # Calculate share of records with jobless == 1 for each AGEGRP and IMMSTAT\n",
    "    summary = df.groupby([\"AGEGRP\", df[\"IMMSTAT\"].apply(lambda x: \"IMMSTAT_3\" if x == 3 else \"IMMSTAT_not_3\")])[\"jobless\"].mean().reset_index()\n",
    "    \n",
    "    # Save summary to a CSV file\n",
    "    output_path = os.path.join(folder_path, f\"census_share_{year}.csv\")\n",
    "    summary.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated netshare and netinc for 2022\n",
      "✅ Updated netshare and netinc for 2023\n",
      "✅ Updated netshare and netinc for 2024\n",
      "✅ Updated netshare and netinc for 2025\n",
      "✅ Updated netshare and netinc for 2026\n",
      "✅ Updated netshare and netinc for 2027\n",
      "✅ Updated netshare and netinc for 2028\n",
      "✅ Updated netshare and netinc for 2029\n",
      "✅ Updated netshare and netinc for 2030\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load netshare from 2021\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "census2021_household = pd.read_csv(os.path.join(input_base_path, \"census2021_household_chn.csv\"))\n",
    "netshare_2021 = (census2021_household['TOTINC_AT'] / census2021_household['totalincome']).clip(upper=1.0)\n",
    "\n",
    "# Apply to years 2022–2030\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "for year in range(2022, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Assign netshare from 2021 (assumes same order/row count)\n",
    "        census_df['netshare'] = netshare_2021.values\n",
    "        census_df['netinc'] = census_df['totalincome'] * census_df['netshare']\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Updated netshare and netinc for {year}\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Calculated gap for 2021 using updated CHN conditions\n",
      "✅ Calculated gap for 2022 using updated CHN conditions\n",
      "✅ Calculated gap for 2023 using updated CHN conditions\n",
      "✅ Calculated gap for 2024 using updated CHN conditions\n",
      "✅ Calculated gap for 2025 using updated CHN conditions\n",
      "✅ Calculated gap for 2026 using updated CHN conditions\n",
      "✅ Calculated gap for 2027 using updated CHN conditions\n",
      "✅ Calculated gap for 2028 using updated CHN conditions\n",
      "✅ Calculated gap for 2029 using updated CHN conditions\n",
      "✅ Calculated gap for 2030 using updated CHN conditions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Default gap to 0.0\n",
    "        census_df['gap'] = 0.0\n",
    "\n",
    "        # Only calculate gap where CHN = 1\n",
    "        chn_condition = census_df['chn'] == 1\n",
    "\n",
    "        # Use SHELCO if NOS == 1, REPAIR != 3, and SHELCO < mmr \n",
    "        use_shelco = (\n",
    "            chn_condition &\n",
    "            (census_df['NOS'] == 1) &\n",
    "            (census_df['REPAIR'] != 3) &\n",
    "            (census_df['SHELCO'] < census_df['mmr'])\n",
    "        )\n",
    "\n",
    "        # Use AMR (mmr) otherwise\n",
    "        use_amr = chn_condition & ~use_shelco  # CHN == 1 but doesn't meet SHELCO condition\n",
    "\n",
    "        # Apply SHELCO-based gap\n",
    "        census_df.loc[use_shelco, 'gap'] = (\n",
    "            census_df.loc[use_shelco, 'SHELCO'] * 12 - 0.3 * census_df.loc[use_shelco, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Apply AMR-based gap\n",
    "        census_df.loc[use_amr, 'gap'] = (\n",
    "            (census_df.loc[use_amr, 'mmr']) * 12 - 0.3 * census_df.loc[use_amr, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Calculated gap for {year} using updated CHN conditions\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030: CHN=1 count: 8505, GAP > 0 count: 8505\n"
     ]
    }
   ],
   "source": [
    "# After assigning gaps\n",
    "print(f\"{year}: CHN=1 count: {census_df['chn'].sum()}, GAP > 0 count: {(census_df['gap'] > 0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 2021: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 27,916\n",
      "✅ Finished processing 2021\n",
      "📅 2022: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 33,665\n",
      "✅ Finished processing 2022\n",
      "📅 2023: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 51,530\n",
      "✅ Finished processing 2023\n",
      "📅 2024: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 67,545\n",
      "✅ Finished processing 2024\n",
      "📅 2025: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 62,548\n",
      "✅ Finished processing 2025\n",
      "📅 2026: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 57,076\n",
      "✅ Finished processing 2026\n",
      "📅 2027: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 49,609\n",
      "✅ Finished processing 2027\n",
      "📅 2028: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 49,606\n",
      "✅ Finished processing 2028\n",
      "📅 2029: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 49,920\n",
      "✅ Finished processing 2029\n",
      "📅 2030: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 49,223\n",
      "✅ Finished processing 2030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load NPR household IDs from CSV\n",
    "npr_hh_path = os.path.join(folder_path, \"npr_household_ids.csv\")\n",
    "npr_hh_df = pd.read_csv(npr_hh_path)\n",
    "npr_household_ids = npr_hh_df['HH_ID'].tolist()\n",
    "\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "for year in range(2021, 2031):  # 2021 to 2030 inclusive\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # ✅ Add nprhh column based on imported NPR household IDs\n",
    "        if 'HH_ID' in census_df.columns:\n",
    "            census_df['nprhh'] = census_df['HH_ID'].isin(npr_household_ids).astype(int)\n",
    "        else:\n",
    "            print(f\"⚠️ HH_ID column not found in {year} dataset.\")\n",
    "            census_df['nprhh'] = 0\n",
    "\n",
    "        # Initialize COHB to 0.0\n",
    "        census_df['cohb'] = 0.0\n",
    "\n",
    "        # Condition: renter, in core housing need, stir > 0.3\n",
    "        condition = (\n",
    "            (census_df['TENUR'] == 2) &\n",
    "            (census_df['chn'] == 1) &\n",
    "            (census_df['stir'] > 0.3)\n",
    "        )\n",
    "\n",
    "        # Pre-calculate COHB components\n",
    "        mmr_80 = 0.8 * 12 * census_df.loc[condition, 'mmr']\n",
    "        shelco_100_capped = (12 * census_df.loc[condition, 'SHELCO']).clip(\n",
    "            upper=(12 * census_df.loc[condition, 'mmr'])\n",
    "        )\n",
    "        eligible_cost = pd.concat([mmr_80, shelco_100_capped], axis=1).max(axis=1)\n",
    "\n",
    "        netinc_30 = 0.3 * census_df.loc[condition, 'netinc']\n",
    "        cohb_values = eligible_cost - netinc_30\n",
    "\n",
    "        # Final COHB assignment with clipping\n",
    "        census_df.loc[condition, 'cohb'] = cohb_values.clip(lower=0)\n",
    "\n",
    "        # 🔢 Filter: nprhh == 1, chn == 1, and exclude HCORENEED_IND == 888\n",
    "        filter_condition = (\n",
    "            (census_df['nprhh'] == 1) &\n",
    "            (census_df['chn'] == 1) &\n",
    "            (census_df['HCORENEED_IND'] != 888)\n",
    "        )\n",
    "\n",
    "        if 'WEIGHT' in census_df.columns:\n",
    "            weighted_count = census_df.loc[filter_condition, 'WEIGHT'].sum()\n",
    "            print(f\"📅 {year}: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: {weighted_count:,.0f}\")\n",
    "        else:\n",
    "            print(f\"⚠️ WEIGHT column missing in {year} data.\")\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Finished processing {year}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 4401.67\n",
      "2022: 4919.80\n",
      "2023: 5625.75\n",
      "2024: 6019.27\n",
      "2025: 6287.90\n",
      "2026: 6476.86\n",
      "2027: 6539.40\n",
      "2028: 6670.99\n",
      "2029: 6723.64\n",
      "2030: 6829.38\n",
      "\n",
      "📊 Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 3783.17\n",
      "2022: 4105.45\n",
      "2023: 4540.28\n",
      "2024: 4840.74\n",
      "2025: 5028.76\n",
      "2026: 5155.58\n",
      "2027: 5230.50\n",
      "2028: 5342.70\n",
      "2029: 5409.72\n",
      "2030: 5513.75\n",
      "\n",
      "📊 Weighted Mean Income (ALL households, INCLUDING HCORENEED_IND == 888):\n",
      "2021: $59,184.27\n",
      "2022: $60,507.28\n",
      "2023: $61,719.46\n",
      "2024: $64,771.23\n",
      "2025: $66,716.81\n",
      "2026: $68,422.17\n",
      "2027: $70,485.50\n",
      "2028: $72,256.10\n",
      "2029: $74,023.55\n",
      "2030: $75,798.59\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "mean_cohb = {}\n",
    "mean_gap = {}\n",
    "mean_income = {}  # NEW dictionary to store weighted avg income\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df_full = pd.read_csv(file_path)  # full, unfiltered\n",
    "\n",
    "        # Ensure the necessary columns exist\n",
    "        if all(col in census_df_full.columns for col in ['cohb', 'gap', 'WEIGHT', 'HCORENEED_IND', 'totalincome']):\n",
    "            \n",
    "            # 1️⃣ For COHB & GAP → exclude HCORENEED_IND == 888\n",
    "            census_df = census_df_full[census_df_full['HCORENEED_IND'] != 888].copy()\n",
    "\n",
    "            # COHB > 0\n",
    "            cohb_positive = census_df[census_df['cohb'] > 0]\n",
    "            if not cohb_positive.empty:\n",
    "                weighted_mean_cohb = (cohb_positive['cohb'] * cohb_positive['WEIGHT']).sum() / cohb_positive['WEIGHT'].sum()\n",
    "                mean_cohb[year] = weighted_mean_cohb\n",
    "            else:\n",
    "                mean_cohb[year] = 0\n",
    "\n",
    "            # GAP > 0\n",
    "            gap_positive = census_df[census_df['gap'] > 0]\n",
    "            if not gap_positive.empty:\n",
    "                weighted_mean_gap = (gap_positive['gap'] * gap_positive['WEIGHT']).sum() / gap_positive['WEIGHT'].sum()\n",
    "                mean_gap[year] = weighted_mean_gap\n",
    "            else:\n",
    "                mean_gap[year] = 0\n",
    "\n",
    "            # 2️⃣ For Income → use *full*, unfiltered dataframe\n",
    "            if not census_df_full.empty:\n",
    "                weighted_mean_income = (census_df_full['totalincome'] * census_df_full['WEIGHT']).sum() / census_df_full['WEIGHT'].sum()\n",
    "                mean_income[year] = weighted_mean_income\n",
    "            else:\n",
    "                mean_income[year] = 0\n",
    "        else:\n",
    "            print(f\"❌ Missing columns in {year}, skipping.\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}\")\n",
    "\n",
    "# ✅ Print results\n",
    "print(\"\\n📊 Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_cohb.items():\n",
    "    print(f\"{year}: {val:.2f}\")\n",
    "\n",
    "print(\"\\n📊 Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_gap.items():\n",
    "    print(f\"{year}: {val:.2f}\")\n",
    "\n",
    "print(\"\\n📊 Weighted Mean Income (ALL households, INCLUDING HCORENEED_IND == 888):\")\n",
    "for year, val in mean_income.items():\n",
    "    print(f\"{year}: ${val:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chn_trace_row52.csv created at C:/Users/mgordon/OneDrive - Financial Accountability Office of Ontario/FA2404 Housing and Homelessness Update/Data\\Microsimulations\\with_chn\\chn_trace_row53.csv\n"
     ]
    }
   ],
   "source": [
    "#trace file\n",
    "\n",
    "\n",
    "# Folder path\n",
    "base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Years to process\n",
    "years = range(2021, 2031)\n",
    "\n",
    "# Row number to extract (0-based index)\n",
    "target_row = 51  # Change this to any row index you want\n",
    "\n",
    "# List to store selected rows\n",
    "selected_rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_name = f\"census{year}_household_chn.csv\"\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if len(df) > target_row:\n",
    "            selected_row = df.iloc[target_row]\n",
    "            selected_rows.append(selected_row)\n",
    "        else:\n",
    "            print(f\"File {file_name} has less than {target_row + 1} rows.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Combine and save to chn_trace.csv\n",
    "if selected_rows:\n",
    "    chn_trace_df = pd.DataFrame(selected_rows)\n",
    "    output_path = os.path.join(base_path, f\"chn_trace_row{target_row + 2}.csv\")\n",
    "    chn_trace_df.to_csv(output_path, index=False)\n",
    "    print(f\"chn_trace_row{target_row + 1}.csv created at {output_path}\")\n",
    "else:\n",
    "    print(\"No data found to create trace file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Weighted Average Netshare by Quintile:\n",
      "\n",
      "   quintile  weighted_netshare\n",
      "0         1             0.9442\n",
      "1         2             0.9152\n",
      "2         3             0.8822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\510544637.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2021_household_chn.csv\"))\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['netinc', 'totalincome', 'WEIGHT', 'quintile']\n",
    "if all(col in df.columns for col in required_cols):\n",
    "    # Compute netshare safely\n",
    "    df['netshare'] = df['netinc'] / df['totalincome']\n",
    "    df = df.replace([float('inf'), -float('inf')], pd.NA).dropna(subset=['netshare'])\n",
    "\n",
    "    # Group by quintile and calculate weighted average netshare\n",
    "    summary = (\n",
    "        df.groupby('quintile')\n",
    "        .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    "        .reset_index(name='weighted_netshare')\n",
    "    )\n",
    "\n",
    "    # Format output\n",
    "    summary['weighted_netshare'] = summary['weighted_netshare'].round(4)\n",
    "    print(\"\\n✅ Weighted Average Netshare by Quintile:\\n\")\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"❌ Missing required columns: netinc, totalincome, WEIGHT, or quintile.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       totalincome   netinc   netshare\n",
      "5860           401  -7000.0 -17.456359\n",
      "30378         1000  -6000.0  -6.000000\n",
      "25865         4400 -23000.0  -5.227273\n",
      "8073         11300 -50000.0  -4.424779\n",
      "5262          1000  -4000.0  -4.000000\n",
      "16587        13400 -53000.0  -3.955224\n",
      "24238          801  -3000.0  -3.745318\n",
      "9991          6800 -20000.0  -2.941176\n",
      "7770         11900 -30000.0  -2.521008\n",
      "4357          4500 -11000.0  -2.444444\n"
     ]
    }
   ],
   "source": [
    "print(df[['totalincome', 'netinc', 'netshare']].sort_values(by='netshare').head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore for now: COHB/affordable housing program analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted average income: $19,998.39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "output_path = os.path.join(folder_path, \"with_chn\", \"subset_2024.csv\")\n",
    "# Example: assuming df2024 is loaded\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2024_household_chn.csv\"))\n",
    "\n",
    "df = df[\n",
    "    (df['TENUR'] == 2) &\n",
    "    (df['chn'] == 1) &\n",
    "    (df['SUBSIDY'] == 0)\n",
    "].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_households_below_weighted_avg(df, target_avg=20000):\n",
    "    df_sorted = df.sort_values(by=\"totalincome\").reset_index(drop=True)\n",
    "    \n",
    "    df_sorted['cum_weighted_income'] = (df_sorted['totalincome'] * df_sorted['WEIGHT']).cumsum()\n",
    "    df_sorted['cum_weight'] = df_sorted['WEIGHT'].cumsum()\n",
    "    df_sorted['cum_weighted_avg'] = df_sorted['cum_weighted_income'] / df_sorted['cum_weight']\n",
    "\n",
    "    # Find the row where weighted average is closest to target_avg\n",
    "    df_sorted['abs_diff'] = (df_sorted['cum_weighted_avg'] - target_avg).abs()\n",
    "    best_idx = df_sorted['abs_diff'].idxmin()\n",
    "\n",
    "    subset_df = df_sorted.loc[:best_idx].copy()\n",
    "    subset_df.drop(columns=['cum_weighted_income', 'cum_weight', 'cum_weighted_avg', 'abs_diff'], inplace=True)\n",
    "\n",
    "    return subset_df\n",
    "\n",
    "\n",
    "\n",
    "subset_df = select_households_below_weighted_avg(df)\n",
    "\n",
    "subset_df['estgap'] = 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "subset_df['ntgap'] = 0.8 * 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "\n",
    "\n",
    "weighted_avg = (subset_df['totalincome'] * subset_df['WEIGHT']).sum() / subset_df['WEIGHT'].sum()\n",
    "print(f\"Weighted average income: ${weighted_avg:,.2f}\")\n",
    "subset_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final weighted avg COHB: $10,165.81\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "output_path = os.path.join(folder_path, \"with_chn\", \"final_subset_2024_greedy.csv\")\n",
    "# Load your subset\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"subset_2024.csv\"))\n",
    "\n",
    "target_weight = 22000\n",
    "target_avg_cohb = 10600\n",
    "\n",
    "# Create a new column: absolute difference from target\n",
    "df['cohb_diff'] = (df['cohb'] - target_avg_cohb).abs()\n",
    "\n",
    "# Sort by closest to target COHB first\n",
    "df_sorted = df.sort_values(by='cohb_diff').reset_index(drop=True)\n",
    "\n",
    "selected_rows = []\n",
    "total_weight = 0\n",
    "total_weighted_cohb = 0\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    weight = row['WEIGHT']\n",
    "    cohb = row['cohb']\n",
    "\n",
    "    if total_weight + weight > target_weight:\n",
    "        remaining_weight = target_weight - total_weight\n",
    "        total_weighted_cohb += cohb * remaining_weight\n",
    "        row_copy = row.copy()\n",
    "        row_copy['WEIGHT'] = remaining_weight\n",
    "        selected_rows.append(row_copy)\n",
    "        total_weight = target_weight\n",
    "        break\n",
    "    else:\n",
    "        total_weight += weight\n",
    "        total_weighted_cohb += cohb * weight\n",
    "        selected_rows.append(row)\n",
    "\n",
    "# Calculate final weighted average\n",
    "weighted_avg_cohb = total_weighted_cohb / total_weight\n",
    "print(f\"✅ Final weighted avg COHB: ${weighted_avg_cohb:,.2f}\")\n",
    "\n",
    "# Save\n",
    "final_df = pd.DataFrame(selected_rows)\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2024: Weighted avg COHB = $10,165.12 over 22,007 weighted households\n",
      "✅ 2025: Weighted avg COHB = $10,587.99 over 21,979 weighted households\n",
      "✅ 2026: Weighted avg COHB = $10,902.19 over 21,885 weighted households\n",
      "✅ 2027: Weighted avg COHB = $11,070.07 over 21,788 weighted households\n",
      "✅ 2028: Weighted avg COHB = $11,299.94 over 21,955 weighted households\n",
      "✅ 2029: Weighted avg COHB = $11,449.78 over 22,134 weighted households\n",
      "✅ 2030: Weighted avg COHB = $11,656.61 over 22,318 weighted households\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your final selected households (from 2024)\n",
    "final_subset = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"final_subset_2024_greedy.csv\"))\n",
    "\n",
    "\n",
    "# Get the list of selected HH_IDs\n",
    "selected_hh_ids = final_subset['HH_ID'].unique()\n",
    "\n",
    "# Range of years to check\n",
    "years = range(2024, 2031)  # 2025 to 2030 inclusive\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    # Load that year's census file\n",
    "    df_year = pd.read_csv(os.path.join(folder_path, \"with_chn\", f\"census{year}_household_chn.csv\"))\n",
    "    \n",
    "    # Filter to only the selected HH_IDs\n",
    "    df_matched = df_year[df_year['HH_ID'].isin(selected_hh_ids)].copy()\n",
    "\n",
    "    if df_matched.empty:\n",
    "        print(f\"⚠️ No matching HH_IDs found in {year} data.\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate weighted average COHB\n",
    "    weighted_avg_cohb = (df_matched['cohb'] * df_matched['WEIGHT']).sum() / df_matched['WEIGHT'].sum()\n",
    "\n",
    "    print(f\"✅ {year}: Weighted avg COHB = ${weighted_avg_cohb:,.2f} over {df_matched['WEIGHT'].sum():,.0f} weighted households\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2023_household_chn.csv\"))\n",
    "\n",
    "\n",
    "# Define conditions based on bedsuit values\n",
    "conditions = [\n",
    "    (df['bedsuit'] == 0) & (df['totalincome'] <= 40000),\n",
    "    (df['bedsuit'] == 1) & (df['totalincome'] <= 49000),\n",
    "    (df['bedsuit'] == 2) & (df['totalincome'] <= 56000),\n",
    "    (df['bedsuit'] == 3) & (df['totalincome'] <= 62000),\n",
    "    (df['bedsuit'] >= 4) & (df['totalincome'] <= 75000),\n",
    "]\n",
    "\n",
    "# Combine all conditions using logical OR\n",
    "combined_condition = conditions[0]\n",
    "for cond in conditions[1:]:\n",
    "    combined_condition |= cond\n",
    "\n",
    "# Create the filtered subset\n",
    "subset_df = df[combined_condition].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENUR\n",
      "1.0    241147.576884\n",
      "2.0    445077.157150\n",
      "Name: WEIGHT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filter for households in core housing need and valid HCORENEED_IND\n",
    "chn_df = subset_df[(subset_df['chn'] == 1) & (subset_df['HCORENEED_IND'] != 888)]\n",
    "\n",
    "# Group by TENUR and sum the WEIGHT for each group\n",
    "weighted_chn_by_tenur = chn_df.groupby('TENUR')['WEIGHT'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(weighted_chn_by_tenur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate CNIT to use for asset threshold shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedsuit\n",
      "0.0     46739.260049\n",
      "1.0     58171.510037\n",
      "2.0     68383.754562\n",
      "3.0     82392.623954\n",
      "4.0    104664.859686\n",
      "5.0    103760.971054\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 104547.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\413785005.py:16: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2023_household_chn.csv\"))\n",
    "\n",
    "# Step 2: Create a filtered copy for households in core housing need with valid HCORENEED_IND\n",
    "df_copy = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)].copy()\n",
    "\n",
    "# Step 3: Create the cnit variable\n",
    "df_copy['cnit'] = (12 * df_copy['mmr']) / 0.3\n",
    "\n",
    "# Step 4: Calculate weighted average CNIT by bedsuit\n",
    "weighted_avg_cnit = (\n",
    "    df_copy\n",
    "    .groupby('bedsuit')\n",
    "    .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(weighted_avg_cnit)\n",
    "\n",
    "\n",
    "# Filter households with bedsuit >= 4\n",
    "df_4plus = df_copy[df_copy['bedsuit'] >= 4]\n",
    "\n",
    "# Calculate weighted average CNIT\n",
    "weighted_avg_cnit_4plus = (df_4plus['cnit'] * df_4plus['WEIGHT']).sum() / df_4plus['WEIGHT'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Weighted average CNIT for 4+ bedroom-suitable households: {weighted_avg_cnit_4plus:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2023 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     46739.260049\n",
      "1.0     58171.510037\n",
      "2.0     68383.754562\n",
      "3.0     82392.623954\n",
      "4.0    104664.859686\n",
      "5.0    103760.971054\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 104547.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2024 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     49168.414346\n",
      "1.0     61234.767167\n",
      "2.0     72050.645307\n",
      "3.0     86849.170762\n",
      "4.0    110182.989784\n",
      "5.0    108820.268116\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 109994.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2025 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     50890.834160\n",
      "1.0     63433.209644\n",
      "2.0     74552.513586\n",
      "3.0     89787.577795\n",
      "4.0    113900.082036\n",
      "5.0    113075.526211\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 113790.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2026 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     52211.967423\n",
      "1.0     64891.608915\n",
      "2.0     76405.471141\n",
      "3.0     92143.129955\n",
      "4.0    116577.713349\n",
      "5.0    116421.940234\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 116557.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2027 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     53095.496088\n",
      "1.0     65941.991508\n",
      "2.0     77756.706121\n",
      "3.0     93747.119127\n",
      "4.0    118842.391422\n",
      "5.0    118866.606682\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 118845.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2028 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     54153.253545\n",
      "1.0     67136.744979\n",
      "2.0     79283.313113\n",
      "3.0     95687.747382\n",
      "4.0    121389.292514\n",
      "5.0    121249.097609\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 121372.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2029 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     54998.407829\n",
      "1.0     68162.101955\n",
      "2.0     80546.498405\n",
      "3.0     97269.013974\n",
      "4.0    123246.887682\n",
      "5.0    123089.473839\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 123227.23\n",
      "\n",
      "Year 2030 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     55983.419020\n",
      "1.0     69327.349676\n",
      "2.0     81964.661446\n",
      "3.0     99078.868801\n",
      "4.0    125429.553285\n",
      "5.0    125352.727606\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 125419.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_26396\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your directory\n",
    "base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Loop through years 2023 to 2030\n",
    "for year in range(2023, 2031):\n",
    "    file_path = os.path.join(base_path, f\"census{year}_household_chn.csv\")\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter for households in core housing need with valid HCORENEED_IND\n",
    "    df_copy = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)].copy()\n",
    "    \n",
    "    # Create the cnit variable\n",
    "    df_copy['cnit'] = (12 * df_copy['mmr']) / 0.3\n",
    "    \n",
    "    # Calculate weighted average CNIT by bedsuit\n",
    "    weighted_avg_cnit = (\n",
    "        df_copy\n",
    "        .groupby('bedsuit')\n",
    "        .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    "    )\n",
    "    \n",
    "    # Display the result for this year\n",
    "    print(f\"\\nYear {year} - Weighted average CNIT by bedsuit:\")\n",
    "    print(weighted_avg_cnit)\n",
    "    \n",
    "    # Filter households with bedsuit >= 4\n",
    "    df_4plus = df_copy[df_copy['bedsuit'] >= 4]\n",
    "    \n",
    "    # Calculate weighted average CNIT for 4+ bedroom-suitable households\n",
    "    if not df_4plus.empty:\n",
    "        weighted_avg_cnit_4plus = (df_4plus['cnit'] * df_4plus['WEIGHT']).sum() / df_4plus['WEIGHT'].sum()\n",
    "        print(f\"Weighted average CNIT for 4+ bedroom-suitable households: {weighted_avg_cnit_4plus:.2f}\")\n",
    "    else:\n",
    "        print(\"No 4+ bedroom-suitable households in this year.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted count (TENUR = 2): 547,209\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 122,220\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 208,622\n",
      "Total weighted households (filtered): 878,052\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2023_household_chn.csv\"))\n",
    "\n",
    "# Subset for CHN == 1, HCORENEED_IND != 888, and NPRHH == 0\n",
    "filtered_df = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888) ]\n",
    "#& (df['nprhh'] == 0)\n",
    "# Calculate weighted counts for each category\n",
    "tenur_2 = filtered_df[filtered_df['TENUR'] == 2]['WEIGHT'].sum()\n",
    "tenur_1_presmortg_0 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 0)]['WEIGHT'].sum()\n",
    "tenur_1_presmortg_1 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 1)]['WEIGHT'].sum()\n",
    "\n",
    "\n",
    "# Total weighted households (after filtering)\n",
    "total_weighted = filtered_df['WEIGHT'].sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Weighted count (TENUR = 2): {tenur_2:,.0f}\")\n",
    "print(f\"Weighted count (TENUR = 1 & PRESMORTG = 0): {tenur_1_presmortg_0:,.0f}\")\n",
    "print(f\"Weighted count (TENUR = 1 & PRESMORTG = 1): {tenur_1_presmortg_1:,.0f}\")\n",
    "print(f\"Total weighted households (filtered): {total_weighted:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Year 2023:\n",
      "Weighted count (TENUR = 2): 547,209\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 122,220\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 208,622\n",
      "Total weighted households (filtered): 878,052\n",
      "\n",
      "📅 Year 2024:\n",
      "Weighted count (TENUR = 2): 576,120\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 126,014\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 215,990\n",
      "Total weighted households (filtered): 918,124\n",
      "\n",
      "📅 Year 2025:\n",
      "Weighted count (TENUR = 2): 589,236\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 131,226\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 222,112\n",
      "Total weighted households (filtered): 942,575\n",
      "\n",
      "📅 Year 2026:\n",
      "Weighted count (TENUR = 2): 597,275\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 136,773\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 228,028\n",
      "Total weighted households (filtered): 962,077\n",
      "\n",
      "📅 Year 2027:\n",
      "Weighted count (TENUR = 2): 590,953\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 141,208\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 226,300\n",
      "Total weighted households (filtered): 958,460\n",
      "\n",
      "📅 Year 2028:\n",
      "Weighted count (TENUR = 2): 595,972\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 146,074\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 228,250\n",
      "Total weighted households (filtered): 970,296\n",
      "\n",
      "📅 Year 2029:\n",
      "Weighted count (TENUR = 2): 596,902\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 153,121\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 227,240\n",
      "Total weighted households (filtered): 977,263\n",
      "\n",
      "📅 Year 2030:\n",
      "Weighted count (TENUR = 2): 600,567\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 159,203\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 228,012\n",
      "Total weighted households (filtered): 987,783\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your directory\n",
    "base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Loop through years 2023 to 2030\n",
    "for year in range(2023, 2031):\n",
    "    file_path = os.path.join(base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the data\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Subset for CHN == 1, HCORENEED_IND != 888, and NPRHH == 0\n",
    "        filtered_df = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)]\n",
    "\n",
    "        # Calculate weighted counts\n",
    "        tenur_2 = filtered_df[filtered_df['TENUR'] == 2]['WEIGHT'].sum()\n",
    "        tenur_1_presmortg_0 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 0)]['WEIGHT'].sum()\n",
    "        tenur_1_presmortg_1 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 1)]['WEIGHT'].sum()\n",
    "\n",
    "        # Total weighted households (after filtering)\n",
    "        total_weighted = filtered_df['WEIGHT'].sum()\n",
    "\n",
    "        # Print the results for this year\n",
    "        print(f\"\\n📅 Year {year}:\")\n",
    "        print(f\"Weighted count (TENUR = 2): {tenur_2:,.0f}\")\n",
    "        print(f\"Weighted count (TENUR = 1 & PRESMORTG = 0): {tenur_1_presmortg_0:,.0f}\")\n",
    "        print(f\"Weighted count (TENUR = 1 & PRESMORTG = 1): {tenur_1_presmortg_1:,.0f}\")\n",
    "        print(f\"Total weighted households (filtered): {total_weighted:,.0f}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ File not found for year {year}, skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate avg income growth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
