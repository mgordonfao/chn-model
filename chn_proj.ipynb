{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CHN values and added stir/alt_stir for 2022\n",
      "Updated CHN values and added stir/alt_stir for 2023\n",
      "Updated CHN values and added stir/alt_stir for 2024\n",
      "Updated CHN values and added stir/alt_stir for 2025\n",
      "Updated CHN values and added stir/alt_stir for 2026\n",
      "Updated CHN values and added stir/alt_stir for 2027\n",
      "Updated CHN values and added stir/alt_stir for 2028\n",
      "Updated CHN values and added stir/alt_stir for 2029\n",
      "Updated CHN values and added stir/alt_stir for 2030\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "input_base_path = \"../Microsimulations/household/\"\n",
    "output_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Dictionary to store updated data\n",
    "census_data = {}\n",
    "\n",
    "# Loop through the years 2022 to 2030\n",
    "for year in range(2022, 2031):\n",
    "    input_file_path = os.path.join(input_base_path, f\"census{year}_household.csv\")\n",
    "    output_file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    \n",
    "    if os.path.exists(input_file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(input_file_path)\n",
    "        \n",
    "        # Initialize CHN column to 0\n",
    "        census_df['chn'] = 0\n",
    "\n",
    "        # Define housing issue conditions\n",
    "        housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.30) |  # Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define market unaffordability condition\n",
    "        market_unaffordable = (census_df['mmr']) * 12 > 0.30 * census_df['totalincome']\n",
    "\n",
    "        # Update CHN variable\n",
    "        census_df.loc[\n",
    "            housing_issue & market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'chn'\n",
    "        ] = 1\n",
    "\n",
    "        # Now create stir and alt_stir after chn is assigned\n",
    "       # census_df['stir'] = census_df['SHELCO'] * 12 / census_df['totalincome']\n",
    "        #census_df['alt_stir'] = (census_df['mmr']) * 12 / census_df['totalincome']\n",
    "\n",
    "\n",
    "        # Update CHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"chn\"] = 0\n",
    "        \n",
    "\n",
    "        # Define deep core housing issue condition (using 50% income threshold)\n",
    "        deep_housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.50) |  # Deeply Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define deep market unaffordability condition (50% threshold)\n",
    "        deep_market_unaffordable = (census_df['mmr']) * 12 > 0.50 * census_df['totalincome']\n",
    "\n",
    "        # Initialize dchn column to 0\n",
    "        census_df['dchn'] = 0\n",
    "\n",
    "        # Update dchn variable\n",
    "        census_df.loc[\n",
    "            deep_housing_issue & deep_market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'dchn'\n",
    "        ] = 1\n",
    "\n",
    "        # Update DCHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"dchn\"] = 0\n",
    "\n",
    "        # Export updated data\n",
    "        census_df.to_csv(output_file_path, index=False)\n",
    "        \n",
    "        print(f\"Updated CHN values and added stir/alt_stir for {year}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: ../Microsimulations/with_chn/census2021_household_chn.csv\n"
     ]
    }
   ],
   "source": [
    "#add 2021 census file to folder\n",
    "\n",
    "# Define input and output paths\n",
    "input_base_path = \"../Microsimulations/household/\"\n",
    "output_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# File name\n",
    "input_file_name = \"census2021_household.csv\"\n",
    "output_file_name = \"census2021_household_chn.csv\"\n",
    "\n",
    "# Full paths\n",
    "input_file_path = os.path.join(input_base_path, input_file_name)\n",
    "output_file_path = os.path.join(output_base_path, output_file_name)\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "\n",
    "  # Now create stir and alt_stir after chn is assigned\n",
    "df['stir'] = df['SHELCO'] * 12 / df['totalincome']\n",
    "df['alt_stir'] = (df['mmr']) * 12 / df['totalincome']\n",
    "\n",
    "#net income share\n",
    "df['netshare'] = (\n",
    "    df['TOTINC_AT'] / df['totalincome']\n",
    ").clip(upper=1.0)\n",
    "\n",
    "#net income\n",
    "df['netinc'] = df['totalincome'] * df['netshare']\n",
    "\n",
    "# Save the modified dataframe\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021] CHN weighted count: 684245.5791455524, DCHN weighted count: 154141.0278820697\n",
      "[2022] CHN weighted count: 775663.1996845966, DCHN weighted count: 193171.441375075\n",
      "[2023] CHN weighted count: 898977.6674971142, DCHN weighted count: 246696.35102513284\n",
      "[2024] CHN weighted count: 961182.9994609419, DCHN weighted count: 267950.07722888654\n",
      "[2025] CHN weighted count: 1002609.9839466489, DCHN weighted count: 286965.47288308386\n",
      "[2026] CHN weighted count: 1041591.8021459938, DCHN weighted count: 308772.29141312605\n",
      "[2027] CHN weighted count: 1081257.9059293726, DCHN weighted count: 330058.47826986696\n",
      "[2028] CHN weighted count: 1139147.8167526051, DCHN weighted count: 356986.8514300904\n",
      "[2029] CHN weighted count: 1201660.7058479595, DCHN weighted count: 388469.46203185746\n",
      "[2030] CHN weighted count: 1268572.9000548227, DCHN weighted count: 425247.337325424\n",
      "\n",
      "Total household weights per year:\n",
      "Sum of WEIGHT for 2021: 3310969.362428404\n",
      "Sum of WEIGHT for 2022: 3391447.579802723\n",
      "Sum of WEIGHT for 2023: 3499096.9189999024\n",
      "Sum of WEIGHT for 2024: 3601408.5594069078\n",
      "Sum of WEIGHT for 2025: 3640330.1222850895\n",
      "Sum of WEIGHT for 2026: 3669265.6668494893\n",
      "Sum of WEIGHT for 2027: 3697396.7557604937\n",
      "Sum of WEIGHT for 2028: 3750344.351647416\n",
      "Sum of WEIGHT for 2029: 3799703.2702311664\n",
      "Sum of WEIGHT for 2030: 3846727.361787364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path for processed files\n",
    "output_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# Dictionaries to store weighted household counts\n",
    "chn_weighted_counts = {}\n",
    "dchn_weighted_counts = {}\n",
    "\n",
    "# Loop through years 2022 to 2030\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if required columns exist\n",
    "        required_columns = {'chn', 'dchn', 'WEIGHT', 'HCORENEED_IND'}\n",
    "        missing_columns = required_columns - set(census_df.columns)\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"Skipping {year} due to missing columns: {missing_columns}\")\n",
    "            continue  # Skip processing this file\n",
    "\n",
    "        # Exclude households where HCORENEED_IND == 888\n",
    "        filtered_df = census_df[census_df['HCORENEED_IND'] != 888]\n",
    "\n",
    "        # Calculate weighted count of households where chn == 1\n",
    "        chn_weight = filtered_df.loc[filtered_df['chn'] == 1, 'WEIGHT'].sum()\n",
    "        chn_weighted_counts[year] = chn_weight\n",
    "\n",
    "        # Calculate weighted count of households where dchn == 1\n",
    "        dchn_weight = filtered_df.loc[filtered_df['dchn'] == 1, 'WEIGHT'].sum()\n",
    "        dchn_weighted_counts[year] = dchn_weight\n",
    "\n",
    "        print(f\"[{year}] CHN weighted count: {chn_weight}, DCHN weighted count: {dchn_weight}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "print(\"\\nTotal household weights per year:\")\n",
    "\n",
    "# Print the sum of WEIGHT for each year\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "        print(f\"Sum of WEIGHT for {year}: {census_df['WEIGHT'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define year to process\n",
    "year = 2021\n",
    "file_path = f\"../Microsimulations/census{year}.csv\"\n",
    "\n",
    "# Check if file exists before proceeding\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Create jobless variable\n",
    "    df[\"jobless\"] = df[\"LFACT\"].between(2, 10).astype(int)\n",
    "    \n",
    "    # Calculate share of records with jobless == 1 for each AGEGRP and IMMSTAT\n",
    "    summary = df.groupby([\"AGEGRP\", df[\"IMMSTAT\"].apply(lambda x: \"IMMSTAT_3\" if x == 3 else \"IMMSTAT_not_3\")])[\"jobless\"].mean().reset_index()\n",
    "    \n",
    "    # Save summary to a CSV file\n",
    "    output_path = f\"../Microsimulations/census_share_{year}.csv\"\n",
    "    summary.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated netshare and netinc for 2022\n",
      "‚úÖ Updated netshare and netinc for 2023\n",
      "‚úÖ Updated netshare and netinc for 2024\n",
      "‚úÖ Updated netshare and netinc for 2025\n",
      "‚úÖ Updated netshare and netinc for 2026\n",
      "‚úÖ Updated netshare and netinc for 2027\n",
      "‚úÖ Updated netshare and netinc for 2028\n",
      "‚úÖ Updated netshare and netinc for 2029\n",
      "‚úÖ Updated netshare and netinc for 2030\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load netshare from 2021\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "census2021_household = pd.read_csv(os.path.join(input_base_path, \"census2021_household_chn.csv\"))\n",
    "netshare_2021 = (census2021_household['TOTINC_AT'] / census2021_household['totalincome']).clip(upper=1.0)\n",
    "\n",
    "# Apply to years 2022‚Äì2030\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "for year in range(2022, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Assign netshare from 2021 (assumes same order/row count)\n",
    "        census_df['netshare'] = netshare_2021.values\n",
    "        census_df['netinc'] = census_df['totalincome'] * census_df['netshare']\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"‚úÖ Updated netshare and netinc for {year}\")\n",
    "    else:\n",
    "        print(f\"‚ùå File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Calculated gap for 2021 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2022 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2023 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2024 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2025 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2026 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2027 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2028 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2029 using updated CHN conditions\n",
      "‚úÖ Calculated gap for 2030 using updated CHN conditions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Default gap to 0.0\n",
    "        census_df['gap'] = 0.0\n",
    "\n",
    "        # Only calculate gap where CHN = 1\n",
    "        chn_condition = census_df['chn'] == 1\n",
    "\n",
    "        # Use SHELCO if NOS == 1, REPAIR != 3, and SHELCO < mmr \n",
    "        use_shelco = (\n",
    "            chn_condition &\n",
    "            (census_df['NOS'] == 1) &\n",
    "            (census_df['REPAIR'] != 3) &\n",
    "            (census_df['SHELCO'] < census_df['mmr'])\n",
    "        )\n",
    "\n",
    "        # Use AMR (mmr) otherwise\n",
    "        use_amr = chn_condition & ~use_shelco  # CHN == 1 but doesn't meet SHELCO condition\n",
    "\n",
    "        # Apply SHELCO-based gap\n",
    "        census_df.loc[use_shelco, 'gap'] = (\n",
    "            census_df.loc[use_shelco, 'SHELCO'] * 12 - 0.3 * census_df.loc[use_shelco, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Apply AMR-based gap\n",
    "        census_df.loc[use_amr, 'gap'] = (\n",
    "            (census_df.loc[use_amr, 'mmr']) * 12 - 0.3 * census_df.loc[use_amr, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"‚úÖ Calculated gap for {year} using updated CHN conditions\")\n",
    "    else:\n",
    "        print(f\"‚ùå File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030: CHN=1 count: 10960, GAP > 0 count: 10960\n"
     ]
    }
   ],
   "source": [
    "# After assigning gaps\n",
    "print(f\"{year}: CHN=1 count: {census_df['chn'].sum()}, GAP > 0 count: {(census_df['gap'] > 0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Added COHB values for renters in CHN in 2021\n",
      "‚úÖ Added COHB values for renters in CHN in 2022\n",
      "‚úÖ Added COHB values for renters in CHN in 2023\n",
      "‚úÖ Added COHB values for renters in CHN in 2024\n",
      "‚úÖ Added COHB values for renters in CHN in 2025\n",
      "‚úÖ Added COHB values for renters in CHN in 2026\n",
      "‚úÖ Added COHB values for renters in CHN in 2027\n",
      "‚úÖ Added COHB values for renters in CHN in 2028\n",
      "‚úÖ Added COHB values for renters in CHN in 2029\n",
      "‚úÖ Added COHB values for renters in CHN in 2030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "input_base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "for year in range(2021, 2031):  # 2021 to 2030 inclusive\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Initialize COHB to 0.0\n",
    "        census_df['cohb'] = 0.0\n",
    "\n",
    "        # Condition: renter, in core housing need, stir > 0.3\n",
    "        condition = (\n",
    "            (census_df['TENUR'] == 2) &\n",
    "            (census_df['chn'] == 1) &\n",
    "            (census_df['stir'] > 0.3)\n",
    "        )\n",
    "\n",
    "        # Pre-calculate parts\n",
    "        mmr_80 = 0.8 * 12 * census_df.loc[condition, 'mmr']\n",
    "        shelco_100_capped = (12 * census_df.loc[condition, 'SHELCO']).clip(upper=(12 * census_df.loc[condition, 'mmr']))\n",
    "        eligible_cost = pd.concat([mmr_80, shelco_100_capped], axis=1).max(axis=1)\n",
    "\n",
    "        netinc_30 = 0.3 * census_df.loc[condition, 'netinc']\n",
    "        cohb_values = eligible_cost - netinc_30\n",
    "\n",
    "        # Clip to ensure no negative COHB\n",
    "        census_df.loc[condition, 'cohb'] = cohb_values.clip(lower=0)\n",
    "\n",
    "        # Save back to file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"‚úÖ Added COHB values for renters in CHN in {year}\")\n",
    "    else:\n",
    "        print(f\"‚ùå File not found for {year}: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 4401.67\n",
      "2022: 4946.46\n",
      "2023: 5712.38\n",
      "2024: 6168.52\n",
      "2025: 6626.07\n",
      "2026: 7156.80\n",
      "2027: 7646.38\n",
      "2028: 8236.55\n",
      "2029: 8893.56\n",
      "2030: 9675.36\n",
      "\n",
      "üìä Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 3783.17\n",
      "2022: 4123.14\n",
      "2023: 4580.42\n",
      "2024: 4900.40\n",
      "2025: 5191.18\n",
      "2026: 5561.62\n",
      "2027: 5898.50\n",
      "2028: 6299.93\n",
      "2029: 6745.96\n",
      "2030: 7260.19\n"
     ]
    }
   ],
   "source": [
    "mean_cohb = {}\n",
    "mean_gap = {}\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure the necessary columns exist\n",
    "        if all(col in census_df.columns for col in ['cohb', 'gap', 'WEIGHT', 'HCORENEED_IND']):\n",
    "            # Exclude records where HCORENEED_IND == 888\n",
    "            census_df = census_df[census_df['HCORENEED_IND'] != 888]\n",
    "\n",
    "            # Filter rows where cohb > 0\n",
    "            cohb_positive = census_df[census_df['cohb'] > 0]\n",
    "            if not cohb_positive.empty:\n",
    "                weighted_mean_cohb = (cohb_positive['cohb'] * cohb_positive['WEIGHT']).sum() / cohb_positive['WEIGHT'].sum()\n",
    "                mean_cohb[year] = weighted_mean_cohb\n",
    "            else:\n",
    "                mean_cohb[year] = 0\n",
    "\n",
    "            # Filter rows where gap > 0\n",
    "            gap_positive = census_df[census_df['gap'] > 0]\n",
    "            if not gap_positive.empty:\n",
    "                weighted_mean_gap = (gap_positive['gap'] * gap_positive['WEIGHT']).sum() / gap_positive['WEIGHT'].sum()\n",
    "                mean_gap[year] = weighted_mean_gap\n",
    "            else:\n",
    "                mean_gap[year] = 0\n",
    "        else:\n",
    "            print(f\"‚ùå Missing columns in {year}, skipping.\")\n",
    "    else:\n",
    "        print(f\"‚ùå File not found for {year}\")\n",
    "\n",
    "# ‚úÖ Print results\n",
    "print(\"\\nüìä Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_cohb.items():\n",
    "    print(f\"{year}: {val:.2f}\")\n",
    "\n",
    "print(\"\\nüìä Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_gap.items():\n",
    "    print(f\"{year}: {val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chn_trace_row52.csv created at ../Microsimulations/with_chn/chn_trace_row53.csv\n"
     ]
    }
   ],
   "source": [
    "#trace file\n",
    "\n",
    "\n",
    "# Folder path\n",
    "base_path = \"../Microsimulations/with_chn/\"\n",
    "\n",
    "# Years to process\n",
    "years = range(2021, 2031)\n",
    "\n",
    "# Row number to extract (0-based index)\n",
    "target_row = 51  # Change this to any row index you want\n",
    "\n",
    "# List to store selected rows\n",
    "selected_rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_name = f\"census{year}_household_chn.csv\"\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if len(df) > target_row:\n",
    "            selected_row = df.iloc[target_row]\n",
    "            selected_rows.append(selected_row)\n",
    "        else:\n",
    "            print(f\"File {file_name} has less than {target_row + 1} rows.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Combine and save to chn_trace.csv\n",
    "if selected_rows:\n",
    "    chn_trace_df = pd.DataFrame(selected_rows)\n",
    "    output_path = os.path.join(base_path, f\"chn_trace_row{target_row + 2}.csv\")\n",
    "    chn_trace_df.to_csv(output_path, index=False)\n",
    "    print(f\"chn_trace_row{target_row + 1}.csv created at {output_path}\")\n",
    "else:\n",
    "    print(\"No data found to create trace file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Weighted Average Netshare by Quintile:\n",
      "\n",
      "   quintile  weighted_netshare\n",
      "0         1             0.9442\n",
      "1         2             0.9152\n",
      "2         3             0.8822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_31240\\3567599826.py:14: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/census2021_household_chn.csv\")\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['netinc', 'totalincome', 'WEIGHT', 'quintile']\n",
    "if all(col in df.columns for col in required_cols):\n",
    "    # Compute netshare safely\n",
    "    df['netshare'] = df['netinc'] / df['totalincome']\n",
    "    df = df.replace([float('inf'), -float('inf')], pd.NA).dropna(subset=['netshare'])\n",
    "\n",
    "    # Group by quintile and calculate weighted average netshare\n",
    "    summary = (\n",
    "        df.groupby('quintile')\n",
    "        .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    "        .reset_index(name='weighted_netshare')\n",
    "    )\n",
    "\n",
    "    # Format output\n",
    "    summary['weighted_netshare'] = summary['weighted_netshare'].round(4)\n",
    "    print(\"\\n‚úÖ Weighted Average Netshare by Quintile:\\n\")\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"‚ùå Missing required columns: netinc, totalincome, WEIGHT, or quintile.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       totalincome   netinc   netshare\n",
      "5860           401  -7000.0 -17.456359\n",
      "30378         1000  -6000.0  -6.000000\n",
      "25865         4400 -23000.0  -5.227273\n",
      "8073         11300 -50000.0  -4.424779\n",
      "5262          1000  -4000.0  -4.000000\n",
      "16587        13400 -53000.0  -3.955224\n",
      "24238          801  -3000.0  -3.745318\n",
      "9991          6800 -20000.0  -2.941176\n",
      "7770         11900 -30000.0  -2.521008\n",
      "4357          4500 -11000.0  -2.444444\n"
     ]
    }
   ],
   "source": [
    "print(df[['totalincome', 'netinc', 'netshare']].sort_values(by='netshare').head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore for now: COHB/affordable housing program analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted average income: $37,548.84\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example: assuming df2024 is loaded\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/census2024_household_chn.csv\")\n",
    "df = df[\n",
    "    (df['TENUR'] == 2) &\n",
    "    (df['HCORENEED_IND'] != 888) &\n",
    "    (df['SUBSIDY'] == 1)\n",
    "].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_households_below_weighted_avg(df, target_avg=20000):\n",
    "    df_sorted = df.sort_values(by=\"totalincome\")\n",
    "    \n",
    "    subset_rows = []\n",
    "    total_weighted_income = 0\n",
    "    total_weight = 0\n",
    "\n",
    "    for _, row in df_sorted.iterrows():\n",
    "        income = row['totalincome']\n",
    "        weight = row['WEIGHT']\n",
    "        \n",
    "        # Predict the new weighted average if we add this household\n",
    "        new_total_weighted_income = total_weighted_income + income * weight\n",
    "        new_total_weight = total_weight + weight\n",
    "        new_weighted_avg = new_total_weighted_income / new_total_weight\n",
    "        \n",
    "        if new_weighted_avg <= target_avg:\n",
    "            subset_rows.append(row)\n",
    "            total_weighted_income = new_total_weighted_income\n",
    "            total_weight = new_total_weight\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    subset_df = pd.DataFrame(subset_rows)\n",
    "    return subset_df\n",
    "\n",
    "\n",
    "subset_df = select_households_below_weighted_avg(df)\n",
    "\n",
    "subset_df['estgap'] = 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "subset_df['ntgap'] = 0.8 * 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "\n",
    "\n",
    "weighted_avg = (subset_df['totalincome'] * subset_df['WEIGHT']).sum() / subset_df['WEIGHT'].sum()\n",
    "print(f\"Weighted average income: ${weighted_avg:,.2f}\")\n",
    "subset_df.to_csv(\"../Microsimulations/with_chn/subset_2024.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total weighted sample: 55,181\n",
      "üè† Weighted count (estgap > 0): 55,181\n",
      "üè† Weighted count (ntgap < 0): 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your subset\n",
    "subset_df = pd.read_csv(\"../Microsimulations/with_chn/subset_2022.csv\")\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "shuffled_df = subset_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Select rows until cumulative WEIGHT is close to 555606 ¬± 500\n",
    "target_weight = 55606\n",
    "tolerance = 500\n",
    "\n",
    "selected_rows = []\n",
    "total_weight = 0\n",
    "\n",
    "for _, row in shuffled_df.iterrows():\n",
    "    row_weight = row['WEIGHT']\n",
    "    if total_weight + row_weight > target_weight + tolerance:\n",
    "        continue\n",
    "    selected_rows.append(row)\n",
    "    total_weight += row_weight\n",
    "    if total_weight >= target_weight - tolerance:\n",
    "        break\n",
    "\n",
    "# Convert to DataFrame\n",
    "sample_df = pd.DataFrame(selected_rows)\n",
    "\n",
    "# Calculate weighted count where estgap > 0\n",
    "estgap_positive_weighted = sample_df.loc[sample_df['estgap'] > 0, 'WEIGHT'].sum()\n",
    "\n",
    "# Calculate weighted count where ntgap < 0\n",
    "ntgap_negative_weighted = sample_df.loc[sample_df['ntgap'] < 0, 'WEIGHT'].sum()\n",
    "\n",
    "print(f\"‚úÖ Total weighted sample: {total_weight:,.0f}\")\n",
    "print(f\"üè† Weighted count (estgap > 0): {estgap_positive_weighted:,.0f}\")\n",
    "print(f\"üè† Weighted count (ntgap < 0): {ntgap_negative_weighted:,.0f}\")\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "sample_df.to_csv(\"../Microsimulations/with_chn/sample_subset_2022.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Match found on attempt 54: Weighted avg COHB = $6,119.38\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your subset\n",
    "df = pd.read_csv(\"../Microsimulations/with_chn/subset_2024.csv\")\n",
    "\n",
    "# Target constraints\n",
    "target_weight = 514\n",
    "target_avg_cohb = 6154\n",
    "tolerance = 50  # allowable ¬± range for avg COHB\n",
    "\n",
    "best_match = None\n",
    "closest_diff = float('inf')\n",
    "\n",
    "for i in range(10000):  # Max tries\n",
    "    # Random sample\n",
    "    sample = df.sample(frac=1, replace=False).copy()\n",
    "    selected_rows = []\n",
    "    total_weight = 0\n",
    "    total_weighted_cohb = 0\n",
    "\n",
    "    for _, row in sample.iterrows():\n",
    "        weight = row['WEIGHT']\n",
    "        cohb = row['cohb']\n",
    "\n",
    "        if total_weight + weight > target_weight:\n",
    "            remaining_weight = target_weight - total_weight\n",
    "            total_weighted_cohb += cohb * remaining_weight\n",
    "            row_copy = row.copy()\n",
    "            row_copy['WEIGHT'] = remaining_weight\n",
    "            selected_rows.append(row_copy)\n",
    "            total_weight = target_weight\n",
    "            break\n",
    "        else:\n",
    "            total_weight += weight\n",
    "            total_weighted_cohb += cohb * weight\n",
    "            selected_rows.append(row)\n",
    "\n",
    "    weighted_avg_cohb = total_weighted_cohb / total_weight\n",
    "    diff = abs(weighted_avg_cohb - target_avg_cohb)\n",
    "\n",
    "    if diff < closest_diff:\n",
    "        closest_diff = diff\n",
    "        best_match = pd.DataFrame(selected_rows)\n",
    "\n",
    "    if diff <= tolerance:\n",
    "        print(f\"‚úÖ Match found on attempt {i+1}: Weighted avg COHB = ${weighted_avg_cohb:,.2f}\")\n",
    "        break\n",
    "\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Closest match after 10,000 attempts: Weighted avg COHB = ${weighted_avg_cohb:,.2f}\")\n",
    "\n",
    "# Save the best match\n",
    "best_match.to_csv(\"../Microsimulations/with_chn/final_subset_2022_random_targeted.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
