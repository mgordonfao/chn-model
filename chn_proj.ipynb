{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "data_dir = os.getenv(\"DATA_PATH\")\n",
    "folder_path = os.path.join(data_dir, \"Microsimulations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CHN values and added stir/alt_stir for 2022\n",
      "Updated CHN values and added stir/alt_stir for 2023\n",
      "Updated CHN values and added stir/alt_stir for 2024\n",
      "Updated CHN values and added stir/alt_stir for 2025\n",
      "Updated CHN values and added stir/alt_stir for 2026\n",
      "Updated CHN values and added stir/alt_stir for 2027\n",
      "Updated CHN values and added stir/alt_stir for 2028\n",
      "Updated CHN values and added stir/alt_stir for 2029\n",
      "Updated CHN values and added stir/alt_stir for 2030\n"
     ]
    }
   ],
   "source": [
    "# Define input and output paths\n",
    "input_base_path = os.path.join(folder_path, \"household\")\n",
    "output_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(output_base_path, exist_ok=True)\n",
    "\n",
    "# Dictionary to store updated data\n",
    "census_data = {}\n",
    "\n",
    "# Loop through the years 2022 to 2030\n",
    "for year in range(2022, 2031):\n",
    "    input_file_path = os.path.join(input_base_path, f\"census{year}_household.csv\")\n",
    "    output_file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    \n",
    "    if os.path.exists(input_file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(input_file_path)\n",
    "        \n",
    "        # Initialize CHN column to 0\n",
    "        census_df['chn'] = 0\n",
    "\n",
    "        # Define housing issue conditions\n",
    "        housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.30) |  # Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define market unaffordability condition\n",
    "        market_unaffordable = (census_df['mmr']) * 12 > 0.30 * census_df['totalincome']\n",
    "\n",
    "        # Update CHN variable\n",
    "        census_df.loc[\n",
    "            housing_issue & market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'chn'\n",
    "        ] = 1\n",
    "\n",
    "        # Now create stir and alt_stir after chn is assigned\n",
    "       # census_df['stir'] = census_df['SHELCO'] * 12 / census_df['totalincome']\n",
    "        #census_df['alt_stir'] = (census_df['mmr']) * 12 / census_df['totalincome']\n",
    "\n",
    "\n",
    "        # Update CHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"chn\"] = 0\n",
    "        \n",
    "\n",
    "        # Define deep core housing issue condition (using 50% income threshold)\n",
    "        deep_housing_issue = (\n",
    "            (census_df['SHELCO'] * 12 / census_df['totalincome'] > 0.50) |  # Deeply Unaffordable\n",
    "            (census_df['NOS'] == 0) |  # Unsuitable\n",
    "            (census_df['REPAIR'] == 3)  # Inadequate\n",
    "        )\n",
    "\n",
    "        # Define deep market unaffordability condition (50% threshold)\n",
    "        deep_market_unaffordable = (census_df['mmr']) * 12 > 0.50 * census_df['totalincome']\n",
    "\n",
    "        # Initialize dchn column to 0\n",
    "        census_df['dchn'] = 0\n",
    "\n",
    "        # Update dchn variable\n",
    "        census_df.loc[\n",
    "            deep_housing_issue & deep_market_unaffordable &\n",
    "            ~((census_df['student_household'] == 1) & (census_df['non_family_household'] == 1)),\n",
    "            'dchn'\n",
    "        ] = 1\n",
    "\n",
    "        # Update DCHN: Exclude individuals with STIR >= 1\n",
    "        census_df.loc[census_df[\"stir\"] >= 1, \"dchn\"] = 0\n",
    "\n",
    "        # Export updated data\n",
    "        census_df.to_csv(output_file_path, index=False)\n",
    "        \n",
    "        print(f\"Updated CHN values and added stir/alt_stir for {year}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {input_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved to: C:/Users/mgordon/OneDrive - Financial Accountability Office of Ontario/FA2404 Housing and Homelessness Update/Data\\Microsimulations\\with_chn\\census2021_household_chn.csv\n"
     ]
    }
   ],
   "source": [
    "#add 2021 census file to folder\n",
    "\n",
    "# Define input and output paths\n",
    "input_base_path = os.path.join(folder_path, \"household\")\n",
    "output_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# File name\n",
    "input_file_name = \"census2021_household.csv\"\n",
    "output_file_name = \"census2021_household_chn.csv\"\n",
    "\n",
    "# Full paths\n",
    "input_file_path = os.path.join(input_base_path, input_file_name)\n",
    "output_file_path = os.path.join(output_base_path, output_file_name)\n",
    "\n",
    "# Read the file\n",
    "df = pd.read_csv(input_file_path)\n",
    "\n",
    "\n",
    "  # Now create stir and alt_stir after chn is assigned\n",
    "df['stir'] = df['SHELCO'] * 12 / df['totalincome']\n",
    "df['alt_stir'] = (df['mmr']) * 12 / df['totalincome']\n",
    "\n",
    "#net income share\n",
    "df['netshare'] = (\n",
    "    df['TOTINC_AT'] / df['totalincome']\n",
    ").clip(upper=1.0)\n",
    "\n",
    "#net income\n",
    "df['netinc'] = df['totalincome'] * df['netshare']\n",
    "\n",
    "# Save the modified dataframe\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"File saved to: {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2021] CHN weighted count: 684245.5791455524, DCHN weighted count: 154141.0278820697\n",
      "[2022] CHN weighted count: 771298.6736885151, DCHN weighted count: 191579.39066948672\n",
      "[2023] CHN weighted count: 886930.2297600426, DCHN weighted count: 242733.8459279306\n",
      "[2024] CHN weighted count: 955408.110663731, DCHN weighted count: 266338.3140160834\n",
      "[2025] CHN weighted count: 980784.7275827618, DCHN weighted count: 277608.2407062943\n",
      "[2026] CHN weighted count: 997194.6912657222, DCHN weighted count: 284760.7549856046\n",
      "[2027] CHN weighted count: 994861.5017718943, DCHN weighted count: 283707.6747521975\n",
      "[2028] CHN weighted count: 1006926.3042634907, DCHN weighted count: 288144.53678365034\n",
      "[2029] CHN weighted count: 1012989.2499386603, DCHN weighted count: 289871.05782227765\n",
      "[2030] CHN weighted count: 1024278.994921774, DCHN weighted count: 293921.2812099075\n",
      "\n",
      "Total household weights per year:\n",
      "Sum of WEIGHT for 2021: 3310969.362428404\n",
      "Sum of WEIGHT for 2022: 3393369.992059277\n",
      "Sum of WEIGHT for 2023: 3506218.7139442815\n",
      "Sum of WEIGHT for 2024: 3618931.0267847637\n",
      "Sum of WEIGHT for 2025: 3652464.700934248\n",
      "Sum of WEIGHT for 2026: 3680027.7488595005\n",
      "Sum of WEIGHT for 2027: 3707116.336482008\n",
      "Sum of WEIGHT for 2028: 3759582.0574440784\n",
      "Sum of WEIGHT for 2029: 3810996.4508053204\n",
      "Sum of WEIGHT for 2030: 3860604.032507048\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the path for processed files\n",
    "output_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Dictionaries to store weighted household counts\n",
    "chn_weighted_counts = {}\n",
    "dchn_weighted_counts = {}\n",
    "\n",
    "# Loop through years 2022 to 2030\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the data\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Check if required columns exist\n",
    "        required_columns = {'chn', 'dchn', 'WEIGHT', 'HCORENEED_IND'}\n",
    "        missing_columns = required_columns - set(census_df.columns)\n",
    "\n",
    "        if missing_columns:\n",
    "            print(f\"Skipping {year} due to missing columns: {missing_columns}\")\n",
    "            continue  # Skip processing this file\n",
    "\n",
    "        # Exclude households where HCORENEED_IND == 888\n",
    "        filtered_df = census_df[census_df['HCORENEED_IND'] != 888]\n",
    "\n",
    "        # Calculate weighted count of households where chn == 1\n",
    "        chn_weight = filtered_df.loc[filtered_df['chn'] == 1, 'WEIGHT'].sum()\n",
    "        chn_weighted_counts[year] = chn_weight\n",
    "\n",
    "        # Calculate weighted count of households where dchn == 1\n",
    "        dchn_weight = filtered_df.loc[filtered_df['dchn'] == 1, 'WEIGHT'].sum()\n",
    "        dchn_weighted_counts[year] = dchn_weight\n",
    "\n",
    "        print(f\"[{year}] CHN weighted count: {chn_weight}, DCHN weighted count: {dchn_weight}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "print(\"\\nTotal household weights per year:\")\n",
    "\n",
    "# Print the sum of WEIGHT for each year\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(output_base_path, f\"census{year}_household_chn.csv\")\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "        print(f\"Sum of WEIGHT for {year}: {census_df['WEIGHT'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define year to process\n",
    "year = 2021\n",
    "file_path = os.path.join(folder_path, f\"census{year}.csv\")\n",
    "\n",
    "# Check if file exists before proceeding\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Create jobless variable\n",
    "    df[\"jobless\"] = df[\"LFACT\"].between(3, 10).astype(int)\n",
    "    \n",
    "    # Calculate share of records with jobless == 1 for each AGEGRP and IMMSTAT\n",
    "    summary = df.groupby([\"AGEGRP\", df[\"IMMSTAT\"].apply(lambda x: \"IMMSTAT_3\" if x == 3 else \"IMMSTAT_not_3\")])[\"jobless\"].mean().reset_index()\n",
    "    \n",
    "    # Save summary to a CSV file\n",
    "    output_path = os.path.join(folder_path, f\"census_share_{year}.csv\")\n",
    "    summary.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated netshare and netinc for 2022\n",
      "✅ Updated netshare and netinc for 2023\n",
      "✅ Updated netshare and netinc for 2024\n",
      "✅ Updated netshare and netinc for 2025\n",
      "✅ Updated netshare and netinc for 2026\n",
      "✅ Updated netshare and netinc for 2027\n",
      "✅ Updated netshare and netinc for 2028\n",
      "✅ Updated netshare and netinc for 2029\n",
      "✅ Updated netshare and netinc for 2030\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load netshare from 2021\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "census2021_household = pd.read_csv(os.path.join(input_base_path, \"census2021_household_chn.csv\"))\n",
    "netshare_2021 = (census2021_household['TOTINC_AT'] / census2021_household['totalincome']).clip(upper=1.0)\n",
    "\n",
    "# Apply to years 2022–2030\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "for year in range(2022, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Assign netshare from 2021 (assumes same order/row count)\n",
    "        census_df['netshare'] = netshare_2021.values\n",
    "        census_df['netinc'] = census_df['totalincome'] * census_df['netshare']\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Updated netshare and netinc for {year}\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Calculated gap for 2021 using updated CHN conditions\n",
      "✅ Calculated gap for 2022 using updated CHN conditions\n",
      "✅ Calculated gap for 2023 using updated CHN conditions\n",
      "✅ Calculated gap for 2024 using updated CHN conditions\n",
      "✅ Calculated gap for 2025 using updated CHN conditions\n",
      "✅ Calculated gap for 2026 using updated CHN conditions\n",
      "✅ Calculated gap for 2027 using updated CHN conditions\n",
      "✅ Calculated gap for 2028 using updated CHN conditions\n",
      "✅ Calculated gap for 2029 using updated CHN conditions\n",
      "✅ Calculated gap for 2030 using updated CHN conditions\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Default gap to 0.0\n",
    "        census_df['gap'] = 0.0\n",
    "\n",
    "        # Only calculate gap where CHN = 1\n",
    "        chn_condition = census_df['chn'] == 1\n",
    "\n",
    "        # Use SHELCO if NOS == 1, REPAIR != 3, and SHELCO < mmr \n",
    "        use_shelco = (\n",
    "            chn_condition &\n",
    "            (census_df['NOS'] == 1) &\n",
    "            (census_df['REPAIR'] != 3) &\n",
    "            (census_df['SHELCO'] < census_df['mmr'])\n",
    "        )\n",
    "\n",
    "        # Use AMR (mmr) otherwise\n",
    "        use_amr = chn_condition & ~use_shelco  # CHN == 1 but doesn't meet SHELCO condition\n",
    "\n",
    "        # Apply SHELCO-based gap\n",
    "        census_df.loc[use_shelco, 'gap'] = (\n",
    "            census_df.loc[use_shelco, 'SHELCO'] * 12 - 0.3 * census_df.loc[use_shelco, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Apply AMR-based gap\n",
    "        census_df.loc[use_amr, 'gap'] = (\n",
    "            (census_df.loc[use_amr, 'mmr']) * 12 - 0.3 * census_df.loc[use_amr, 'totalincome']\n",
    "        )\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Calculated gap for {year} using updated CHN conditions\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030: CHN=1 count: 8807, GAP > 0 count: 8807\n"
     ]
    }
   ],
   "source": [
    "# After assigning gaps\n",
    "print(f\"{year}: CHN=1 count: {census_df['chn'].sum()}, GAP > 0 count: {(census_df['gap'] > 0).sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 2021: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 27,916\n",
      "✅ Finished processing 2021\n",
      "📅 2022: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 33,786\n",
      "✅ Finished processing 2022\n",
      "📅 2023: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 51,065\n",
      "✅ Finished processing 2023\n",
      "📅 2024: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 70,783\n",
      "✅ Finished processing 2024\n",
      "📅 2025: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 65,026\n",
      "✅ Finished processing 2025\n",
      "📅 2026: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 59,078\n",
      "✅ Finished processing 2026\n",
      "📅 2027: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 52,695\n",
      "✅ Finished processing 2027\n",
      "📅 2028: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 52,295\n",
      "✅ Finished processing 2028\n",
      "📅 2029: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 51,249\n",
      "✅ Finished processing 2029\n",
      "📅 2030: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: 50,472\n",
      "✅ Finished processing 2030\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Load NPR household IDs from CSV\n",
    "npr_hh_path = os.path.join(folder_path, \"npr_household_ids.csv\")\n",
    "npr_hh_df = pd.read_csv(npr_hh_path)\n",
    "npr_household_ids = npr_hh_df['HH_ID'].tolist()\n",
    "\n",
    "input_base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "for year in range(2021, 2031):  # 2021 to 2030 inclusive\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df = pd.read_csv(file_path)\n",
    "\n",
    "        # ✅ Add nprhh column based on imported NPR household IDs\n",
    "        if 'HH_ID' in census_df.columns:\n",
    "            census_df['nprhh'] = census_df['HH_ID'].isin(npr_household_ids).astype(int)\n",
    "        else:\n",
    "            print(f\"⚠️ HH_ID column not found in {year} dataset.\")\n",
    "            census_df['nprhh'] = 0\n",
    "\n",
    "        # Initialize COHB to 0.0\n",
    "        census_df['cohb'] = 0.0\n",
    "\n",
    "        # Condition: renter, in core housing need, stir > 0.3\n",
    "        condition = (\n",
    "            (census_df['TENUR'] == 2) &\n",
    "            (census_df['chn'] == 1) &\n",
    "            (census_df['stir'] > 0.3)\n",
    "        )\n",
    "\n",
    "        # Pre-calculate COHB components\n",
    "        mmr_80 = 0.8 * 12 * census_df.loc[condition, 'mmr']\n",
    "        shelco_100_capped = (12 * census_df.loc[condition, 'SHELCO']).clip(\n",
    "            upper=(12 * census_df.loc[condition, 'mmr'])\n",
    "        )\n",
    "        eligible_cost = pd.concat([mmr_80, shelco_100_capped], axis=1).max(axis=1)\n",
    "\n",
    "        netinc_30 = 0.3 * census_df.loc[condition, 'netinc']\n",
    "        cohb_values = eligible_cost - netinc_30\n",
    "\n",
    "        # Final COHB assignment with clipping\n",
    "        census_df.loc[condition, 'cohb'] = cohb_values.clip(lower=0)\n",
    "\n",
    "        # 🔢 Filter: nprhh == 1, chn == 1, and exclude HCORENEED_IND == 888\n",
    "        filter_condition = (\n",
    "            (census_df['nprhh'] == 1) &\n",
    "            (census_df['chn'] == 1) &\n",
    "            (census_df['HCORENEED_IND'] != 888)\n",
    "        )\n",
    "\n",
    "        if 'WEIGHT' in census_df.columns:\n",
    "            weighted_count = census_df.loc[filter_condition, 'WEIGHT'].sum()\n",
    "            print(f\"📅 {year}: Weighted households with nprhh == 1, chn == 1, HCORENEED_IND != 888: {weighted_count:,.0f}\")\n",
    "        else:\n",
    "            print(f\"⚠️ WEIGHT column missing in {year} data.\")\n",
    "\n",
    "        # Save updated file\n",
    "        census_df.to_csv(file_path, index=False)\n",
    "        print(f\"✅ Finished processing {year}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 4401.67\n",
      "2022: 4939.80\n",
      "2023: 5702.54\n",
      "2024: 6167.62\n",
      "2025: 6430.52\n",
      "2026: 6622.31\n",
      "2027: 6690.86\n",
      "2028: 6825.85\n",
      "2029: 6889.59\n",
      "2030: 6997.42\n",
      "\n",
      "📊 Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\n",
      "2021: 3783.17\n",
      "2022: 4119.00\n",
      "2023: 4571.30\n",
      "2024: 4892.98\n",
      "2025: 5079.02\n",
      "2026: 5221.81\n",
      "2027: 5286.65\n",
      "2028: 5396.79\n",
      "2029: 5467.11\n",
      "2030: 5566.00\n",
      "\n",
      "📊 Weighted Mean Income (ALL households, INCLUDING HCORENEED_IND == 888):\n",
      "2021: $59,184.27\n",
      "2022: $60,515.66\n",
      "2023: $61,660.81\n",
      "2024: $63,270.80\n",
      "2025: $65,174.57\n",
      "2026: $66,849.32\n",
      "2027: $68,872.31\n",
      "2028: $70,609.91\n",
      "2029: $72,344.94\n",
      "2030: $74,087.13\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "mean_cohb = {}\n",
    "mean_gap = {}\n",
    "mean_income = {}  # NEW dictionary to store weighted avg income\n",
    "\n",
    "for year in range(2021, 2031):\n",
    "    file_path = os.path.join(input_base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        census_df_full = pd.read_csv(file_path)  # full, unfiltered\n",
    "\n",
    "        # Ensure the necessary columns exist\n",
    "        if all(col in census_df_full.columns for col in ['cohb', 'gap', 'WEIGHT', 'HCORENEED_IND', 'totalincome']):\n",
    "            \n",
    "            # 1️⃣ For COHB & GAP → exclude HCORENEED_IND == 888\n",
    "            census_df = census_df_full[census_df_full['HCORENEED_IND'] != 888].copy()\n",
    "\n",
    "            # COHB > 0\n",
    "            cohb_positive = census_df[census_df['cohb'] > 0]\n",
    "            if not cohb_positive.empty:\n",
    "                weighted_mean_cohb = (cohb_positive['cohb'] * cohb_positive['WEIGHT']).sum() / cohb_positive['WEIGHT'].sum()\n",
    "                mean_cohb[year] = weighted_mean_cohb\n",
    "            else:\n",
    "                mean_cohb[year] = 0\n",
    "\n",
    "            # GAP > 0\n",
    "            gap_positive = census_df[census_df['gap'] > 0]\n",
    "            if not gap_positive.empty:\n",
    "                weighted_mean_gap = (gap_positive['gap'] * gap_positive['WEIGHT']).sum() / gap_positive['WEIGHT'].sum()\n",
    "                mean_gap[year] = weighted_mean_gap\n",
    "            else:\n",
    "                mean_gap[year] = 0\n",
    "\n",
    "            # 2️⃣ For Income → use *full*, unfiltered dataframe\n",
    "            if not census_df_full.empty:\n",
    "                weighted_mean_income = (census_df_full['totalincome'] * census_df_full['WEIGHT']).sum() / census_df_full['WEIGHT'].sum()\n",
    "                mean_income[year] = weighted_mean_income\n",
    "            else:\n",
    "                mean_income[year] = 0\n",
    "        else:\n",
    "            print(f\"❌ Missing columns in {year}, skipping.\")\n",
    "    else:\n",
    "        print(f\"❌ File not found for {year}\")\n",
    "\n",
    "# ✅ Print results\n",
    "print(\"\\n📊 Weighted Mean COHB (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_cohb.items():\n",
    "    print(f\"{year}: {val:.2f}\")\n",
    "\n",
    "print(\"\\n📊 Weighted Mean GAP (for values > 0, excluding HCORENEED_IND == 888):\")\n",
    "for year, val in mean_gap.items():\n",
    "    print(f\"{year}: {val:.2f}\")\n",
    "\n",
    "print(\"\\n📊 Weighted Mean Income (ALL households, INCLUDING HCORENEED_IND == 888):\")\n",
    "for year, val in mean_income.items():\n",
    "    print(f\"{year}: ${val:,.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chn_trace_row52.csv created at C:/Users/mgordon/OneDrive - Financial Accountability Office of Ontario/FA2404 Housing and Homelessness Update/Data\\Microsimulations\\with_chn\\chn_trace_row53.csv\n"
     ]
    }
   ],
   "source": [
    "#trace file\n",
    "\n",
    "\n",
    "# Folder path\n",
    "base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Years to process\n",
    "years = range(2021, 2031)\n",
    "\n",
    "# Row number to extract (0-based index)\n",
    "target_row = 51  # Change this to any row index you want\n",
    "\n",
    "# List to store selected rows\n",
    "selected_rows = []\n",
    "\n",
    "for year in years:\n",
    "    file_name = f\"census{year}_household_chn.csv\"\n",
    "    file_path = os.path.join(base_path, file_name)\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_csv(file_path)\n",
    "        if len(df) > target_row:\n",
    "            selected_row = df.iloc[target_row]\n",
    "            selected_rows.append(selected_row)\n",
    "        else:\n",
    "            print(f\"File {file_name} has less than {target_row + 1} rows.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "\n",
    "# Combine and save to chn_trace.csv\n",
    "if selected_rows:\n",
    "    chn_trace_df = pd.DataFrame(selected_rows)\n",
    "    output_path = os.path.join(base_path, f\"chn_trace_row{target_row + 2}.csv\")\n",
    "    chn_trace_df.to_csv(output_path, index=False)\n",
    "    print(f\"chn_trace_row{target_row + 1}.csv created at {output_path}\")\n",
    "else:\n",
    "    print(\"No data found to create trace file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Weighted Average Netshare by Quintile:\n",
      "\n",
      "   quintile  weighted_netshare\n",
      "0         1             0.9442\n",
      "1         2             0.9152\n",
      "2         3             0.8822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_19304\\510544637.py:14: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2021_household_chn.csv\"))\n",
    "\n",
    "# Check required columns\n",
    "required_cols = ['netinc', 'totalincome', 'WEIGHT', 'quintile']\n",
    "if all(col in df.columns for col in required_cols):\n",
    "    # Compute netshare safely\n",
    "    df['netshare'] = df['netinc'] / df['totalincome']\n",
    "    df = df.replace([float('inf'), -float('inf')], pd.NA).dropna(subset=['netshare'])\n",
    "\n",
    "    # Group by quintile and calculate weighted average netshare\n",
    "    summary = (\n",
    "        df.groupby('quintile')\n",
    "        .apply(lambda g: (g['netshare'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    "        .reset_index(name='weighted_netshare')\n",
    "    )\n",
    "\n",
    "    # Format output\n",
    "    summary['weighted_netshare'] = summary['weighted_netshare'].round(4)\n",
    "    print(\"\\n✅ Weighted Average Netshare by Quintile:\\n\")\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"❌ Missing required columns: netinc, totalincome, WEIGHT, or quintile.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       totalincome   netinc   netshare\n",
      "5860           401  -7000.0 -17.456359\n",
      "30378         1000  -6000.0  -6.000000\n",
      "25865         4400 -23000.0  -5.227273\n",
      "8073         11300 -50000.0  -4.424779\n",
      "5262          1000  -4000.0  -4.000000\n",
      "16587        13400 -53000.0  -3.955224\n",
      "24238          801  -3000.0  -3.745318\n",
      "9991          6800 -20000.0  -2.941176\n",
      "7770         11900 -30000.0  -2.521008\n",
      "4357          4500 -11000.0  -2.444444\n"
     ]
    }
   ],
   "source": [
    "print(df[['totalincome', 'netinc', 'netshare']].sort_values(by='netshare').head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore for now: COHB/affordable housing program analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted average income: $19,998.73\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "output_path = os.path.join(folder_path, \"with_chn\", \"subset_2024.csv\")\n",
    "# Example: assuming df2024 is loaded\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2024_household_chn.csv\"))\n",
    "\n",
    "df = df[\n",
    "    (df['TENUR'] == 2) &\n",
    "    (df['chn'] == 1) &\n",
    "    (df['SUBSIDY'] == 0)\n",
    "].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def select_households_below_weighted_avg(df, target_avg=20000):\n",
    "    df_sorted = df.sort_values(by=\"totalincome\").reset_index(drop=True)\n",
    "    \n",
    "    df_sorted['cum_weighted_income'] = (df_sorted['totalincome'] * df_sorted['WEIGHT']).cumsum()\n",
    "    df_sorted['cum_weight'] = df_sorted['WEIGHT'].cumsum()\n",
    "    df_sorted['cum_weighted_avg'] = df_sorted['cum_weighted_income'] / df_sorted['cum_weight']\n",
    "\n",
    "    # Find the row where weighted average is closest to target_avg\n",
    "    df_sorted['abs_diff'] = (df_sorted['cum_weighted_avg'] - target_avg).abs()\n",
    "    best_idx = df_sorted['abs_diff'].idxmin()\n",
    "\n",
    "    subset_df = df_sorted.loc[:best_idx].copy()\n",
    "    subset_df.drop(columns=['cum_weighted_income', 'cum_weight', 'cum_weighted_avg', 'abs_diff'], inplace=True)\n",
    "\n",
    "    return subset_df\n",
    "\n",
    "\n",
    "\n",
    "subset_df = select_households_below_weighted_avg(df)\n",
    "\n",
    "subset_df['estgap'] = 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "subset_df['ntgap'] = 0.8 * 12 * subset_df['mmr'] - 0.3 * subset_df['totalincome']\n",
    "\n",
    "\n",
    "weighted_avg = (subset_df['totalincome'] * subset_df['WEIGHT']).sum() / subset_df['WEIGHT'].sum()\n",
    "print(f\"Weighted average income: ${weighted_avg:,.2f}\")\n",
    "subset_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final weighted avg COHB: $10,260.61\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "output_path = os.path.join(folder_path, \"with_chn\", \"final_subset_2024_greedy.csv\")\n",
    "# Load your subset\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"subset_2024.csv\"))\n",
    "\n",
    "target_weight = 22000\n",
    "target_avg_cohb = 10600\n",
    "\n",
    "# Create a new column: absolute difference from target\n",
    "df['cohb_diff'] = (df['cohb'] - target_avg_cohb).abs()\n",
    "\n",
    "# Sort by closest to target COHB first\n",
    "df_sorted = df.sort_values(by='cohb_diff').reset_index(drop=True)\n",
    "\n",
    "selected_rows = []\n",
    "total_weight = 0\n",
    "total_weighted_cohb = 0\n",
    "\n",
    "for _, row in df_sorted.iterrows():\n",
    "    weight = row['WEIGHT']\n",
    "    cohb = row['cohb']\n",
    "\n",
    "    if total_weight + weight > target_weight:\n",
    "        remaining_weight = target_weight - total_weight\n",
    "        total_weighted_cohb += cohb * remaining_weight\n",
    "        row_copy = row.copy()\n",
    "        row_copy['WEIGHT'] = remaining_weight\n",
    "        selected_rows.append(row_copy)\n",
    "        total_weight = target_weight\n",
    "        break\n",
    "    else:\n",
    "        total_weight += weight\n",
    "        total_weighted_cohb += cohb * weight\n",
    "        selected_rows.append(row)\n",
    "\n",
    "# Calculate final weighted average\n",
    "weighted_avg_cohb = total_weighted_cohb / total_weight\n",
    "print(f\"✅ Final weighted avg COHB: ${weighted_avg_cohb:,.2f}\")\n",
    "\n",
    "# Save\n",
    "final_df = pd.DataFrame(selected_rows)\n",
    "final_df.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2024: Weighted avg COHB = $10,268.12 over 22,067 weighted households\n",
      "✅ 2025: Weighted avg COHB = $10,696.35 over 22,103 weighted households\n",
      "✅ 2026: Weighted avg COHB = $11,012.80 over 22,073 weighted households\n",
      "✅ 2027: Weighted avg COHB = $11,182.62 over 22,039 weighted households\n",
      "✅ 2028: Weighted avg COHB = $11,409.98 over 22,249 weighted households\n",
      "✅ 2029: Weighted avg COHB = $11,556.87 over 22,473 weighted households\n",
      "✅ 2030: Weighted avg COHB = $11,761.68 over 22,700 weighted households\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your final selected households (from 2024)\n",
    "final_subset = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"final_subset_2024_greedy.csv\"))\n",
    "\n",
    "\n",
    "# Get the list of selected HH_IDs\n",
    "selected_hh_ids = final_subset['HH_ID'].unique()\n",
    "\n",
    "# Range of years to check\n",
    "years = range(2024, 2031)  # 2025 to 2030 inclusive\n",
    "\n",
    "# Loop through each year\n",
    "for year in years:\n",
    "    # Load that year's census file\n",
    "    df_year = pd.read_csv(os.path.join(folder_path, \"with_chn\", f\"census{year}_household_chn.csv\"))\n",
    "    \n",
    "    # Filter to only the selected HH_IDs\n",
    "    df_matched = df_year[df_year['HH_ID'].isin(selected_hh_ids)].copy()\n",
    "\n",
    "    if df_matched.empty:\n",
    "        print(f\"⚠️ No matching HH_IDs found in {year} data.\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate weighted average COHB\n",
    "    weighted_avg_cohb = (df_matched['cohb'] * df_matched['WEIGHT']).sum() / df_matched['WEIGHT'].sum()\n",
    "\n",
    "    print(f\"✅ {year}: Weighted avg COHB = ${weighted_avg_cohb:,.2f} over {df_matched['WEIGHT'].sum():,.0f} weighted households\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2023_household_chn.csv\"))\n",
    "\n",
    "\n",
    "# Define conditions based on bedsuit values\n",
    "conditions = [\n",
    "    (df['bedsuit'] == 0) & (df['totalincome'] <= 40000),\n",
    "    (df['bedsuit'] == 1) & (df['totalincome'] <= 49000),\n",
    "    (df['bedsuit'] == 2) & (df['totalincome'] <= 56000),\n",
    "    (df['bedsuit'] == 3) & (df['totalincome'] <= 62000),\n",
    "    (df['bedsuit'] >= 4) & (df['totalincome'] <= 75000),\n",
    "]\n",
    "\n",
    "# Combine all conditions using logical OR\n",
    "combined_condition = conditions[0]\n",
    "for cond in conditions[1:]:\n",
    "    combined_condition |= cond\n",
    "\n",
    "# Create the filtered subset\n",
    "subset_df = df[combined_condition].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TENUR\n",
      "1.0    244646.081644\n",
      "2.0    454062.048452\n",
      "Name: WEIGHT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Filter for households in core housing need and valid HCORENEED_IND\n",
    "chn_df = subset_df[(subset_df['chn'] == 1) & (subset_df['HCORENEED_IND'] != 888)]\n",
    "\n",
    "# Group by TENUR and sum the WEIGHT for each group\n",
    "weighted_chn_by_tenur = chn_df.groupby('TENUR')['WEIGHT'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(weighted_chn_by_tenur)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate CNIT to use for asset threshold shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bedsuit\n",
      "0.0     46724.400324\n",
      "1.0     58053.722211\n",
      "2.0     68328.038662\n",
      "3.0     82338.155187\n",
      "4.0    104398.072549\n",
      "5.0    103760.971054\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 104316.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_19304\\413785005.py:16: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2023_household_chn.csv\"))\n",
    "\n",
    "# Step 2: Create a filtered copy for households in core housing need with valid HCORENEED_IND\n",
    "df_copy = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)].copy()\n",
    "\n",
    "# Step 3: Create the cnit variable\n",
    "df_copy['cnit'] = (12 * df_copy['mmr']) / 0.3\n",
    "\n",
    "# Step 4: Calculate weighted average CNIT by bedsuit\n",
    "weighted_avg_cnit = (\n",
    "    df_copy\n",
    "    .groupby('bedsuit')\n",
    "    .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(weighted_avg_cnit)\n",
    "\n",
    "\n",
    "# Filter households with bedsuit >= 4\n",
    "df_4plus = df_copy[df_copy['bedsuit'] >= 4]\n",
    "\n",
    "# Calculate weighted average CNIT\n",
    "weighted_avg_cnit_4plus = (df_4plus['cnit'] * df_4plus['WEIGHT']).sum() / df_4plus['WEIGHT'].sum()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Weighted average CNIT for 4+ bedroom-suitable households: {weighted_avg_cnit_4plus:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_19304\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_19304\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_19304\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2023 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     46724.400324\n",
      "1.0     58053.722211\n",
      "2.0     68328.038662\n",
      "3.0     82338.155187\n",
      "4.0    104398.072549\n",
      "5.0    103760.971054\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 104316.37\n",
      "\n",
      "Year 2024 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     49241.674140\n",
      "1.0     61239.313784\n",
      "2.0     72037.359170\n",
      "3.0     86748.661240\n",
      "4.0    109641.845837\n",
      "5.0    108820.268116\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 109530.04\n",
      "\n",
      "Year 2025 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     50983.730918\n",
      "1.0     63283.908753\n",
      "2.0     74530.514423\n",
      "3.0     89749.614773\n",
      "4.0    113528.866343\n",
      "5.0    112080.641879\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 113334.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_19304\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_19304\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2026 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     52285.792747\n",
      "1.0     64860.060199\n",
      "2.0     76455.721483\n",
      "3.0     92056.100042\n",
      "4.0    116425.186916\n",
      "5.0    115337.807822\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 116284.23\n",
      "\n",
      "Year 2027 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     53177.495158\n",
      "1.0     65957.169225\n",
      "2.0     77748.395571\n",
      "3.0     93687.764203\n",
      "4.0    118373.705082\n",
      "5.0    117691.400495\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 118288.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_19304\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_19304\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Year 2028 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     54231.681334\n",
      "1.0     67231.055977\n",
      "2.0     79260.514830\n",
      "3.0     95582.547042\n",
      "4.0    120757.732903\n",
      "5.0    120047.571994\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 120669.12\n",
      "\n",
      "Year 2029 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     55045.191330\n",
      "1.0     68123.260568\n",
      "2.0     80455.905687\n",
      "3.0     97096.844724\n",
      "4.0    122777.027021\n",
      "5.0    121867.800415\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 122663.14\n",
      "\n",
      "Year 2030 - Weighted average CNIT by bedsuit:\n",
      "bedsuit\n",
      "0.0     56029.625739\n",
      "1.0     69359.221440\n",
      "2.0     81959.915181\n",
      "3.0     98971.059202\n",
      "4.0    125052.978220\n",
      "5.0    124106.771379\n",
      "dtype: float64\n",
      "Weighted average CNIT for 4+ bedroom-suitable households: 124934.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgordon\\AppData\\Local\\Temp\\ipykernel_19304\\3545644307.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to your directory\n",
    "base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Loop through years 2023 to 2030\n",
    "for year in range(2023, 2031):\n",
    "    file_path = os.path.join(base_path, f\"census{year}_household_chn.csv\")\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Filter for households in core housing need with valid HCORENEED_IND\n",
    "    df_copy = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)].copy()\n",
    "    \n",
    "    # Create the cnit variable\n",
    "    df_copy['cnit'] = (12 * df_copy['mmr']) / 0.3\n",
    "    \n",
    "    # Calculate weighted average CNIT by bedsuit\n",
    "    weighted_avg_cnit = (\n",
    "        df_copy\n",
    "        .groupby('bedsuit')\n",
    "        .apply(lambda g: (g['cnit'] * g['WEIGHT']).sum() / g['WEIGHT'].sum())\n",
    "    )\n",
    "    \n",
    "    # Display the result for this year\n",
    "    print(f\"\\nYear {year} - Weighted average CNIT by bedsuit:\")\n",
    "    print(weighted_avg_cnit)\n",
    "    \n",
    "    # Filter households with bedsuit >= 4\n",
    "    df_4plus = df_copy[df_copy['bedsuit'] >= 4]\n",
    "    \n",
    "    # Calculate weighted average CNIT for 4+ bedroom-suitable households\n",
    "    if not df_4plus.empty:\n",
    "        weighted_avg_cnit_4plus = (df_4plus['cnit'] * df_4plus['WEIGHT']).sum() / df_4plus['WEIGHT'].sum()\n",
    "        print(f\"Weighted average CNIT for 4+ bedroom-suitable households: {weighted_avg_cnit_4plus:.2f}\")\n",
    "    else:\n",
    "        print(\"No 4+ bedroom-suitable households in this year.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted count (TENUR = 2): 552,721\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 124,093\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 210,116\n",
      "Total weighted households (filtered): 886,930\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv(os.path.join(folder_path, \"with_chn\", \"census2023_household_chn.csv\"))\n",
    "\n",
    "# Subset for CHN == 1, HCORENEED_IND != 888, and NPRHH == 0\n",
    "filtered_df = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888) ]\n",
    "#& (df['nprhh'] == 0)\n",
    "# Calculate weighted counts for each category\n",
    "tenur_2 = filtered_df[filtered_df['TENUR'] == 2]['WEIGHT'].sum()\n",
    "tenur_1_presmortg_0 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 0)]['WEIGHT'].sum()\n",
    "tenur_1_presmortg_1 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 1)]['WEIGHT'].sum()\n",
    "\n",
    "\n",
    "# Total weighted households (after filtering)\n",
    "total_weighted = filtered_df['WEIGHT'].sum()\n",
    "\n",
    "# Print the results\n",
    "print(f\"Weighted count (TENUR = 2): {tenur_2:,.0f}\")\n",
    "print(f\"Weighted count (TENUR = 1 & PRESMORTG = 0): {tenur_1_presmortg_0:,.0f}\")\n",
    "print(f\"Weighted count (TENUR = 1 & PRESMORTG = 1): {tenur_1_presmortg_1:,.0f}\")\n",
    "print(f\"Total weighted households (filtered): {total_weighted:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Year 2023:\n",
      "Weighted count (TENUR = 2): 552,721\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 124,093\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 210,116\n",
      "Total weighted households (filtered): 886,930\n",
      "\n",
      "📅 Year 2024:\n",
      "Weighted count (TENUR = 2): 600,387\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 130,737\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 224,284\n",
      "Total weighted households (filtered): 955,408\n",
      "\n",
      "📅 Year 2025:\n",
      "Weighted count (TENUR = 2): 613,723\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 135,896\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 231,165\n",
      "Total weighted households (filtered): 980,785\n",
      "\n",
      "📅 Year 2026:\n",
      "Weighted count (TENUR = 2): 619,175\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 141,822\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 236,198\n",
      "Total weighted households (filtered): 997,195\n",
      "\n",
      "📅 Year 2027:\n",
      "Weighted count (TENUR = 2): 612,477\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 147,342\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 235,043\n",
      "Total weighted households (filtered): 994,862\n",
      "\n",
      "📅 Year 2028:\n",
      "Weighted count (TENUR = 2): 616,630\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 153,007\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 237,289\n",
      "Total weighted households (filtered): 1,006,926\n",
      "\n",
      "📅 Year 2029:\n",
      "Weighted count (TENUR = 2): 616,950\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 158,521\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 237,519\n",
      "Total weighted households (filtered): 1,012,989\n",
      "\n",
      "📅 Year 2030:\n",
      "Weighted count (TENUR = 2): 620,119\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 0): 166,489\n",
      "Weighted count (TENUR = 1 & PRESMORTG = 1): 237,671\n",
      "Total weighted households (filtered): 1,024,279\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your directory\n",
    "base_path = os.path.join(folder_path, \"with_chn\")\n",
    "\n",
    "# Loop through years 2023 to 2030\n",
    "for year in range(2023, 2031):\n",
    "    file_path = os.path.join(base_path, f\"census{year}_household_chn.csv\")\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "        # Load the data\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Subset for CHN == 1, HCORENEED_IND != 888, and NPRHH == 0\n",
    "        filtered_df = df[(df['chn'] == 1) & (df['HCORENEED_IND'] != 888)]\n",
    "\n",
    "        # Calculate weighted counts\n",
    "        tenur_2 = filtered_df[filtered_df['TENUR'] == 2]['WEIGHT'].sum()\n",
    "        tenur_1_presmortg_0 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 0)]['WEIGHT'].sum()\n",
    "        tenur_1_presmortg_1 = filtered_df[(filtered_df['TENUR'] == 1) & (filtered_df['PRESMORTG'] == 1)]['WEIGHT'].sum()\n",
    "\n",
    "        # Total weighted households (after filtering)\n",
    "        total_weighted = filtered_df['WEIGHT'].sum()\n",
    "\n",
    "        # Print the results for this year\n",
    "        print(f\"\\n📅 Year {year}:\")\n",
    "        print(f\"Weighted count (TENUR = 2): {tenur_2:,.0f}\")\n",
    "        print(f\"Weighted count (TENUR = 1 & PRESMORTG = 0): {tenur_1_presmortg_0:,.0f}\")\n",
    "        print(f\"Weighted count (TENUR = 1 & PRESMORTG = 1): {tenur_1_presmortg_1:,.0f}\")\n",
    "        print(f\"Total weighted households (filtered): {total_weighted:,.0f}\")\n",
    "\n",
    "    else:\n",
    "        print(f\"❌ File not found for year {year}, skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate avg income growth\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
